{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEKOjCS5U7so"
      },
      "source": [
        "\n",
        "# Menyiapkan Environment\n",
        "\n",
        "- [Enable GPU Runtime](https://docs.haystack.deepset.ai/docs/enabling-gpu-acceleration#enabling-the-gpu-in-colab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hu6H9Es_KAYD"
      },
      "source": [
        "## Instalasi Haystack\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgmFOp82Oht_",
        "outputId": "ddfefa7e-a764-42b7-df6c-273a375717d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.1-py3-none-any.whl (1.8 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.8/1.8 MB 10.1 MB/s eta 0:00:00\n",
            "Installing collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.1\n",
            "Collecting farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]\n",
            "  Downloading farm_haystack-1.26.2-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting boilerpy3 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting events (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting httpx (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.19.2)\n",
            "Collecting lazy-imports==0.3.1 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.0.3)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (9.4.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.2.2)\n",
            "Collecting posthog (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting prompthub-py==4.0.0 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading prompthub_py-4.0.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pydantic<2 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (151 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 151.6/151.6 kB 9.7 MB/s eta 0:00:00\n",
            "Collecting quantulum3 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading quantulum3-0.9.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting rank-bm25 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.31.0)\n",
            "Collecting requests-cache<1.0.0 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading requests_cache-0.9.8-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting scikit-learn>=1.3.0 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting sseclient-py (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (8.4.1)\n",
            "Collecting tiktoken>=0.5.1 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.66.4)\n",
            "Collecting transformers==4.39.3 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.8/134.8 kB 12.6 MB/s eta 0:00:00\n",
            "Collecting elastic-transport<8 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading elastic_transport-7.16.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting elasticsearch<8,>=7.17 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading elasticsearch-7.17.9-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting langdetect (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 981.5/981.5 kB 28.1 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.8.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.23.4)\n",
            "Collecting sentence-transformers>=2.2.0 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting mlflow (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading mlflow-2.14.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting rapidfuzz<2.8.0,>=2.0.15 (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading rapidfuzz-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.11.4)\n",
            "Collecting seqeval (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 43.6/43.6 kB 3.7 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting pillow (from farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=6.0 in /usr/local/lib/python3.10/dist-packages (from prompthub-py==4.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (24.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2024.5.15)\n",
            "Collecting tokenizers<0.19,>=0.14 (from transformers==4.39.3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.4.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.20.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.3.0+cu121)\n",
            "Collecting accelerate>=0.21.0 (from transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading accelerate-0.31.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting urllib3<2,>=1.21.1 (from elastic-transport<8->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 49.3/49.3 kB 4.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: six>=1.12 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<8->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.16.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<8->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2024.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.5.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.12.2)\n",
            "Collecting jarowinkler<2.0.0,>=1.2.0 (from rapidfuzz<2.8.0,>=2.0.15->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading jarowinkler-1.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.7)\n",
            "Collecting appdirs>=1.4.4 (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: attrs>=21.2 in /usr/local/lib/python3.10/dist-packages (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (23.2.0)\n",
            "Collecting cattrs>=22.2 (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading cattrs-23.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting url-normalize>=1.4 (from requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading url_normalize-1.4.3-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.5.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.18.1)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.2.5)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading alembic-1.13.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.2.1)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: entrypoints<1 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.4)\n",
            "Collecting gitpython<4,>=3.1.9 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting graphene<4 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading graphene-3.3-py2.py3-none-any.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: importlib-metadata!=4.7.0,<8,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (7.1.0)\n",
            "Requirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.6)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.7.1)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: pyarrow<16,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (14.0.2)\n",
            "Requirement already satisfied: pytz<2025 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2023.4)\n",
            "Collecting querystring-parser<2 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl.metadata (559 bytes)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.0.30)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.5.0)\n",
            "Requirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.10/dist-packages (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.1.4)\n",
            "Collecting gunicorn<23 (from mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading gunicorn-22.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2024.1)\n",
            "Collecting monotonic>=1.5 (from posthog->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from quantulum3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (7.0.0)\n",
            "Collecting num2words (from quantulum3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (5.9.5)\n",
            "Collecting Mako (from alembic!=1.10.0,<2->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: exceptiongroup>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from cattrs>=22.2->requests-cache<1.0.0->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.2.1)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.2.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading graphql_core-3.2.3-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting aniso8601<10,>=8 (from graphene<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata!=4.7.0,<8,>=3.7.0->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2<4,>=2.11->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<4->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.1.2)\n",
            "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.12.1)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting docopt>=0.6.2 (from num2words->quantulum3->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.14.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[sentencepiece,torch]==4.39.3; extra == \"inference\"->farm-haystack[colab,elasticsearch,inference,metrics,preprocessing]) (1.3.0)\n",
            "Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Downloading prompthub_py-4.0.0-py3-none-any.whl (6.9 kB)\n",
            "Downloading transformers-4.39.3-py3-none-any.whl (8.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.8/8.8 MB 76.1 MB/s eta 0:00:00\n",
            "Downloading elastic_transport-7.16.0-py2.py3-none-any.whl (35 kB)\n",
            "Downloading elasticsearch-7.17.9-py2.py3-none-any.whl (385 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 386.0/386.0 kB 27.5 MB/s eta 0:00:00\n",
            "Downloading Pillow-9.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 74.1 MB/s eta 0:00:00\n",
            "Downloading pydantic-1.10.17-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.1/3.1 MB 69.8 MB/s eta 0:00:00\n",
            "Downloading rapidfuzz-2.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 63.7 MB/s eta 0:00:00\n",
            "Downloading requests_cache-0.9.8-py3-none-any.whl (48 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.7/48.7 kB 4.0 MB/s eta 0:00:00\n",
            "Downloading scikit_learn-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/13.3 MB 74.1 MB/s eta 0:00:00\n",
            "Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 227.1/227.1 kB 19.3 MB/s eta 0:00:00\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 48.1 MB/s eta 0:00:00\n",
            "Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
            "Downloading farm_haystack-1.26.2-py3-none-any.whl (763 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 763.7/763.7 kB 40.7 MB/s eta 0:00:00\n",
            "Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 75.6/75.6 kB 6.9 MB/s eta 0:00:00\n",
            "Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 kB 7.3 MB/s eta 0:00:00\n",
            "Downloading mlflow-2.14.1-py3-none-any.whl (25.8 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 25.8/25.8 MB 55.1 MB/s eta 0:00:00\n",
            "Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.3/41.3 kB 3.5 MB/s eta 0:00:00\n",
            "Downloading quantulum3-0.9.1-py3-none-any.whl (10.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.7/10.7 MB 84.7 MB/s eta 0:00:00\n",
            "Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading accelerate-0.31.0-py3-none-any.whl (309 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 309.4/309.4 kB 23.9 MB/s eta 0:00:00\n",
            "Downloading alembic-1.13.1-py3-none-any.whl (233 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 233.4/233.4 kB 19.2 MB/s eta 0:00:00\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading cattrs-23.2.3-py3-none-any.whl (57 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 5.3 MB/s eta 0:00:00\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 147.8/147.8 kB 13.1 MB/s eta 0:00:00\n",
            "Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 16.6 MB/s eta 0:00:00\n",
            "Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 128.2/128.2 kB 10.6 MB/s eta 0:00:00\n",
            "Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.4/84.4 kB 8.3 MB/s eta 0:00:00\n",
            "Downloading jarowinkler-1.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.2/114.2 kB 11.6 MB/s eta 0:00:00\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 59.9/59.9 kB 4.2 MB/s eta 0:00:00\n",
            "Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 107.0/107.0 kB 10.2 MB/s eta 0:00:00\n",
            "Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.5/130.5 kB 13.4 MB/s eta 0:00:00\n",
            "Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n",
            "Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 104.8 MB/s eta 0:00:00\n",
            "Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 4.3 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 56.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 34.8 MB/s eta 0:00:00\n",
            "Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 39.0 MB/s eta 0:00:00\n",
            "Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 861.6 kB/s eta 0:00:00\n",
            "Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 7.4 MB/s eta 0:00:00\n",
            "Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 12.6 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 7.5 MB/s eta 0:00:00\n",
            "Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 6.2 MB/s eta 0:00:00\n",
            "Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 176.2/176.2 MB 6.7 MB/s eta 0:00:00\n",
            "Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 9.6 MB/s eta 0:00:00\n",
            "Downloading url_normalize-1.4.3-py2.py3-none-any.whl (6.8 kB)\n",
            "Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.9/143.9 kB 14.9 MB/s eta 0:00:00\n",
            "Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.3/143.3 kB 14.9 MB/s eta 0:00:00\n",
            "Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 5.1 MB/s eta 0:00:00\n",
            "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 5.8 MB/s eta 0:00:00\n",
            "Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 202.9/202.9 kB 19.2 MB/s eta 0:00:00\n",
            "Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 4.8 MB/s eta 0:00:00\n",
            "Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 7.6 MB/s eta 0:00:00\n",
            "Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.3/21.3 MB 63.5 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: langdetect, seqeval, docopt\n",
            "  Building wheel for langdetect (setup.py): started\n",
            "  Building wheel for langdetect (setup.py): finished with status 'done'\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993227 sha256=88014388caaf9c91405baf2767c63398da522ce612f81a41440cfc1b3d3da5e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for seqeval (setup.py): started\n",
            "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=bb4a8b21b22c5d9bacee5d36f3c0d386763f264d667e610c62337ad7f6ae4a58\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
            "  Building wheel for docopt (setup.py): started\n",
            "  Building wheel for docopt (setup.py): finished with status 'done'\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=8f4dfa0af424ad482d00388b2b3d42e684b437045b6ab74a073bc8282ec619b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built langdetect seqeval docopt\n",
            "Installing collected packages: sseclient-py, monotonic, events, docopt, appdirs, aniso8601, urllib3, url-normalize, smmap, rank-bm25, querystring-parser, pydantic, pillow, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, num2words, Mako, lazy-imports, langdetect, jarowinkler, h11, gunicorn, graphql-core, deprecated, cattrs, boilerpy3, backoff, scikit-learn, rapidfuzz, opentelemetry-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, httpcore, graphql-relay, gitdb, elasticsearch, elastic-transport, alembic, tiktoken, seqeval, requests-cache, quantulum3, prompthub-py, posthog, opentelemetry-semantic-conventions, nvidia-cusolver-cu12, httpx, graphene, gitpython, docker, tokenizers, opentelemetry-sdk, transformers, mlflow, accelerate, sentence-transformers, farm-haystack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.4\n",
            "    Uninstalling pydantic-2.7.4:\n",
            "      Successfully uninstalled pydantic-2.7.4\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed Mako-1.3.5 accelerate-0.31.0 alembic-1.13.1 aniso8601-9.0.1 appdirs-1.4.4 backoff-2.2.1 boilerpy3-1.0.7 cattrs-23.2.3 deprecated-1.2.14 docker-7.1.0 docopt-0.6.2 elastic-transport-7.16.0 elasticsearch-7.17.9 events-0.5 farm-haystack-1.26.2 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 gunicorn-22.0.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jarowinkler-1.2.3 langdetect-1.0.9 lazy-imports-0.3.1 mlflow-2.14.1 monotonic-1.6 num2words-0.5.13 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 opentelemetry-api-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 pillow-9.0.0 posthog-3.5.0 prompthub-py-4.0.0 pydantic-1.10.17 quantulum3-0.9.1 querystring-parser-1.2.4 rank-bm25-0.2.2 rapidfuzz-2.7.0 requests-cache-0.9.8 scikit-learn-1.5.0 sentence-transformers-3.0.1 seqeval-1.2.2 smmap-5.0.1 sseclient-py-1.8.0 tiktoken-0.7.0 tokenizers-0.15.2 transformers-4.39.3 url-normalize-1.4.3 urllib3-1.26.19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "pip install --upgrade pip\n",
        "pip install farm-haystack[colab,preprocessing,elasticsearch,metrics,inference]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b6DmRTVKAYF"
      },
      "outputs": [],
      "source": [
        "from haystack.telemetry import tutorial_running\n",
        "\n",
        "tutorial_running(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "s9m14THwKAYF"
      },
      "source": [
        "Set the logging level to INFO:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNCLGO_pKAYG"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
        "logging.getLogger(\"haystack\").setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kag2xA2lCO9t"
      },
      "outputs": [],
      "source": [
        "# Here are some imports that we'll need\n",
        "\n",
        "from haystack.nodes import DensePassageRetriever\n",
        "from haystack.utils import fetch_archive_from_http\n",
        "from haystack.document_stores import InMemoryDocumentStore"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x5BMha8K_aZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0b82c49-1d81-47c3-8748-aeecfdd43a18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHQ8wIoKCQmD"
      },
      "outputs": [],
      "source": [
        "# Variabel untuk fine-tuning\n",
        "doc_dir = \"/content\"\n",
        "train_filename = \"DPRtrain.json\"\n",
        "dev_filename = \"DPRuji.json\"\n",
        "\n",
        "query_model = \"firqaaa/indo-dpr-question_encoder-single-squad-base\"\n",
        "passage_model = \"firqaaa/indo-dpr-ctx_encoder-single-squad-base\"\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/models\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqyVPNQWvX5g"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OihcnS71KAYG"
      },
      "source": [
        "## Start an Elasticsearch server\n",
        "\n",
        "You can start Elasticsearch on your local machine instance using Docker:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zA1W7L75KAYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e9866ad-1afc-4316-81f6-404fbbab96f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:haystack.utils.doc_store:Tried to start Elasticsearch through Docker but this failed. It is likely that there is already an existing Elasticsearch instance running. \n"
          ]
        }
      ],
      "source": [
        "# Recommended: Start Elasticsearch using Docker via the Haystack utility function\n",
        "from haystack.utils import launch_es\n",
        "\n",
        "launch_es()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LSydGBPKAYH"
      },
      "source": [
        "If Docker is not readily available in your environment (eg., in Colab notebooks), then you can manually download and execute Elasticsearch from source:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5HQtje7KAYH"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.9.2-linux-x86_64.tar.gz -q\n",
        "tar -xzf elasticsearch-7.9.2-linux-x86_64.tar.gz\n",
        "chown -R daemon:daemon elasticsearch-7.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVtVha5dKAYH"
      },
      "outputs": [],
      "source": [
        "%%bash --bg\n",
        "\n",
        "sudo -u daemon -- elasticsearch-7.9.2/bin/elasticsearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xcFKSQHKAYH"
      },
      "source": [
        "Wait 30 seconds only to be sure Elasticsearch is ready before continuing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XuZ8SKTGKAYH"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "time.sleep(30)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "PpATABuW9FSY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B_NEtezLOhu5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from haystack.document_stores import ElasticsearchDocumentStore\n",
        "\n",
        "\n",
        "# make sure these indices do not collide with existing ones, the indices will be wiped clean before data is inserted\n",
        "doc_index = \"docs\"\n",
        "label_index = \"labels\"\n",
        "\n",
        "# Get the host where Elasticsearch is running, default to localhost\n",
        "host = os.environ.get(\"ELASTICSEARCH_HOST\", \"localhost\")\n",
        "\n",
        "# Connect to Elasticsearch\n",
        "document_store = ElasticsearchDocumentStore(\n",
        "    host=host,\n",
        "    username=\"\",\n",
        "    password=\"\",\n",
        "    index=doc_index,\n",
        "    label_index=label_index,\n",
        "    embedding_field=\"emb\",\n",
        "    embedding_dim=768,\n",
        "    excluded_meta_data=[\"emb\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune Model DPR"
      ],
      "metadata": {
        "id": "0phCd8yqihU0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkaN7f7RCxga"
      },
      "outputs": [],
      "source": [
        "# Inisialisasi model DPR versi single-squad\n",
        "\n",
        "retriever = DensePassageRetriever(\n",
        "    document_store=document_store,\n",
        "    query_embedding_model=\"firqaaa/indo-dpr-question_encoder-single-squad-base\",\n",
        "    passage_embedding_model=\"firqaaa/indo-dpr-ctx_encoder-single-squad-base\",\n",
        "    max_seq_len_query=64,\n",
        "    max_seq_len_passage=256,\n",
        "    batch_size=16,\n",
        "    use_gpu=True,\n",
        "    embed_title=True,\n",
        "    use_fast_tokenizers=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 585,
          "referenced_widgets": [
            "df5a0d335c4242b7a67c544cec3154b7",
            "68c72eada4664a6e8b352ffd65821dbe",
            "c5e7c8fd16bb45ad9aa924716d9177ad",
            "0bfd6d15186c4c299286b58a7c57cff2",
            "e0162ed9c2964b72bbba4ed681dc7543",
            "d37549c8a605428d915e051c67274d0a",
            "7b8e4110fa424b13ac704a71f8b65718",
            "f9feda274ee54631b42103bfe607d4ab",
            "fd61b37f72ea44449a50f1985cf298f6",
            "49dece008f7e48e7a58531f9bed46476",
            "f35de7f64d3a4df0b3396930d4a08898",
            "4875f7073f9b4f42902a5df0340e0284",
            "713ea84c144a41c8bf6ef53819e7cf6f",
            "f43fcbea373d4c638977ef28f2f76399",
            "91b24a89db7043b58a1dc28f76073f47",
            "9669807cc1304a81bf383e43cd5d4e39",
            "83b28ad04d9047019522e3ec82b79d5e",
            "1474f785299b4c67aec4cc11e9c30d0e",
            "246bdd7e0eca48bc8e397ef25274a760",
            "6ef221eea1584c81a96efa481df65a5b",
            "7583a520733145da9d23308699e1c778",
            "51bd02a964f140049817c3dec28d6b16",
            "02e4717a0bc44b07ac019e9005b6ad42",
            "12af0414e4d24a85b5d6d0a4266dbf53",
            "7ef9ad3610504796b94f122d640961b6",
            "58d100822c0544acb7f963677587c5b7",
            "7fc7da596d084cc2bb74b43f78b25e9c",
            "c08f94877dc046cfa8d81f20f576c2a8",
            "f1cce9673ad941b4ada834d34b798708",
            "189887150e3f4af299571a33f1f4a656",
            "ba3caab06b0d49f092f1caac67ac047e",
            "1d8d7cdd7ca44270bf49fa5a051a6b62",
            "5a8be239cec34f8bb01f0e7651c35ee6",
            "05b871f2c73c4a21b60eb2024ff2e400",
            "adcbe07438534304a9af1b26829c2915",
            "58adbf112cf743e087b07f4548739b8d",
            "08aeb98c869c457482dbc03c9b960487",
            "e6d94854ebcf439cb96c9b8cb1022883",
            "d0ae7e57a1814c7180209628817375f8",
            "3cdfbd211caf434e9404c81b8d2b3528",
            "d4ea44331f934619a41c1dac7181761c",
            "1710c41452fa40249fa03981be0d3bf9",
            "e93b7002abec4d2ab69dcbdb044ff369",
            "73d9e2617a8f446490ec8f77ec1cf730",
            "92b4bf80ba944a36ba4aa758094f39d6",
            "47d629e70cfa4bcb80b580bc7377f80d",
            "f851a7c3c6d746ff9ab2001aadf107b9",
            "ca3242b9aef941c0aa3b5e1b3ae9a56a",
            "e793f5663a6a45a7a3aec3ce459aa553",
            "344184123bb7470aa924364ad23873ea",
            "6d0f6a7e92ad4924ac0a4516a2a095b5",
            "feac4775fdf84ac99b4bdb4ac0141ec5",
            "66398be3a19c4c539114709f0990de19",
            "bd7014e9217740ce9095e321fce7529a",
            "248f046c45ad456b990d2abfb84e7e01",
            "94722a3f7b9343359281b8f400a0da2f",
            "20c6d7b144604daba81a2eac41595175",
            "c4b8359c6cd64b24af57dd2efe02199c",
            "2fad5bf94115494481b1db0351ebee27",
            "7d416094e86d4dd0ab4484fe68516b31",
            "8af161714f72415fadbfe4e8790374f6",
            "80b2db5a023346ec841718127d866d9b",
            "0f816eb6a4b74436bb0a7e8bdec3d760",
            "7d427a843d9649aeaeff79994cb0b6e5",
            "ef8f1487808c4105b5eac3458e602e65",
            "1514664d72b746458ecdc354f4a6a02b",
            "0eeaf2ba26df404f8cfbed3a2065fa32",
            "6b7c3b0ee3c543e9a9e9d22099efb6ca",
            "536e8d7c69704388986f87d95d92f854",
            "98675d42be2f40f5881a0dce71c370dd",
            "85bd700c721440a6afd22086f9e41792",
            "ae3538b64d1e4ee1b2aebec5b0c919f8",
            "c153180382414971866e529f27905643",
            "148d3bb20f704125a692b129db9a3791",
            "a16ed26372d449f7875e2b22ba838edc",
            "cac3d11335a749ee9a1541bb01ae3ff8",
            "a3c6c5bf202442b7b6c1c755f5f6ffcd",
            "3489928094e64c13b8879397c94d3f8d",
            "e65b19a21fcc453ea4f96a9778884b8d",
            "9d0bd28174c94b058a4c5faf5f8c31e6",
            "9f9201dee31748e5929e6950ad1520d2",
            "d6bfbb3740e4495f897ffb1e644a0a02",
            "19f6825899394831a93b60144512fcd7",
            "4be276baa5bf414ba29aa82f9ca519a4",
            "92838225bb5247a0b280d903967c9ce1",
            "7f1a8ae9f57e45e999099dddb11976a8",
            "843fd818568241339614cb21514b7871",
            "c7b9fdbf0d3b4c94bb890a6a60802f75",
            "7cbe7a1e572d4bcb9608809f34848a0d",
            "a798ac81743e41f4ae577da38946bc30",
            "bb7edf79b2944c9e94b4ae70e9e45722",
            "3e24a0e4385f4382802446a8d08c427d",
            "a6a2da912ce34e5393d6eae36e3058ab",
            "935891f54bca4e01aff37efc779599e2",
            "ecad8bab007d42e58fefdb8d4e2ccd24",
            "fd31981dc0414be79eb6e83e398cc2b4",
            "4b90cbf3c48e411cbdd13f6d49275451",
            "2bced16c8393467a9f47f91aba9b0a6a",
            "8e48799a2e124e9987d0ed85f7b614a5",
            "69cae78cfa7f4423aaeedd100c2d2b36",
            "14762d145a9a45069bad0587b37e3525",
            "470fb229ca8d45848041ea7a28b6276a",
            "addfced7b5684cd69c8b350b03ed72e5",
            "8919638e0ab748da8c7b278542746f6c",
            "387900f34339448e93a8b4c5d0784553",
            "366d0eedcd684107a3d38a93774f6ce4",
            "430c2b55ebce410783449190e33ca96a",
            "59b1221fbffd4f6cbb81b99656d05ac3",
            "e4081912d0714582b15cd5827e1f2f9a",
            "ed9c91b94a3c407bba1e2748abf24c5d",
            "a80f4dcf5af94e28a8736847ba71ba3a",
            "bb9d01d25798498d974797c6be73c4fd",
            "f340a18cc3fe46069d022580dc5c0190",
            "fef39631a2ae46d9acb5fab314d1eeaf",
            "dd2e840939bd450bb8cf970773ab2f33",
            "482833cfae25411b8b552c20b667f3b9",
            "56257250a24541e2a9ff8fa583e41dcb",
            "a5a6c915204e499e8e06c4f41870b8cd",
            "c5f1e6bc5474403ab51367e6d6323605",
            "c78ec1732a7146b8a24f96e41f1d4594",
            "cf9fbf80345a4e71b131c635e1d5485d",
            "d53cfbef2e3e4e6597f326813cd80c35",
            "cc482560f89d411089d688f583d514d2",
            "684b4a92dda34978a770b08eda3834c9",
            "9b7a55b5a1074688a1670a5b02c44988",
            "f1a459bf77a6491c920fa6fc8e3a9a59",
            "99778f57b0504f69a19a53455804503e",
            "e00d9e45d2704eee9395902ff731bbfd",
            "42c0ee07480841a4a05a0fbec4a0ee5b",
            "36e7d27c035a42388c98ba5e86692512",
            "091d7803d934429491c0b9b7ccdd7e3d",
            "b1e3ee3d13ae4f7aa16a019a26d0a867"
          ]
        },
        "id": "eahoRD28-YK9",
        "outputId": "392ac420-764e-459d-966b-a9343ba5adf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/507 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df5a0d335c4242b7a67c544cec3154b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/230k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4875f7073f9b4f42902a5df0340e0284"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02e4717a0bc44b07ac019e9005b6ad42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "05b871f2c73c4a21b60eb2024ff2e400"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/679 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92b4bf80ba944a36ba4aa758094f39d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94722a3f7b9343359281b8f400a0da2f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.model.language_model:Auto-detected model language: english\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eeaf2ba26df404f8cfbed3a2065fa32"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/230k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3489928094e64c13b8879397c94d3f8d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/733k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cbe7a1e572d4bcb9608809f34848a0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "69cae78cfa7f4423aaeedd100c2d2b36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a80f4dcf5af94e28a8736847ba71ba3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d53cfbef2e3e4e6597f326813cd80c35"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.model.language_model:Auto-detected model language: english\n"
          ]
        }
      ],
      "source": [
        "# Inisialisasi model DPR versi multiset\n",
        "\n",
        "retriever2 = DensePassageRetriever(\n",
        "    document_store=document_store,\n",
        "    query_embedding_model=\"firqaaa/indo-dpr-question_encoder-multiset-base\",\n",
        "    passage_embedding_model=\"firqaaa/indo-dpr-ctx_encoder-multiset-base\",\n",
        "    max_seq_len_query=64,\n",
        "    max_seq_len_passage=256,\n",
        "    use_gpu=True,\n",
        "    embed_title=True,\n",
        "    use_fast_tokenizers=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atnmYTaSmpYh"
      },
      "outputs": [],
      "source": [
        "# Inisialisasi torch\n",
        "import torch.distributed as dist\n",
        "import os\n",
        "\n",
        "os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
        "os.environ['MASTER_PORT'] = '29500'\n",
        "\n",
        "dist.init_process_group(\"gloo\", rank=2, world_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Eksperimen fine-tune"
      ],
      "metadata": {
        "id": "FhCzvEv_kP53"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVIRfs7kDzzW",
        "outputId": "eb91a05b-d2fb-48ce-da4b-166e770009a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|\n",
            " (o)(o)------'\\ _ /     ( )\n",
            " \n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING TRAIN DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:==================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading train set from: /content/DPRtrain.json \n",
            "Preprocessing dataset: 100%|██████████| 3/3 [00:02<00:00,  1.20 Dicts/s]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING DEV DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:=================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading dev set from: /content/DPRuji.json\n",
            "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  1.48 Dicts/s]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING TEST DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:=================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading test set from: /content/DPRuji.json\n",
            "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  1.91 Dicts/s]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:DATASETS SUMMARY\n",
            "INFO:haystack.modeling.data_handler.data_silo:================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in train: 1033\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in dev  : 259\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in test : 259\n",
            "INFO:haystack.modeling.data_handler.data_silo:Total examples   : 1551\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:Longest query length observed after clipping: 38   - for max_query_len: 64\n",
            "INFO:haystack.modeling.data_handler.data_silo:Average query length after clipping:          13.432720232333011\n",
            "INFO:haystack.modeling.data_handler.data_silo:Proportion queries clipped:                   0.0\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:Longest passage length observed after clipping: 191.5   - for max_passage_len: 256\n",
            "INFO:haystack.modeling.data_handler.data_silo:Average passage length after clipping:          108.39980638915779\n",
            "INFO:haystack.modeling.data_handler.data_silo:Proportion passages clipped:                    0.0\n",
            "INFO:haystack.modeling.model.optimization:Loading optimizer 'AdamW': {'correct_bias': True, 'weight_decay': 0.0, 'eps': 1e-08, 'lr': 1e-05}\n",
            "INFO:haystack.modeling.model.optimization:Using scheduler 'get_linear_schedule_with_warmup'\n",
            "INFO:haystack.modeling.model.optimization:Loading schedule 'get_linear_schedule_with_warmup': '{'num_warmup_steps': 100, 'num_training_steps': 40}'\n",
            "INFO:haystack.modeling.training.base:No train checkpoints found. Starting a new training ...\n",
            "Train epoch 0/4 (Cur. train loss: 0.0000):   0%|          | 0/65 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/haystack/modeling/utils.py:206: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:83.)\n",
            "  all_gather_list._buffer = torch.cuda.ByteTensor(buffer_size)\n",
            "Train epoch 0/4 (Cur. train loss: 23.9745): 100%|██████████| 65/65 [01:32<00:00,  1.42s/it]\n",
            "Train epoch 1/4 (Cur. train loss: 23.8390): 100%|██████████| 65/65 [01:38<00:00,  1.52s/it]\n",
            "Train epoch 2/4 (Cur. train loss: 20.1189): 100%|██████████| 65/65 [01:38<00:00,  1.52s/it]\n",
            "Train epoch 3/4 (Cur. train loss: 20.6057): 100%|██████████| 65/65 [01:38<00:00,  1.52s/it]\n",
            "Train epoch 4/4 (Cur. train loss: 19.5188): 100%|██████████| 65/65 [01:38<00:00,  1.52s/it]\n",
            "Evaluating: 100%|██████████| 17/17 [00:09<00:00,  1.81it/s]\n",
            "INFO:haystack.modeling.evaluation.eval:\n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 325 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "INFO:haystack.modeling.evaluation.eval:\n",
            " _________ text_similarity _________\n",
            "INFO:haystack.modeling.evaluation.eval:loss: 0.23761877221950692\n",
            "INFO:haystack.modeling.evaluation.eval:task_name: text_similarity\n",
            "INFO:haystack.modeling.evaluation.eval:acc: 0.9400730816077953\n",
            "INFO:haystack.modeling.evaluation.eval:f1: 0.05019305019305019\n",
            "INFO:haystack.modeling.evaluation.eval:acc_and_f1: 0.4951330659004228\n",
            "INFO:haystack.modeling.evaluation.eval:average_rank: 12.656370656370656\n",
            "INFO:haystack.modeling.evaluation.eval:report: \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "hard_negative     0.9691    0.9691    0.9691      7951\n",
            "     positive     0.0502    0.0502    0.0502       259\n",
            "\n",
            "     accuracy                         0.9401      8210\n",
            "    macro avg     0.5096    0.5096    0.5096      8210\n",
            " weighted avg     0.9401    0.9401    0.9401      8210\n",
            "\n",
            "INFO:haystack.modeling.model.biadaptive_model:prediction_head saving\n"
          ]
        }
      ],
      "source": [
        "# Training model\n",
        "\n",
        "retriever.train(\n",
        "    data_dir=doc_dir,\n",
        "    train_filename=train_filename,\n",
        "    dev_filename=dev_filename,\n",
        "    test_filename=dev_filename,\n",
        "    n_epochs=5,\n",
        "    batch_size=16,\n",
        "    grad_acc_steps=8,\n",
        "    save_dir=save_dir,\n",
        "    evaluate_every=1000,\n",
        "    embed_title=True,\n",
        "    num_positives=1,\n",
        "    num_hard_negatives=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvKGnWw7qHO2",
        "outputId": "873e36fd-8490-4035-d83a-832d3e536da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|\n",
            " (o)(o)------'\\ _ /     ( )\n",
            " \n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING TRAIN DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:==================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading train set from: /content/DPRtrain.json \n",
            "Preprocessing dataset: 100%|██████████| 3/3 [00:01<00:00,  1.75 Dicts/s]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING DEV DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:=================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading dev set from: /content/DPRuji.json\n",
            "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  2.48 Dicts/s]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING TEST DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:=================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading test set from: /content/DPRuji.json\n",
            "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  2.23 Dicts/s]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:DATASETS SUMMARY\n",
            "INFO:haystack.modeling.data_handler.data_silo:================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in train: 1033\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in dev  : 259\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in test : 259\n",
            "INFO:haystack.modeling.data_handler.data_silo:Total examples   : 1551\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:Longest query length observed after clipping: 38   - for max_query_len: 64\n",
            "INFO:haystack.modeling.data_handler.data_silo:Average query length after clipping:          13.432720232333011\n",
            "INFO:haystack.modeling.data_handler.data_silo:Proportion queries clipped:                   0.0\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:Longest passage length observed after clipping: 216.5   - for max_passage_len: 256\n",
            "INFO:haystack.modeling.data_handler.data_silo:Average passage length after clipping:          108.34365924491772\n",
            "INFO:haystack.modeling.data_handler.data_silo:Proportion passages clipped:                    0.0\n",
            "INFO:haystack.modeling.model.optimization:Loading optimizer 'AdamW': {'correct_bias': True, 'weight_decay': 0.0001, 'eps': 1e-08, 'lr': 1e-06}\n",
            "INFO:haystack.modeling.model.optimization:Using scheduler 'get_linear_schedule_with_warmup'\n",
            "INFO:haystack.modeling.model.optimization:Loading schedule 'get_linear_schedule_with_warmup': '{'num_warmup_steps': 100, 'num_training_steps': 40}'\n",
            "INFO:haystack.modeling.training.base:No train checkpoints found. Starting a new training ...\n",
            "Train epoch 0/4 (Cur. train loss: 1.5642): 100%|██████████| 65/65 [01:40<00:00,  1.54s/it]\n",
            "Train epoch 1/4 (Cur. train loss: 0.8089): 100%|██████████| 65/65 [01:40<00:00,  1.54s/it]\n",
            "Train epoch 2/4 (Cur. train loss: 0.9990): 100%|██████████| 65/65 [01:40<00:00,  1.54s/it]\n",
            "Train epoch 3/4 (Cur. train loss: 1.3651): 100%|██████████| 65/65 [01:40<00:00,  1.54s/it]\n",
            "Train epoch 4/4 (Cur. train loss: 1.4209): 100%|██████████| 65/65 [01:40<00:00,  1.54s/it]\n",
            "Evaluating: 100%|██████████| 17/17 [00:09<00:00,  1.79it/s]\n",
            "INFO:haystack.modeling.evaluation.eval:\n",
            "\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "***************************************************\n",
            "***** EVALUATION | TEST SET | AFTER 325 BATCHES *****\n",
            "***************************************************\n",
            "\\\\|//       \\\\|//      \\\\|//       \\\\|//     \\\\|//\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "\n",
            "INFO:haystack.modeling.evaluation.eval:\n",
            " _________ text_similarity _________\n",
            "INFO:haystack.modeling.evaluation.eval:loss: 0.06782113997130627\n",
            "INFO:haystack.modeling.evaluation.eval:task_name: text_similarity\n",
            "INFO:haystack.modeling.evaluation.eval:acc: 0.9817295980511571\n",
            "INFO:haystack.modeling.evaluation.eval:f1: 0.7104247104247104\n",
            "INFO:haystack.modeling.evaluation.eval:acc_and_f1: 0.8460771542379337\n",
            "INFO:haystack.modeling.evaluation.eval:average_rank: 0.8455598455598455\n",
            "INFO:haystack.modeling.evaluation.eval:report: \n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "hard_negative     0.9906    0.9906    0.9906      7951\n",
            "     positive     0.7104    0.7104    0.7104       259\n",
            "\n",
            "     accuracy                         0.9817      8210\n",
            "    macro avg     0.8505    0.8505    0.8505      8210\n",
            " weighted avg     0.9817    0.9817    0.9817      8210\n",
            "\n",
            "INFO:haystack.modeling.model.biadaptive_model:prediction_head saving\n"
          ]
        }
      ],
      "source": [
        "# Training model\n",
        "\n",
        "retriever2.train(\n",
        "    data_dir=doc_dir,\n",
        "    train_filename=train_filename,\n",
        "    dev_filename=dev_filename,\n",
        "    test_filename=dev_filename,\n",
        "    n_epochs=5,\n",
        "    batch_size=16,\n",
        "    grad_acc_steps=8,\n",
        "    save_dir=save_dir,\n",
        "    evaluate_every=500,\n",
        "    embed_title=True,\n",
        "    num_positives=1,\n",
        "    num_hard_negatives=1,\n",
        "    learning_rate=0.000001,\n",
        "    weight_decay=0.0001\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjYyEdJUx8Vi"
      },
      "outputs": [],
      "source": [
        "retriever2.train(\n",
        "    data_dir=doc_dir,\n",
        "    train_filename=train_filename,\n",
        "    dev_filename=dev_filename,\n",
        "    test_filename=dev_filename,\n",
        "    n_epochs=5,\n",
        "    batch_size=16,\n",
        "    grad_acc_steps=8,\n",
        "    save_dir=save_dir,\n",
        "    evaluate_every=1000,\n",
        "    embed_title=True,\n",
        "    num_positives=1,\n",
        "    num_hard_negatives=1,\n",
        "    learning_rate=0.000001,\n",
        "    weight_decay=0.0001\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkOx-hcm5lc6",
        "outputId": "31b0bf23-7355-41e6-9afe-55a3eba250fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "Loading data into the data silo ... \n",
            "              ______\n",
            "               |o  |   !\n",
            "   __          |:`_|---'-.\n",
            "  |__|______.-/ _ \\-----.|\n",
            " (o)(o)------'\\ _ /     ( )\n",
            " \n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING TRAIN DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:==================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading train set from: /content/DPRtrain.json \n",
            "Preprocessing dataset: 100%|██████████| 3/3 [00:03<00:00,  1.15s/ Dicts]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING DEV DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:=================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading dev set from: /content/DPRuji.json\n",
            "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  1.47 Dicts/s]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:LOADING TEST DATA\n",
            "INFO:haystack.modeling.data_handler.data_silo:=================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Loading test set from: /content/DPRuji.json\n",
            "Preprocessing dataset: 100%|██████████| 1/1 [00:00<00:00,  1.45 Dicts/s]\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:DATASETS SUMMARY\n",
            "INFO:haystack.modeling.data_handler.data_silo:================\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in train: 1033\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in dev  : 259\n",
            "INFO:haystack.modeling.data_handler.data_silo:Examples in test : 259\n",
            "INFO:haystack.modeling.data_handler.data_silo:Total examples   : 1551\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:Longest query length observed after clipping: 38   - for max_query_len: 64\n",
            "INFO:haystack.modeling.data_handler.data_silo:Average query length after clipping:          13.432720232333011\n",
            "INFO:haystack.modeling.data_handler.data_silo:Proportion queries clipped:                   0.0\n",
            "INFO:haystack.modeling.data_handler.data_silo:\n",
            "INFO:haystack.modeling.data_handler.data_silo:Longest passage length observed after clipping: 194.0   - for max_passage_len: 256\n",
            "INFO:haystack.modeling.data_handler.data_silo:Average passage length after clipping:          107.87318489835431\n",
            "INFO:haystack.modeling.data_handler.data_silo:Proportion passages clipped:                    0.0\n",
            "INFO:haystack.modeling.model.optimization:Loading optimizer 'AdamW': {'correct_bias': True, 'weight_decay': 0.0, 'eps': 1e-08, 'lr': 1e-05}\n",
            "INFO:haystack.modeling.model.optimization:Using scheduler 'get_linear_schedule_with_warmup'\n",
            "INFO:haystack.modeling.model.optimization:Loading schedule 'get_linear_schedule_with_warmup': '{'num_warmup_steps': 100, 'num_training_steps': 40}'\n",
            "INFO:haystack.modeling.training.base:No train checkpoints found. Starting a new training ...\n",
            "Train epoch 0/4 (Cur. train loss: 0.6373): 100%|██████████| 65/65 [01:30<00:00,  1.39s/it]\n",
            "Train epoch 1/4 (Cur. train loss: 1.1087):  78%|███████▊  | 51/65 [01:13<00:20,  1.45s/it]"
          ]
        }
      ],
      "source": [
        "retriever2.train(\n",
        "    data_dir=doc_dir,\n",
        "    train_filename=train_filename,\n",
        "    dev_filename=dev_filename,\n",
        "    test_filename=dev_filename,\n",
        "    n_epochs=5,\n",
        "    batch_size=16,\n",
        "    grad_acc_steps=8,\n",
        "    save_dir=save_dir,\n",
        "    evaluate_every=3000,\n",
        "    embed_title=True,\n",
        "    num_positives=1,\n",
        "    num_hard_negatives=1,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H9yYymWEW2Y",
        "outputId": "eaf86d7f-b3c2-481e-c9bb-daa485168cd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
            "INFO:haystack.nodes.retriever.dense:DPR model loaded from /content/drive/MyDrive/models\n"
          ]
        }
      ],
      "source": [
        "reloaded_retriever = DensePassageRetriever.load(load_dir=save_dir, document_store=document_store)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pembentukan Hybrid Passage Retrieval"
      ],
      "metadata": {
        "id": "Xst8uyxTlVJG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_Nc6b5mKAYI"
      },
      "source": [
        "## Menyimpan data ke document store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPqDmNcTH6Na"
      },
      "outputs": [],
      "source": [
        "from haystack.utils import fetch_archive_from_http\n",
        "\n",
        "\n",
        "doc_dir = \"/content/drive/MyDrive/dataset/data_pelatihan.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRFsQUAJOhu_",
        "outputId": "fc7753f9-a2a9-4384-cb28-78cff77bafb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "WARNING:haystack.document_stores.utils:No title information found for documents in QA file: /content/drive/MyDrive/dataset/data_pelatihan.json\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1687.17docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2169.84docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1855.07docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1769.00docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1982.19docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2351.07docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1669.71docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1886.78docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1611.33docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2055.02docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1932.86docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2063.11docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1453.33docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 877.65docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1810.23docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1673.70docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1917.83docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1326.05docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1705.00docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1735.33docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1959.96docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1787.09docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1720.39docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2019.40docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1695.35docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1591.16docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1980.31docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1422.28docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1314.42docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1541.46docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1392.53docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1558.06docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1707.78docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1554.02docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1766.02docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1649.35docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1142.24docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 633.10docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1789.38docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1329.41docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1677.05docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1550.57docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1655.21docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1745.44docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2012.62docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1608.25docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1765.28docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1861.65docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2040.03docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1542.02docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1392.53docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1558.06docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 401.45docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2008.77docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 961.56docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1092.84docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1598.44docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1371.14docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1525.76docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1450.31docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 968.89docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1930.19docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 947.01docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2198.27docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1730.32docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1137.59docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1908.24docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1465.00docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2225.09docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1677.05docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1959.04docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2062.10docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 907.07docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2016.49docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1339.61docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2360.33docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1521.88docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1679.06docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2216.86docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1998.24docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1389.30docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1941.81docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2019.40docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1864.96docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1663.09docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1721.09docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1452.82docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1285.02docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1485.24docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1655.21docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1902.18docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1683.78docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1931.97docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1352.13docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2129.09docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1286.99docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1514.19docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1254.28docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1995.39docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2154.24docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1878.33docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1560.96docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1899.59docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1106.97docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1344.33docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1453.83docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1540.89docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1940.91docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 840.37docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1708.47docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 664.18docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1774.99docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1800.13docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 801.97docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2514.57docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1116.69docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1755.67docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1562.12docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1185.17docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1829.18docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1961.79docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1815.72docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1614.44docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1681.08docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1449.31docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1594.19docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1933.75docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2028.19docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1058.37docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2381.77docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1322.71docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1417.95docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1700.85docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1849.34docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1592.98docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1725.34docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1667.72docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1641.61docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1540.89docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1768.26docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1780.26docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1476.87docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1506.57docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1545.43docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1679.74docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1285.02docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2032.12docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1691.25docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1824.40docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1541.46docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1919.59docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1292.15docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2454.24docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2240.55docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1534.13docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1659.14docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2145.42docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1591.16docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1694.67docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1483.13docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1198.72docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1852.61docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1878.33docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2359.00docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1720.39docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2101.35docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2096.10docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2043.01docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1863.31docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2081.54docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1805.55docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1808.67docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1294.14docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 2198.27docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1340.03docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1166.38docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1178.84docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1450.31docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1298.14docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1784.81docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1793.97docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1333.64docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1551.15docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1549.43docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1968.23docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1568.55docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1507.66docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 433.39docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1272.54docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 790.78docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1007.52docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1467.57docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 944.24docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1550.57docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1500.11docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1185.17docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1007.04docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1428.58docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1286.20docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 584.98docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 574.64docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1320.62docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 753.29docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1630.76docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 977.47docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1328.57docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1239.82docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1675.71docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1620.67docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1654.56docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1502.26docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1811.79docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1714.76docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1686.49docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1419.39docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1303.39docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1312.77docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1520.23docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1300.96docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1459.40docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1283.45docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1349.52docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1260.69docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1128.71docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1564.46docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1516.38docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1742.54docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1760.83docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1688.53docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1869.12docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 960.45docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1800.13docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1907.37docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1661.77docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1226.40docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1848.53docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1769.75docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1720.39docs/s]\n",
            "Preprocessing: 100%|██████████| 1/1 [00:00<00:00, 1795.51docs/s]\n",
            "WARNING:haystack.document_stores.utils:Could not convert an answer for 2 questions.\n",
            "There were conversion errors for question ids: [1544, 1579]\n"
          ]
        }
      ],
      "source": [
        "from haystack.nodes import PreProcessor\n",
        "\n",
        "# Add evaluation data to Elasticsearch Document Store\n",
        "# We first delete the custom tutorial indices to not have duplicate elements\n",
        "# and also split our documents into shorter passages using the PreProcessor\n",
        "preprocessor = PreProcessor(\n",
        "    split_respect_sentence_boundary=False,\n",
        "    clean_empty_lines=False,\n",
        "    clean_whitespace=False,\n",
        ")\n",
        "document_store.delete_documents(index=doc_index)\n",
        "document_store.delete_documents(index=label_index)\n",
        "\n",
        "# The add_eval_data() method converts the given dataset in json format into Haystack document and label objects. Those objects are then indexed in their respective document and label index in the document store. The method can be used with any dataset in SQuAD format.\n",
        "document_store.add_eval_data(\n",
        "    filename=doc_dir,\n",
        "    doc_index=doc_index,\n",
        "    label_index=label_index,\n",
        "    preprocessor=preprocessor,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy8YwmSYOhvE"
      },
      "source": [
        "## Inisialisasi dua komponen retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkhaPMIJOhvF",
        "outputId": "9bf0a38e-6454-4601-883d-84022551a92a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
            "INFO:haystack.nodes.retriever.dense:DPR model loaded from /content/drive/MyDrive/models\n"
          ]
        }
      ],
      "source": [
        "# Initialize Retriever\n",
        "from haystack.nodes import BM25Retriever, DensePassageRetriever\n",
        "\n",
        "sparse_retriever = BM25Retriever(document_store=document_store)\n",
        "\n",
        "# Alternative: Evaluate dense retrievers (EmbeddingRetriever or DensePassageRetriever)\n",
        "# The EmbeddingRetriever uses a single transformer based encoder model for query and document.\n",
        "# In contrast, DensePassageRetriever uses two separate encoders for both.\n",
        "\n",
        "# Please make sure the \"embedding_dim\" parameter in the DocumentStore above matches the output dimension of your models!\n",
        "# Please also take care that the PreProcessor splits your files into chunks that can be completely converted with\n",
        "#        the max_seq_len limitations of Transformers\n",
        "# The SentenceTransformer model \"sentence-transformers/multi-qa-mpnet-base-dot-v1\" generally works well with the EmbeddingRetriever on any kind of English text.\n",
        "# For more information and suggestions on different models check out the documentation at: https://www.sbert.net/docs/pretrained_models.html\n",
        "\n",
        "# from haystack.retriever import EmbeddingRetriever, DensePassageRetriever\n",
        "# retriever = EmbeddingRetriever(document_store=document_store,\n",
        "#                                embedding_model=\"sentence-transformers/multi-qa-mpnet-base-dot-v1\")\n",
        "dense_retriever = DensePassageRetriever.load(load_dir=save_dir, document_store=document_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQUts6xrQ8mV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e48e811-f32d-4bae-bdcc-dd44403e3f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.document_stores.search_engine:Updating embeddings for all 246 docs ...\n",
            "Updating embeddings:   0%|          | 0/246 [00:00<?, ? Docs/s]\n",
            "Create embeddings:   0%|          | 0/256 [00:00<?, ? Docs/s]\u001b[A\n",
            "Create embeddings:   6%|▋         | 16/256 [00:01<00:20, 11.90 Docs/s]\u001b[A\n",
            "Create embeddings:  12%|█▎        | 32/256 [00:01<00:09, 23.97 Docs/s]\u001b[A\n",
            "Create embeddings:  19%|█▉        | 48/256 [00:01<00:05, 35.29 Docs/s]\u001b[A\n",
            "Create embeddings:  25%|██▌       | 64/256 [00:01<00:04, 45.28 Docs/s]\u001b[A\n",
            "Create embeddings:  31%|███▏      | 80/256 [00:02<00:03, 53.65 Docs/s]\u001b[A\n",
            "Create embeddings:  38%|███▊      | 96/256 [00:02<00:02, 59.99 Docs/s]\u001b[A\n",
            "Create embeddings:  44%|████▍     | 112/256 [00:02<00:02, 65.39 Docs/s]\u001b[A\n",
            "Create embeddings:  50%|█████     | 128/256 [00:02<00:01, 69.38 Docs/s]\u001b[A\n",
            "Create embeddings:  56%|█████▋    | 144/256 [00:02<00:01, 72.41 Docs/s]\u001b[A\n",
            "Create embeddings:  62%|██████▎   | 160/256 [00:03<00:01, 74.59 Docs/s]\u001b[A\n",
            "Create embeddings:  69%|██████▉   | 176/256 [00:03<00:01, 75.56 Docs/s]\u001b[A\n",
            "Create embeddings:  75%|███████▌  | 192/256 [00:03<00:00, 76.89 Docs/s]\u001b[A\n",
            "Create embeddings:  81%|████████▏ | 208/256 [00:03<00:00, 77.58 Docs/s]\u001b[A\n",
            "Create embeddings:  88%|████████▊ | 224/256 [00:03<00:00, 78.00 Docs/s]\u001b[A\n",
            "Create embeddings:  94%|█████████▍| 240/256 [00:04<00:00, 78.23 Docs/s]\u001b[A\n",
            "Updating embeddings: 10000 Docs [00:07, 1260.78 Docs/s]\n"
          ]
        }
      ],
      "source": [
        "document_store.update_embeddings(retriever=dense_retriever, index=doc_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inisialisasi komponen JoinDocument dan Reranker"
      ],
      "metadata": {
        "id": "My0qew2dlx80"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqQgtKUsU829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249,
          "referenced_widgets": [
            "6b24b39c1c004a37849aff336f32785d",
            "c7febcfb713b45968d424b74f88eb7f5",
            "ffa9adb069924de28f32137e7d2d6008",
            "d48320b04a804e1fa2d0502d9a08acba",
            "8535f078f951410aa2e3b7e88dfae7c7",
            "b06c77aae4b344599a06cc2d4f19f327",
            "27892d59dc9a40ad8f8c7394a74e310c",
            "5dfb390d92114b69bab15373638e8538",
            "84c831259add40f89bf23734353e71d6",
            "28c596cda3d348799043036efaf99c97",
            "7261250c64ad48bfa0832510857eb4fd",
            "aa0dade3f3eb43308f19050e1ff58fa0",
            "fe6b0ec2474d4abab6ca3ffe21050bba",
            "2987d0283f694ea89b596da27f38f2b8",
            "6f585adb998a4f3ab3d39b4f052c6ab6",
            "2d67d77022be4db1a5da9433d78fde9d",
            "0efc7daae48d47849c2591fb92875190",
            "c2651a7b3f704b089434559deb69cdd0",
            "4288ceeb308243748edd4628c1b01c8a",
            "55f322be10f34f44a70479e248082b9f",
            "a26b5c0b481a436884353e1f59e7dfe5",
            "20527cc890b04b84a55e75e16cdad242",
            "4c332fd616d74ab19ad1fcef6d5a9b27",
            "10a9e2ec02414fd6bbe1ccfcbfb2c97d",
            "c3765d6f9f884f9fb8bb44076ab86803",
            "9f0cb6084cce40ed950a422db7b07920",
            "5488fd54d19c4061bfed5b28259a95a3",
            "bf6fe06a27564f15b724ed87c6401efe",
            "94f21b27d107449686e6ab0f2b913b24",
            "af1415b095724a398b0e4de1729fd359",
            "d9b617aa248340f69963574d21c98708",
            "010ac7aa91444d3d8d140d602de75a7f",
            "94983402f65342418d740e5b37eff555",
            "c39f99f17195468c8212c62c50264cea",
            "5e41646718c840f68e233737e8c4e6a0",
            "5194ff2d24a0409b8585a9eda2ee1e42",
            "69b431f29a5849eaaf4cf3c99e908719",
            "e19da90867904a4eb35153186ad42675",
            "cfb6030c95fd47d79f083cc60f8e87e7",
            "bd8e1700d8304c2b87f41d858a055996",
            "0fe53c2f5eeb4fd99fe6ff799ea926d8",
            "216653d87322484ebefb805bbb16170f",
            "8a06754355b54a1ba4a6c4f50e7e6d0f",
            "f9ae544bfffe4d07a004df9cf89d26f6",
            "0eb06c6df82041a88e3d68927daca083",
            "fd73c23d9f7341a69571d787233fa153",
            "9e0e05d4d318439882d28d4353771d0f",
            "2f87eaaeb31a4536ab4b3f98501c2085",
            "d440be7ab703438a91d158dbc6bd0c1d",
            "8cb232d4200d4f6192c0c32e5ac5f67b",
            "a4e545fe49e64f88a42079f80a5f0e0b",
            "9d941a7cbb124e98b137132009db236d",
            "dcfba2f46606404ba871185e5bafc877",
            "465b583afb8f44b297af672b9894c209",
            "a5e4a808d2b645a385b8c3ec4ea4a4c0"
          ]
        },
        "outputId": "725843fb-0987-4700-e4e0-a9ea3d4f757c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.modeling.utils:Using devices: CUDA:0 - Number of GPUs: 1\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b24b39c1c004a37849aff336f32785d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/134M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa0dade3f3eb43308f19050e1ff58fa0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c332fd616d74ab19ad1fcef6d5a9b27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c39f99f17195468c8212c62c50264cea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eb06c6df82041a88e3d68927daca083"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from haystack.nodes import JoinDocuments, SentenceTransformersRanker\n",
        "\n",
        "join_documents = JoinDocuments(join_mode=\"concatenate\")\n",
        "rerank = SentenceTransformersRanker(model_name_or_path=\"cross-encoder/ms-marco-MiniLM-L-12-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Membentuk pipeline"
      ],
      "metadata": {
        "id": "RCFhNKoLl4La"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CE2ISzrYVAbJ"
      },
      "outputs": [],
      "source": [
        "from haystack.pipelines import Pipeline\n",
        "\n",
        "pipeline = Pipeline()\n",
        "pipeline.add_node(component=sparse_retriever, name=\"SparseRetriever\", inputs=[\"Query\"])\n",
        "pipeline.add_node(component=dense_retriever, name=\"DenseRetriever\", inputs=[\"Query\"])\n",
        "pipeline.add_node(component=join_documents, name=\"JoinDocuments\", inputs=[\"SparseRetriever\", \"DenseRetriever\"])\n",
        "pipeline.add_node(component=rerank, name=\"ReRanker\", inputs=[\"JoinDocuments\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Proses retrieval"
      ],
      "metadata": {
        "id": "HnV1t76ul-ME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsCZSI0bVEUH"
      },
      "outputs": [],
      "source": [
        "def pretty_print_results(prediction):\n",
        "  for doc in prediction[\"documents\"]:\n",
        "        print(doc.id, \"\\t\", doc.score)\n",
        "        print(doc.content)\n",
        "        print(doc.meta)\n",
        "        print(\"\\n\", \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKQk_40iVHLp"
      },
      "outputs": [],
      "source": [
        "prediction = pipeline.run(\n",
        "    query=\"siapa yang menulis paper bagus tentang data science atau sains data\",\n",
        "    params={\n",
        "        \"SparseRetriever\": {\"top_k\":1},\n",
        "        \"DenseRetriever\": {\"top_k\": 1},\n",
        "        # \"JoinDocuments\": {\"top_k_join\": 15},  # comment for debug\n",
        "        \"JoinDocuments\": {\"top_k_join\": 2}, #uncomment for debug\n",
        "        \"ReRanker\": {\"top_k\": 1, \"debug\":True}\n",
        "    },\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction2 = pipeline.run(\n",
        "    query=\"siapa yang menulis paper bagus tentang data science atau sains data\",\n",
        "    params={\n",
        "        \"SparseRetriever\": {\"top_k\":5},\n",
        "        \"DenseRetriever\": {\"top_k\": 5},\n",
        "        # \"JoinDocuments\": {\"top_k_join\": 15},  # comment for debug\n",
        "        \"JoinDocuments\": {\"top_k_join\": 10}, #uncomment for debug\n",
        "        \"ReRanker\": {\"top_k\": 5, \"debug\":True},\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "XfiZlgkdBCwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print_results(prediction2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMlanLF_BF66",
        "outputId": "aeaafe30-89d1-4b3f-f988-290422694c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9f5a325f0a3e3aa15376fa412342656f-0 \t 0.9995142221450806\n",
            " ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan\n",
            "{'_split_id': 0, '_split_offset': 0, 'document_id': 508}\n",
            "\n",
            " \n",
            "\n",
            "9e10a7173d9194f36bcd731256aefdf3-0 \t 0.9861878752708435\n",
            "data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan\n",
            "{'_split_id': 0, '_split_offset': 0, 'document_id': 509}\n",
            "\n",
            " \n",
            "\n",
            "794bf66c4de77f567a7ee3302ec1020a-0 \t 0.9636837840080261\n",
            " metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data\n",
            "{'_split_id': 0, '_split_offset': 0, 'document_id': 603}\n",
            "\n",
            " \n",
            "\n",
            "30570601e5eefd31969f704213822733-0 \t 0.8871667385101318\n",
            "dalam ritel dan e commerce  sains data dapat digunakan untuk sistem rekomendasi  analisis prediktif  analisis sentimen  dan lainnya secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif sains data memiliki beberapa keuntungan yang signifikan hal ini memungkinkan kita untuk mendapatkan wawasan yang berharga dari data historis menggunakan alat yang kuat dengan menggunakan sains data  bisnis dapat dioptimalkan  keputusan masa depan yang lebih baik dapat dibuat  dan target pelanggan dapat dipilih dengan lebih baik\n",
            "{'_split_id': 0, '_split_offset': 0, 'document_id': 528}\n",
            "\n",
            " \n",
            "\n",
            "173dd459009093b961c71b030bb8376a-0 \t 0.789050281047821\n",
            "data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data\n",
            "{'_split_id': 0, '_split_offset': 0, 'document_id': 511}\n",
            "\n",
            " \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm9HcCh3YRrl",
        "outputId": "10bd4a1e-6a35-445e-a20d-989fcd305444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9f5a325f0a3e3aa15376fa412342656f-0 \t 0.9995142221450806\n",
            " ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan\n",
            "{'_split_id': 0, '_split_offset': 0, 'document_id': 508}\n",
            "\n",
            " \n",
            "\n"
          ]
        }
      ],
      "source": [
        "pretty_print_results(prediction)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prediction2[\"_debug\"]"
      ],
      "metadata": {
        "id": "zmTr-4UwYIVc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ca9a1b-a679-4db3-ce17-c168c2a819f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'JoinDocuments': {'input': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9311534358016594, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.795965226591358, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.7764678957495014, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.789050281047821, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>,\n",
              "    <Document: {'content': ' metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data', 'content_type': 'text', 'score': 0.9636837840080261, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 603}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '794bf66c4de77f567a7ee3302ec1020a-0'}>],\n",
              "   'query': 'siapa yang menulis paper bagus tentang data science atau sains data',\n",
              "   'inputs': [{'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9311534358016594, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "      <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.795965226591358, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "      <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.7764678957495014, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "      <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.789050281047821, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>,\n",
              "      <Document: {'content': ' metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data', 'content_type': 'text', 'score': 0.9636837840080261, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 603}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '794bf66c4de77f567a7ee3302ec1020a-0'}>],\n",
              "     'root_node': 'Query',\n",
              "     'params': {'SparseRetriever': {'top_k': 5},\n",
              "      'DenseRetriever': {'top_k': 5},\n",
              "      'JoinDocuments': {'top_k_join': 10},\n",
              "      'ReRanker': {'top_k': 5, 'debug': True}},\n",
              "     'query': 'siapa yang menulis paper bagus tentang data science atau sains data',\n",
              "     'node_id': 'SparseRetriever'},\n",
              "    {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9995142221450806, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "      <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9861878752708435, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "      <Document: {'content': 'dalam ritel dan e commerce  sains data dapat digunakan untuk sistem rekomendasi  analisis prediktif  analisis sentimen  dan lainnya secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif sains data memiliki beberapa keuntungan yang signifikan hal ini memungkinkan kita untuk mendapatkan wawasan yang berharga dari data historis menggunakan alat yang kuat dengan menggunakan sains data  bisnis dapat dioptimalkan  keputusan masa depan yang lebih baik dapat dibuat  dan target pelanggan dapat dipilih dengan lebih baik', 'content_type': 'text', 'score': 0.8871667385101318, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 528}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '30570601e5eefd31969f704213822733-0'}>,\n",
              "      <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.7764678957495014, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "      <Document: {'content': ' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll', 'content_type': 'text', 'score': 0.5000859999991519, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 663}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1921f4f4e5a1fffbb78076282820b849-0'}>],\n",
              "     'root_node': 'Query',\n",
              "     'params': {'SparseRetriever': {'top_k': 5},\n",
              "      'DenseRetriever': {'top_k': 5},\n",
              "      'JoinDocuments': {'top_k_join': 10},\n",
              "      'ReRanker': {'top_k': 5, 'debug': True}},\n",
              "     'query': 'siapa yang menulis paper bagus tentang data science atau sains data',\n",
              "     'node_id': 'DenseRetriever'}],\n",
              "   'top_k_join': 10,\n",
              "   'debug': True},\n",
              "  'output': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9995142221450806, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9861878752708435, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.7764678957495014, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.789050281047821, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>,\n",
              "    <Document: {'content': ' metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data', 'content_type': 'text', 'score': 0.9636837840080261, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 603}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '794bf66c4de77f567a7ee3302ec1020a-0'}>,\n",
              "    <Document: {'content': 'dalam ritel dan e commerce  sains data dapat digunakan untuk sistem rekomendasi  analisis prediktif  analisis sentimen  dan lainnya secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif sains data memiliki beberapa keuntungan yang signifikan hal ini memungkinkan kita untuk mendapatkan wawasan yang berharga dari data historis menggunakan alat yang kuat dengan menggunakan sains data  bisnis dapat dioptimalkan  keputusan masa depan yang lebih baik dapat dibuat  dan target pelanggan dapat dipilih dengan lebih baik', 'content_type': 'text', 'score': 0.8871667385101318, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 528}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '30570601e5eefd31969f704213822733-0'}>,\n",
              "    <Document: {'content': ' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll', 'content_type': 'text', 'score': 0.5000859999991519, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 663}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1921f4f4e5a1fffbb78076282820b849-0'}>],\n",
              "   'labels': None,\n",
              "   'root_node': 'Query'},\n",
              "  'exec_time_ms': 0.72},\n",
              " 'ReRanker': {'input': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9995142221450806, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9861878752708435, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.7764678957495014, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.789050281047821, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>,\n",
              "    <Document: {'content': ' metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data', 'content_type': 'text', 'score': 0.9636837840080261, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 603}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '794bf66c4de77f567a7ee3302ec1020a-0'}>,\n",
              "    <Document: {'content': 'dalam ritel dan e commerce  sains data dapat digunakan untuk sistem rekomendasi  analisis prediktif  analisis sentimen  dan lainnya secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif sains data memiliki beberapa keuntungan yang signifikan hal ini memungkinkan kita untuk mendapatkan wawasan yang berharga dari data historis menggunakan alat yang kuat dengan menggunakan sains data  bisnis dapat dioptimalkan  keputusan masa depan yang lebih baik dapat dibuat  dan target pelanggan dapat dipilih dengan lebih baik', 'content_type': 'text', 'score': 0.8871667385101318, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 528}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '30570601e5eefd31969f704213822733-0'}>,\n",
              "    <Document: {'content': ' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll', 'content_type': 'text', 'score': 0.5000859999991519, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 663}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '1921f4f4e5a1fffbb78076282820b849-0'}>],\n",
              "   'query': 'siapa yang menulis paper bagus tentang data science atau sains data',\n",
              "   'top_k': 5,\n",
              "   'debug': True},\n",
              "  'output': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9995142221450806, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9861878752708435, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': ' metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data', 'content_type': 'text', 'score': 0.9636837840080261, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 603}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '794bf66c4de77f567a7ee3302ec1020a-0'}>,\n",
              "    <Document: {'content': 'dalam ritel dan e commerce  sains data dapat digunakan untuk sistem rekomendasi  analisis prediktif  analisis sentimen  dan lainnya secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif sains data memiliki beberapa keuntungan yang signifikan hal ini memungkinkan kita untuk mendapatkan wawasan yang berharga dari data historis menggunakan alat yang kuat dengan menggunakan sains data  bisnis dapat dioptimalkan  keputusan masa depan yang lebih baik dapat dibuat  dan target pelanggan dapat dipilih dengan lebih baik', 'content_type': 'text', 'score': 0.8871667385101318, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 528}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '30570601e5eefd31969f704213822733-0'}>,\n",
              "    <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.789050281047821, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>]},\n",
              "  'exec_time_ms': 74.85}}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXXvrBVflDUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ade5046-ebed-47a1-fe29-8fa0849e4c0a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Query': {'input': {'debug': True}, 'output': {}, 'exec_time_ms': 0.26},\n",
              " 'SparseRetriever': {'input': {'root_node': 'Query',\n",
              "   'query': 'apa saja disiplin ilmu yang menjadi komponen utama dalam data science',\n",
              "   'top_k': 5,\n",
              "   'debug': True},\n",
              "  'output': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9239535087542956, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9056448177777783, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.8939238395542608, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>,\n",
              "    <Document: {'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'score': 0.9952138662338257, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0'}>]},\n",
              "  'exec_time_ms': 20.41},\n",
              " 'DenseRetriever': {'input': {'root_node': 'Query',\n",
              "   'query': 'apa saja disiplin ilmu yang menjadi komponen utama dalam data science',\n",
              "   'top_k': 5,\n",
              "   'debug': True},\n",
              "  'output': {'documents': [<Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9994422793388367, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9986812472343445, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.9988443851470947, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.9958674907684326, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>,\n",
              "    <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>]},\n",
              "  'exec_time_ms': 41.83},\n",
              " 'JoinDocuments': {'input': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9239535087542956, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9056448177777783, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.8939238395542608, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>,\n",
              "    <Document: {'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'score': 0.9952138662338257, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0'}>],\n",
              "   'query': 'apa saja disiplin ilmu yang menjadi komponen utama dalam data science',\n",
              "   'inputs': [{'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9239535087542956, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "      <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9056448177777783, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "      <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.8939238395542608, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "      <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>,\n",
              "      <Document: {'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'score': 0.9952138662338257, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0'}>],\n",
              "     '_debug': {'Query': {'input': {'debug': True},\n",
              "       'output': {},\n",
              "       'exec_time_ms': 0.26},\n",
              "      'SparseRetriever': {'input': {'root_node': 'Query',\n",
              "        'query': 'apa saja disiplin ilmu yang menjadi komponen utama dalam data science',\n",
              "        'top_k': 5,\n",
              "        'debug': True},\n",
              "       'output': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9239535087542956, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "         <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9056448177777783, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "         <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.8939238395542608, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "         <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>,\n",
              "         <Document: {'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'score': 0.9952138662338257, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0'}>]},\n",
              "       'exec_time_ms': 20.41}},\n",
              "     'root_node': 'Query',\n",
              "     'params': {'SparseRetriever': {'top_k': 5},\n",
              "      'DenseRetriever': {'top_k': 5},\n",
              "      'JoinDocuments': {'top_k_join': 10, 'debug': True},\n",
              "      'ReRanker': {'top_k': 5}},\n",
              "     'query': 'apa saja disiplin ilmu yang menjadi komponen utama dalam data science',\n",
              "     'node_id': 'SparseRetriever'},\n",
              "    {'documents': [<Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9994422793388367, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "      <Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9986812472343445, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "      <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.9988443851470947, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "      <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.9958674907684326, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>,\n",
              "      <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>],\n",
              "     '_debug': {'Query': {'input': {'debug': True},\n",
              "       'output': {},\n",
              "       'exec_time_ms': 0.26},\n",
              "      'DenseRetriever': {'input': {'root_node': 'Query',\n",
              "        'query': 'apa saja disiplin ilmu yang menjadi komponen utama dalam data science',\n",
              "        'top_k': 5,\n",
              "        'debug': True},\n",
              "       'output': {'documents': [<Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9994422793388367, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "         <Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9986812472343445, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "         <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.9988443851470947, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "         <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.9958674907684326, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>,\n",
              "         <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>]},\n",
              "       'exec_time_ms': 41.83}},\n",
              "     'root_node': 'Query',\n",
              "     'params': {'SparseRetriever': {'top_k': 5},\n",
              "      'DenseRetriever': {'top_k': 5},\n",
              "      'JoinDocuments': {'top_k_join': 10, 'debug': True},\n",
              "      'ReRanker': {'top_k': 5}},\n",
              "     'query': 'apa saja disiplin ilmu yang menjadi komponen utama dalam data science',\n",
              "     'node_id': 'DenseRetriever'}],\n",
              "   'top_k_join': 10,\n",
              "   'debug': True},\n",
              "  'output': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9986812472343445, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9994422793388367, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.9988443851470947, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>,\n",
              "    <Document: {'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'score': 0.9952138662338257, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0'}>,\n",
              "    <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.9958674907684326, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>],\n",
              "   'labels': None,\n",
              "   'root_node': 'Query'},\n",
              "  'exec_time_ms': 0.82},\n",
              " 'ReRanker': {'input': {'documents': [<Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9986812472343445, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9994422793388367, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.9988443851470947, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'score': 0.8437412223508911, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 513}, 'id_hash_keys': ['content'], 'embedding': None, 'id': 'be23f3bbb54dcc8351ba879718e153c5-0'}>,\n",
              "    <Document: {'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'score': 0.9952138662338257, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0'}>,\n",
              "    <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.9958674907684326, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>],\n",
              "   'query': 'apa saja disiplin ilmu yang menjadi komponen utama dalam data science',\n",
              "   'top_k': 5,\n",
              "   'debug': True},\n",
              "  'output': {'documents': [<Document: {'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'score': 0.9994422793388367, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 509}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9e10a7173d9194f36bcd731256aefdf3-0'}>,\n",
              "    <Document: {'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'score': 0.9988443851470947, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 512}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7e12faba726053a02575954d4312d0af-0'}>,\n",
              "    <Document: {'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'score': 0.9986812472343445, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 508}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '9f5a325f0a3e3aa15376fa412342656f-0'}>,\n",
              "    <Document: {'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'score': 0.9958674907684326, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 511}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '173dd459009093b961c71b030bb8376a-0'}>,\n",
              "    <Document: {'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'score': 0.9952138662338257, 'meta': {'_split_id': 0, '_split_offset': 0, 'document_id': 510}, 'id_hash_keys': ['content'], 'embedding': None, 'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0'}>]},\n",
              "  'exec_time_ms': 72.84}}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "prediction2[\"_debug\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluasi Hybrid Passage Retrieval"
      ],
      "metadata": {
        "id": "olLCgoLgmFsY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2tBuHN_ZZ0gA"
      },
      "outputs": [],
      "source": [
        "eval_labels = document_store.get_all_labels_aggregated(drop_negative_labels=True, drop_no_answers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jq9FhVujZ33_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae45f7e0-c3f3-498a-ac98-80b39b1400ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<MultiLabel: {'labels': [{'id': '3584e40c-e1f6-4c0a-9cdb-9934f6fe413e', 'query': 'apa tujuan utama data science ', 'document': {'id': '9e10a7173d9194f36bcd731256aefdf3-0', 'content': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 995, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'type': 'extractive', 'score': 0.0, 'context': 'data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan', 'offsets_in_document': [{'start': 124, 'end': 226}], 'offsets_in_context': [{'start': 124, 'end': 226}], 'document_ids': ['9e10a7173d9194f36bcd731256aefdf3-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa tujuan utama data science ', 'filters': None, 'id': '6796d1e57d05b80c25fc85f8f2d7e677', 'no_answer': False, 'answers': ['tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan'], 'offsets_in_documents': [{'start': 124, 'end': 226}], 'offsets_in_contexts': [{'start': 124, 'end': 226}], 'document_ids': ['9e10a7173d9194f36bcd731256aefdf3-0'], 'contexts': ['data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan dalam sains data  ilmu komputer digunakan untuk pengenalan pola  visualisasi  pergudangan data  dan komputasi kinerja tinggi matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd036cd61-3c6c-48de-aa1a-44f32f97074a', 'query': 'Aspek apa saja yang termasuk ke dalam bidang data science?', 'document': {'id': 'be23f3bbb54dcc8351ba879718e153c5-0', 'content': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 997, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain ', 'type': 'extractive', 'score': 0.0, 'context': 'matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan', 'offsets_in_document': [{'start': 490, 'end': 686}], 'offsets_in_context': [{'start': 490, 'end': 686}], 'document_ids': ['be23f3bbb54dcc8351ba879718e153c5-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Aspek apa saja yang termasuk ke dalam bidang data science?', 'filters': None, 'id': '5ed13cde7604d212ece721a30d2edee6', 'no_answer': False, 'answers': ['data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain '], 'offsets_in_documents': [{'start': 490, 'end': 686}], 'offsets_in_contexts': [{'start': 490, 'end': 686}], 'document_ids': ['be23f3bbb54dcc8351ba879718e153c5-0'], 'contexts': ['matematika digunakan untuk pemodelan matematika  sedangkan statistik digunakan untuk pemodelan statistic dan stohestic serta probabilitas dengan menggunakan teori dan teknik dari berbagai bidang  data science membantu mengumpulkan  membersihkan  mengintegrasikan  menganalisis  memvisualisasikan  dan berinteraksi dengan data untuk menghasilkan produk data yang bermanfaat bagi para pengambil keputusan di berbagai industri seperti sains  teknik  ekonomi  politik  keuangan  dan pendidikan data science adalah bidang yang menggabungkan berbagai aspek  termasuk data engineering  metode ilmiah  matematika  statistik  komputasi lanjutan  visualisasi  mindset hacker  dan keahlian domain data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd586cffd-1b62-4c23-8dd6-c2e57cd5110b', 'query': 'Kenapa data science penting dalam era digital saat ini?', 'document': {'id': '7e12faba726053a02575954d4312d0af-0', 'content': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 998, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut ', 'type': 'extractive', 'score': 0.0, 'context': 'data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data', 'offsets_in_document': [{'start': 393, 'end': 549}], 'offsets_in_context': [{'start': 393, 'end': 549}], 'document_ids': ['7e12faba726053a02575954d4312d0af-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Kenapa data science penting dalam era digital saat ini?', 'filters': None, 'id': '54c363aa1a331d6ccc3b40b42629f030', 'no_answer': False, 'answers': ['data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut '], 'offsets_in_documents': [{'start': 393, 'end': 549}], 'offsets_in_contexts': [{'start': 393, 'end': 549}], 'document_ids': ['7e12faba726053a02575954d4312d0af-0'], 'contexts': ['data science menggunakan teknik dan pengetahuan dari disiplin ilmu tersebut untuk mengolah data  menganalisis pola  dan membuat prediksi dalam prosesnya  data science menghasilkan output berupa pengetahuan dan wawasan domain  analisis statistik  visualisasi data  rekayasa data  dan kemampuan komputasi lanjutan data science digunakan oleh para profesional yang terlibat dalam bidang tersebut data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '11d10d08-8c87-4252-a7f3-6f0c1304621c', 'query': 'siapa yang menulis paper bagus tentang data science atau sains data ?', 'document': {'id': '9f5a325f0a3e3aa15376fa412342656f-0', 'content': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 994, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'ada paper bagus yang ditulis oleh david donoho  2017 ', 'type': 'extractive', 'score': 0.0, 'context': ' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan', 'offsets_in_document': [{'start': 1, 'end': 54}], 'offsets_in_context': [{'start': 1, 'end': 54}], 'document_ids': ['9f5a325f0a3e3aa15376fa412342656f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'siapa yang menulis paper bagus tentang data science atau sains data ?', 'filters': None, 'id': 'd92e0b923a3d4f9ddd68aef25ef3960a', 'no_answer': False, 'answers': ['ada paper bagus yang ditulis oleh david donoho  2017 '], 'offsets_in_documents': [{'start': 1, 'end': 54}], 'offsets_in_contexts': [{'start': 1, 'end': 54}], 'document_ids': ['9f5a325f0a3e3aa15376fa412342656f-0'], 'contexts': [' ada paper bagus yang ditulis oleh david donoho  2017   seorang profesor statistika dari stanford university yang bergelut dalam sains data donoho mempertegas bahwa tukey  1962  telah mendorong perlunya reformasi statistika dari fokus pada deskripsi dan inferensi menuju akuisisi data dan prediksi inilah yang kemudian dikenal sebagai data science atau sains data  clevaland  2001   yang menjadi istilah pertama kali untuk konsep tersebut data science atau sains data adalah kombinasi dari beberapa disiplin ilmu  seperti ilmu komputer  matematika  dan statistik tujuan utama data science adalah menganalisis data  menemukan pola  dan membuat prediksi di masa depan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd7112b1f-be56-4ae9-9eda-2e1dcb48168d', 'query': 'mengapa jst lapis tunggal cocok untuk pengenalan pola ', 'document': {'id': 'b1083baf0b46be5d2a0fd488420a90d9-0', 'content': ' jaringan syaraf manusia terdiri dari dendrit  akson  sinapsis  dan soma  sedangkan untuk jaringan syaraf tiruan atau disebut juga artificial neural network merupakan sistem pemroses informasi yang memiliki karakteristik mirip dengan jaringan syarat biologi faktor penentu jst ada tiga yaitu pola hubungan antar neuron  fungsi aktivasi  dan metode untuk menentukan bobot penghubung sebuah jst memiliki sejumlah neuron yang tersusun pada sejumlah layer atau lapisan sebagai berikut  satu layer input berfungsi menerima masukan  satu layer output untuk menghasilkan output dan layer tersembunyi yaitu layer di antara layer input dan output ada dua jenis jst berdasarkan layer yakni single layer dan multi layer sedangkan jenis jst yang berdasarkan aliran sinyal juga memiliki dua jenis yakni feedfoward dan recurrent karakterisik jst lapis tunggal cocok untuk pengenalan pola karena semua unit dihubungkan dengan unit output  unit input output dalam satu lapisan  layer  tidak saling berhubungan  bobot wji    bobot hubungan unit input i dengan unit input j  bobot saling independen', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1035, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'karena semua unit dihubungkan dengan unit output  ', 'type': 'extractive', 'score': 0.0, 'context': ' jaringan syaraf manusia terdiri dari dendrit  akson  sinapsis  dan soma  sedangkan untuk jaringan syaraf tiruan atau disebut juga artificial neural network merupakan sistem pemroses informasi yang memiliki karakteristik mirip dengan jaringan syarat biologi faktor penentu jst ada tiga yaitu pola hubungan antar neuron  fungsi aktivasi  dan metode untuk menentukan bobot penghubung sebuah jst memiliki sejumlah neuron yang tersusun pada sejumlah layer atau lapisan sebagai berikut  satu layer input berfungsi menerima masukan  satu layer output untuk menghasilkan output dan layer tersembunyi yaitu layer di antara layer input dan output ada dua jenis jst berdasarkan layer yakni single layer dan multi layer sedangkan jenis jst yang berdasarkan aliran sinyal juga memiliki dua jenis yakni feedfoward dan recurrent karakterisik jst lapis tunggal cocok untuk pengenalan pola karena semua unit dihubungkan dengan unit output  unit input output dalam satu lapisan  layer  tidak saling berhubungan  bobot wji    bobot hubungan unit input i dengan unit input j  bobot saling independen', 'offsets_in_document': [{'start': 874, 'end': 924}], 'offsets_in_context': [{'start': 874, 'end': 924}], 'document_ids': ['b1083baf0b46be5d2a0fd488420a90d9-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'mengapa jst lapis tunggal cocok untuk pengenalan pola ', 'filters': None, 'id': 'c5c77e8668b4173d5e8900ebe5ae1d46', 'no_answer': False, 'answers': ['karena semua unit dihubungkan dengan unit output  '], 'offsets_in_documents': [{'start': 874, 'end': 924}], 'offsets_in_contexts': [{'start': 874, 'end': 924}], 'document_ids': ['b1083baf0b46be5d2a0fd488420a90d9-0'], 'contexts': [' jaringan syaraf manusia terdiri dari dendrit  akson  sinapsis  dan soma  sedangkan untuk jaringan syaraf tiruan atau disebut juga artificial neural network merupakan sistem pemroses informasi yang memiliki karakteristik mirip dengan jaringan syarat biologi faktor penentu jst ada tiga yaitu pola hubungan antar neuron  fungsi aktivasi  dan metode untuk menentukan bobot penghubung sebuah jst memiliki sejumlah neuron yang tersusun pada sejumlah layer atau lapisan sebagai berikut  satu layer input berfungsi menerima masukan  satu layer output untuk menghasilkan output dan layer tersembunyi yaitu layer di antara layer input dan output ada dua jenis jst berdasarkan layer yakni single layer dan multi layer sedangkan jenis jst yang berdasarkan aliran sinyal juga memiliki dua jenis yakni feedfoward dan recurrent karakterisik jst lapis tunggal cocok untuk pengenalan pola karena semua unit dihubungkan dengan unit output  unit input output dalam satu lapisan  layer  tidak saling berhubungan  bobot wji    bobot hubungan unit input i dengan unit input j  bobot saling independen']}>,\n",
              " <MultiLabel: {'labels': [{'id': '4511824c-bbf0-4548-831a-961fbb1bc437', 'query': 'bagaimana cara menerapkan knn imputation dengan tepat?', 'document': {'id': '3a3bb182dc9ebb558b52081bc3cba37d-0', 'content': 'pada knn imputation  sampel baru diperhitungkan dengan menemukan sejumlah sampel dalam training set yang  paling dekat  dengannya  kemudian nilai sampel yang kosong pada kolom tertentu diisi berdasarkan nilai sample terdekat pada kolom tersebut untuk menerapkan knn imputation dengan tepat  sebaiknya knn imputation hanya dihitung pada latih saja  lalu hasil yang didapat baru diterapkan pada data uji untuk mendapatkan hasil terbaik  paramater pada knn imputation juga dapat dieksperimen parameter tersebut seperti ukuran jarak  jumlah tetangga terdekat  dan bobot iterative imputation adalah proses di mana setiap fitur dimodelkan sebagai fungsi dari fitur lainnya  misalnya sebagai masalah regresi untuk memprediksi missing value', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1038, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'untuk menerapkan knn imputation dengan tepat  sebaiknya knn imputation hanya dihitung pada latih saja  lalu hasil yang didapat baru diterapkan pada data uji', 'type': 'extractive', 'score': 0.0, 'context': 'pada knn imputation  sampel baru diperhitungkan dengan menemukan sejumlah sampel dalam training set yang  paling dekat  dengannya  kemudian nilai sampel yang kosong pada kolom tertentu diisi berdasarkan nilai sample terdekat pada kolom tersebut untuk menerapkan knn imputation dengan tepat  sebaiknya knn imputation hanya dihitung pada latih saja  lalu hasil yang didapat baru diterapkan pada data uji untuk mendapatkan hasil terbaik  paramater pada knn imputation juga dapat dieksperimen parameter tersebut seperti ukuran jarak  jumlah tetangga terdekat  dan bobot iterative imputation adalah proses di mana setiap fitur dimodelkan sebagai fungsi dari fitur lainnya  misalnya sebagai masalah regresi untuk memprediksi missing value', 'offsets_in_document': [{'start': 245, 'end': 401}], 'offsets_in_context': [{'start': 245, 'end': 401}], 'document_ids': ['3a3bb182dc9ebb558b52081bc3cba37d-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara menerapkan knn imputation dengan tepat?', 'filters': None, 'id': '95427f9af6f86640b88d2896f131d600', 'no_answer': False, 'answers': ['untuk menerapkan knn imputation dengan tepat  sebaiknya knn imputation hanya dihitung pada latih saja  lalu hasil yang didapat baru diterapkan pada data uji'], 'offsets_in_documents': [{'start': 245, 'end': 401}], 'offsets_in_contexts': [{'start': 245, 'end': 401}], 'document_ids': ['3a3bb182dc9ebb558b52081bc3cba37d-0'], 'contexts': ['pada knn imputation  sampel baru diperhitungkan dengan menemukan sejumlah sampel dalam training set yang  paling dekat  dengannya  kemudian nilai sampel yang kosong pada kolom tertentu diisi berdasarkan nilai sample terdekat pada kolom tersebut untuk menerapkan knn imputation dengan tepat  sebaiknya knn imputation hanya dihitung pada latih saja  lalu hasil yang didapat baru diterapkan pada data uji untuk mendapatkan hasil terbaik  paramater pada knn imputation juga dapat dieksperimen parameter tersebut seperti ukuran jarak  jumlah tetangga terdekat  dan bobot iterative imputation adalah proses di mana setiap fitur dimodelkan sebagai fungsi dari fitur lainnya  misalnya sebagai masalah regresi untuk memprediksi missing value']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'ada2cc51-a400-458d-b8c7-fdf8a6e9ea58', 'query': 'fungsi apakah yang memiliki kekurangan yang sama dengan fungsi sigmoid ', 'document': {'id': '755032e394d1f0ff8c50386c2f8782e9-0', 'content': 'hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai tetapi kekurangannya adalah sama seperti fungsi sigmoid fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1042, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'fungsi tanh ', 'type': 'extractive', 'score': 0.0, 'context': 'hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai tetapi kekurangannya adalah sama seperti fungsi sigmoid fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif', 'offsets_in_document': [{'start': 126, 'end': 138}], 'offsets_in_context': [{'start': 126, 'end': 138}], 'document_ids': ['755032e394d1f0ff8c50386c2f8782e9-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'fungsi apakah yang memiliki kekurangan yang sama dengan fungsi sigmoid ', 'filters': None, 'id': 'abf191974593893a94759470f052a9f2', 'no_answer': False, 'answers': ['fungsi tanh '], 'offsets_in_documents': [{'start': 126, 'end': 138}], 'offsets_in_contexts': [{'start': 126, 'end': 138}], 'document_ids': ['755032e394d1f0ff8c50386c2f8782e9-0'], 'contexts': ['hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai tetapi kekurangannya adalah sama seperti fungsi sigmoid fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b0255fe5-b399-42ab-b8a1-5dfd9e083aee', 'query': 'apa salah satu kekurangan fungsi tanh ', 'document': {'id': '755032e394d1f0ff8c50386c2f8782e9-0', 'content': 'hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai tetapi kekurangannya adalah sama seperti fungsi sigmoid fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1042, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kekurangannya adalah sama seperti fungsi sigmoid', 'type': 'extractive', 'score': 0.0, 'context': 'hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai tetapi kekurangannya adalah sama seperti fungsi sigmoid fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif', 'offsets_in_document': [{'start': 280, 'end': 328}], 'offsets_in_context': [{'start': 280, 'end': 328}], 'document_ids': ['755032e394d1f0ff8c50386c2f8782e9-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa salah satu kekurangan fungsi tanh ', 'filters': None, 'id': '5d2acda3b87f45b55edab3284723d927', 'no_answer': False, 'answers': ['kekurangannya adalah sama seperti fungsi sigmoid'], 'offsets_in_documents': [{'start': 280, 'end': 328}], 'offsets_in_contexts': [{'start': 280, 'end': 328}], 'document_ids': ['755032e394d1f0ff8c50386c2f8782e9-0'], 'contexts': ['hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai tetapi kekurangannya adalah sama seperti fungsi sigmoid fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif']}>,\n",
              " <MultiLabel: {'labels': [{'id': '8b778f0a-66e8-4d05-a468-6197de05db3c', 'query': 'mengapa fungsi aktivasi linier mengubah jaringan saraf menjadi hanya satu lapisan ', 'document': {'id': 'fe5e4ce3306e073e6ae4f2d449c3e15-0', 'content': 'jadi tidak mungkin untuk kembali dan memahami yang bobot di input neuron dapat memberikan prediksi yang lebih baik dan semua lapisan jaringan saraf runtuh menjadi satu  dengan linier fungsi aktivasi  tidak peduli berapa banyak lapisan dalam saraf jaringan  lapisan terakhir akan menjadi fungsi linier dari lapisan pertama  karena kombinasi linear dari fungsi linear tetaplah linear fungsi jadi fungsi aktivasi linier mengubah jaringan saraf menjadi hanya satu lapisan model jaringan saraf modern menggunakan fungsi aktivasi non linier mereka mengizinkan model untuk membuat pemetaan kompleks antara input dan output jaringan  yang penting untuk belajar dan memodelkan data yang kompleks  seperti gambar  kumpulan video  audio  dan data yang tidak linier atau berdimensi tinggi mereka mengizinkan backpropagation karena mereka punya fungsi turunan yang terkait dengan input dan juga memungkinkan  penumpukan  beberapa lapisan neuron untuk membuat jaringan saraf yang dalam', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1043, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' karena kombinasi linear dari fungsi linear tetaplah linear fungsi', 'type': 'extractive', 'score': 0.0, 'context': 'jadi tidak mungkin untuk kembali dan memahami yang bobot di input neuron dapat memberikan prediksi yang lebih baik dan semua lapisan jaringan saraf runtuh menjadi satu  dengan linier fungsi aktivasi  tidak peduli berapa banyak lapisan dalam saraf jaringan  lapisan terakhir akan menjadi fungsi linier dari lapisan pertama  karena kombinasi linear dari fungsi linear tetaplah linear fungsi jadi fungsi aktivasi linier mengubah jaringan saraf menjadi hanya satu lapisan model jaringan saraf modern menggunakan fungsi aktivasi non linier mereka mengizinkan model untuk membuat pemetaan kompleks antara input dan output jaringan  yang penting untuk belajar dan memodelkan data yang kompleks  seperti gambar  kumpulan video  audio  dan data yang tidak linier atau berdimensi tinggi mereka mengizinkan backpropagation karena mereka punya fungsi turunan yang terkait dengan input dan juga memungkinkan  penumpukan  beberapa lapisan neuron untuk membuat jaringan saraf yang dalam', 'offsets_in_document': [{'start': 322, 'end': 388}], 'offsets_in_context': [{'start': 322, 'end': 388}], 'document_ids': ['fe5e4ce3306e073e6ae4f2d449c3e15-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'mengapa fungsi aktivasi linier mengubah jaringan saraf menjadi hanya satu lapisan ', 'filters': None, 'id': '83d50c80a09d13fbdad57b9a5a9b5dac', 'no_answer': False, 'answers': [' karena kombinasi linear dari fungsi linear tetaplah linear fungsi'], 'offsets_in_documents': [{'start': 322, 'end': 388}], 'offsets_in_contexts': [{'start': 322, 'end': 388}], 'document_ids': ['fe5e4ce3306e073e6ae4f2d449c3e15-0'], 'contexts': ['jadi tidak mungkin untuk kembali dan memahami yang bobot di input neuron dapat memberikan prediksi yang lebih baik dan semua lapisan jaringan saraf runtuh menjadi satu  dengan linier fungsi aktivasi  tidak peduli berapa banyak lapisan dalam saraf jaringan  lapisan terakhir akan menjadi fungsi linier dari lapisan pertama  karena kombinasi linear dari fungsi linear tetaplah linear fungsi jadi fungsi aktivasi linier mengubah jaringan saraf menjadi hanya satu lapisan model jaringan saraf modern menggunakan fungsi aktivasi non linier mereka mengizinkan model untuk membuat pemetaan kompleks antara input dan output jaringan  yang penting untuk belajar dan memodelkan data yang kompleks  seperti gambar  kumpulan video  audio  dan data yang tidak linier atau berdimensi tinggi mereka mengizinkan backpropagation karena mereka punya fungsi turunan yang terkait dengan input dan juga memungkinkan  penumpukan  beberapa lapisan neuron untuk membuat jaringan saraf yang dalam']}>,\n",
              " <MultiLabel: {'labels': [{'id': '4d1a36ea-1b4b-40ba-844d-5c6dcf07922e', 'query': 'berapa jumlah layer tersembunyi pada jaringan syaraf tiruan lapis banyak ', 'document': {'id': 'c67a5747c9b9436b4295391c1bd305cb-0', 'content': 'ada dua jenis jst berdasarkan layer yakni single layer dan multi layer sedangkan jenis jst yang berdasarkan aliran sinyal juga memiliki dua jenis yakni feedfoward dan recurrent karakterisik jst lapis tunggal cocok untuk pengenalan pola karena semua unit dihubungkan dengan unit output  unit input output dalam satu lapisan  layer  tidak saling berhubungan  bobot wji    bobot hubungan unit input i dengan unit input j  bobot saling independen sedangkan jst lapis banyak memiliki karakteristik sebagai berikut  ada lapisan lain selain lapisan input dan output  disebut lapisan   layer tersembunyi   jumlah layer tersembunyi bisa 1 atau lebih  unit dalam satu layer tidak saling berhubungan dapat menyelesaikan masalah yang lebih kompleks  dan proses pelatihan lebih kompleks dan lama jaringan recurrent adalah jaringan yang terdapat neuron output yang memberi sinyal pada unit input sering disebut feedback loop fungsi aktivasi adalah  gerbang  matematis di antara input yang memberi makan neuron saat ini dan outputnya menuju lapisan berikutnya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1044, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'bisa 1 atau lebih  unit ', 'type': 'extractive', 'score': 0.0, 'context': 'ada dua jenis jst berdasarkan layer yakni single layer dan multi layer sedangkan jenis jst yang berdasarkan aliran sinyal juga memiliki dua jenis yakni feedfoward dan recurrent karakterisik jst lapis tunggal cocok untuk pengenalan pola karena semua unit dihubungkan dengan unit output  unit input output dalam satu lapisan  layer  tidak saling berhubungan  bobot wji    bobot hubungan unit input i dengan unit input j  bobot saling independen sedangkan jst lapis banyak memiliki karakteristik sebagai berikut  ada lapisan lain selain lapisan input dan output  disebut lapisan   layer tersembunyi   jumlah layer tersembunyi bisa 1 atau lebih  unit dalam satu layer tidak saling berhubungan dapat menyelesaikan masalah yang lebih kompleks  dan proses pelatihan lebih kompleks dan lama jaringan recurrent adalah jaringan yang terdapat neuron output yang memberi sinyal pada unit input sering disebut feedback loop fungsi aktivasi adalah  gerbang  matematis di antara input yang memberi makan neuron saat ini dan outputnya menuju lapisan berikutnya', 'offsets_in_document': [{'start': 623, 'end': 647}], 'offsets_in_context': [{'start': 623, 'end': 647}], 'document_ids': ['c67a5747c9b9436b4295391c1bd305cb-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'berapa jumlah layer tersembunyi pada jaringan syaraf tiruan lapis banyak ', 'filters': None, 'id': '6cfe02711aed8ec381618234b8b1b612', 'no_answer': False, 'answers': ['bisa 1 atau lebih  unit '], 'offsets_in_documents': [{'start': 623, 'end': 647}], 'offsets_in_contexts': [{'start': 623, 'end': 647}], 'document_ids': ['c67a5747c9b9436b4295391c1bd305cb-0'], 'contexts': ['ada dua jenis jst berdasarkan layer yakni single layer dan multi layer sedangkan jenis jst yang berdasarkan aliran sinyal juga memiliki dua jenis yakni feedfoward dan recurrent karakterisik jst lapis tunggal cocok untuk pengenalan pola karena semua unit dihubungkan dengan unit output  unit input output dalam satu lapisan  layer  tidak saling berhubungan  bobot wji    bobot hubungan unit input i dengan unit input j  bobot saling independen sedangkan jst lapis banyak memiliki karakteristik sebagai berikut  ada lapisan lain selain lapisan input dan output  disebut lapisan   layer tersembunyi   jumlah layer tersembunyi bisa 1 atau lebih  unit dalam satu layer tidak saling berhubungan dapat menyelesaikan masalah yang lebih kompleks  dan proses pelatihan lebih kompleks dan lama jaringan recurrent adalah jaringan yang terdapat neuron output yang memberi sinyal pada unit input sering disebut feedback loop fungsi aktivasi adalah  gerbang  matematis di antara input yang memberi makan neuron saat ini dan outputnya menuju lapisan berikutnya']}>,\n",
              " <MultiLabel: {'labels': [{'id': '9aa2160e-7bb6-4bb9-b5fc-7f95f2864fc2', 'query': 'apa saja kegunaan dari metode group by?', 'document': {'id': '5c694f73dd5b6c7bdd3c77dcfe9cfaa0-0', 'content': 'untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data frame  dropna   sebagai drop all records dengan missing values untuk memilih kolom dalam data frame dapat menggunakan dua metode  metode pertama dengan subset data frame menggunakan nama kolom contohnya df  sex   dan metode kedua dengan menggunakan nama kolom contohnya df sex selanjutnya  dalam pandas memiliki data frame groupby method di mana dengan metode  group by  dapat membagi data menjadi grup berdasarkan beberapa kriteria  menghitung statistik  atau menerapkan fungsi  untuk setiap kelompok  dan mirip dengan fungsi dplyr   dalam r  nantinya setelah objek groupby dibuat barulah dapat menghitung berbagai statistik untuk setiap kelompok pada groupby', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1126, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' metode  group by  dapat membagi data menjadi grup berdasarkan beberapa kriteria  menghitung statistik  atau menerapkan fungsi  untuk setiap kelompok  dan mirip dengan fungsi dplyr   dalam r ', 'type': 'extractive', 'score': 0.0, 'context': 'untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data frame  dropna   sebagai drop all records dengan missing values untuk memilih kolom dalam data frame dapat menggunakan dua metode  metode pertama dengan subset data frame menggunakan nama kolom contohnya df  sex   dan metode kedua dengan menggunakan nama kolom contohnya df sex selanjutnya  dalam pandas memiliki data frame groupby method di mana dengan metode  group by  dapat membagi data menjadi grup berdasarkan beberapa kriteria  menghitung statistik  atau menerapkan fungsi  untuk setiap kelompok  dan mirip dengan fungsi dplyr   dalam r  nantinya setelah objek groupby dibuat barulah dapat menghitung berbagai statistik untuk setiap kelompok pada groupby', 'offsets_in_document': [{'start': 753, 'end': 944}], 'offsets_in_context': [{'start': 753, 'end': 944}], 'document_ids': ['5c694f73dd5b6c7bdd3c77dcfe9cfaa0-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa saja kegunaan dari metode group by?', 'filters': None, 'id': 'b449cf328f442902d36aef58e483190b', 'no_answer': False, 'answers': [' metode  group by  dapat membagi data menjadi grup berdasarkan beberapa kriteria  menghitung statistik  atau menerapkan fungsi  untuk setiap kelompok  dan mirip dengan fungsi dplyr   dalam r '], 'offsets_in_documents': [{'start': 753, 'end': 944}], 'offsets_in_contexts': [{'start': 753, 'end': 944}], 'document_ids': ['5c694f73dd5b6c7bdd3c77dcfe9cfaa0-0'], 'contexts': ['untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data frame  dropna   sebagai drop all records dengan missing values untuk memilih kolom dalam data frame dapat menggunakan dua metode  metode pertama dengan subset data frame menggunakan nama kolom contohnya df  sex   dan metode kedua dengan menggunakan nama kolom contohnya df sex selanjutnya  dalam pandas memiliki data frame groupby method di mana dengan metode  group by  dapat membagi data menjadi grup berdasarkan beberapa kriteria  menghitung statistik  atau menerapkan fungsi  untuk setiap kelompok  dan mirip dengan fungsi dplyr   dalam r  nantinya setelah objek groupby dibuat barulah dapat menghitung berbagai statistik untuk setiap kelompok pada groupby']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f5b9a2ab-639d-41e4-93ae-7f68b67ff1c3', 'query': 'bagaimana cara mengukur separasi pada clustering ', 'document': {'id': '5d05a1f9c28b85f344efcbf954b53dd4-0', 'content': 'kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain separasi didapatkan dengan menghitung rata rata jarak data dengan data data pada cluster lain separasi dinyatakan dalam ukuran between cluster sum of square  ssb cluster yang baik akan mempunyai nilai sse  kohesi  yang rendah dan nilai bss  separasi  yang tinggi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1081, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' separasi dinyatakan dalam ukuran between cluster sum of square  ssb', 'type': 'extractive', 'score': 0.0, 'context': 'kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain separasi didapatkan dengan menghitung rata rata jarak data dengan data data pada cluster lain separasi dinyatakan dalam ukuran between cluster sum of square  ssb cluster yang baik akan mempunyai nilai sse  kohesi  yang rendah dan nilai bss  separasi  yang tinggi', 'offsets_in_document': [{'start': 321, 'end': 389}], 'offsets_in_context': [{'start': 321, 'end': 389}], 'document_ids': ['5d05a1f9c28b85f344efcbf954b53dd4-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mengukur separasi pada clustering ', 'filters': None, 'id': '198fff129e39d6c663a0d42e2f8ecf41', 'no_answer': False, 'answers': [' separasi dinyatakan dalam ukuran between cluster sum of square  ssb'], 'offsets_in_documents': [{'start': 321, 'end': 389}], 'offsets_in_contexts': [{'start': 321, 'end': 389}], 'document_ids': ['5d05a1f9c28b85f344efcbf954b53dd4-0'], 'contexts': ['kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain separasi didapatkan dengan menghitung rata rata jarak data dengan data data pada cluster lain separasi dinyatakan dalam ukuran between cluster sum of square  ssb cluster yang baik akan mempunyai nilai sse  kohesi  yang rendah dan nilai bss  separasi  yang tinggi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e7b5d17a-974e-4669-a74f-c0e00c69fabc', 'query': 'bagaimana cara mengevaluasi cluster dengan kohesi dan separasi ', 'document': {'id': '5d05a1f9c28b85f344efcbf954b53dd4-0', 'content': 'kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain separasi didapatkan dengan menghitung rata rata jarak data dengan data data pada cluster lain separasi dinyatakan dalam ukuran between cluster sum of square  ssb cluster yang baik akan mempunyai nilai sse  kohesi  yang rendah dan nilai bss  separasi  yang tinggi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1081, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain separasi didapatkan dengan menghitung rata rata jarak data dengan data data pada cluster lain separasi dinyatakan dalam ukuran between cluster sum of square  ssb', 'type': 'extractive', 'score': 0.0, 'context': 'kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain separasi didapatkan dengan menghitung rata rata jarak data dengan data data pada cluster lain separasi dinyatakan dalam ukuran between cluster sum of square  ssb cluster yang baik akan mempunyai nilai sse  kohesi  yang rendah dan nilai bss  separasi  yang tinggi', 'offsets_in_document': [{'start': 0, 'end': 389}], 'offsets_in_context': [{'start': 0, 'end': 389}], 'document_ids': ['5d05a1f9c28b85f344efcbf954b53dd4-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mengevaluasi cluster dengan kohesi dan separasi ', 'filters': None, 'id': '64705a46d3a0cc67d19441641f10d84b', 'no_answer': False, 'answers': ['kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain separasi didapatkan dengan menghitung rata rata jarak data dengan data data pada cluster lain separasi dinyatakan dalam ukuran between cluster sum of square  ssb'], 'offsets_in_documents': [{'start': 0, 'end': 389}], 'offsets_in_contexts': [{'start': 0, 'end': 389}], 'document_ids': ['5d05a1f9c28b85f344efcbf954b53dd4-0'], 'contexts': ['kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain separasi didapatkan dengan menghitung rata rata jarak data dengan data data pada cluster lain separasi dinyatakan dalam ukuran between cluster sum of square  ssb cluster yang baik akan mempunyai nilai sse  kohesi  yang rendah dan nilai bss  separasi  yang tinggi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2f436cd6-3758-4ee2-a21b-77ec3915d90c', 'query': 'apa ciri dari hasil clustering yang baik ', 'document': {'id': 'def09d3d64a22796128626c114fce3eb-0', 'content': 'tujuannya adalah agar objek objek dalam suatu kelompok serupa  terkait  satu sama lain  dan berbeda  atau tidak terkait  dengan objek objek dalam kelompok lain semakin besar kesamaan  atau homogenitas  dalam suatu kelompok dan semakin besar perbedaan antar kelompok  semakin baik atau lebih jelas pengelompokannya hasil clustering yang baik ditunjukkan dengan memaksimalkan jarak data antar cluster  inter cluster   meminimalkan jarak data dalam cluster  intra cluster clustering pada bidang pemasaran dapat digunakan untuk membantu marketer menemukan kelompok yang berbeda dalam basis pelanggan mereka  dan kemudian menggunakan pengetahuan ini untuk mengembangkan program pemasaran clustering pada bidang keamanan bank internet dapat digunakan untuk penemuan pola penipuan spam', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1082, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'hasil clustering yang baik ditunjukkan dengan memaksimalkan jarak data antar cluster  inter cluster   meminimalkan jarak data dalam cluster  intra cluster ', 'type': 'extractive', 'score': 0.0, 'context': 'tujuannya adalah agar objek objek dalam suatu kelompok serupa  terkait  satu sama lain  dan berbeda  atau tidak terkait  dengan objek objek dalam kelompok lain semakin besar kesamaan  atau homogenitas  dalam suatu kelompok dan semakin besar perbedaan antar kelompok  semakin baik atau lebih jelas pengelompokannya hasil clustering yang baik ditunjukkan dengan memaksimalkan jarak data antar cluster  inter cluster   meminimalkan jarak data dalam cluster  intra cluster clustering pada bidang pemasaran dapat digunakan untuk membantu marketer menemukan kelompok yang berbeda dalam basis pelanggan mereka  dan kemudian menggunakan pengetahuan ini untuk mengembangkan program pemasaran clustering pada bidang keamanan bank internet dapat digunakan untuk penemuan pola penipuan spam', 'offsets_in_document': [{'start': 314, 'end': 469}], 'offsets_in_context': [{'start': 314, 'end': 469}], 'document_ids': ['def09d3d64a22796128626c114fce3eb-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa ciri dari hasil clustering yang baik ', 'filters': None, 'id': '826f71f157df06e6d941f83a4a33ce15', 'no_answer': False, 'answers': ['hasil clustering yang baik ditunjukkan dengan memaksimalkan jarak data antar cluster  inter cluster   meminimalkan jarak data dalam cluster  intra cluster '], 'offsets_in_documents': [{'start': 314, 'end': 469}], 'offsets_in_contexts': [{'start': 314, 'end': 469}], 'document_ids': ['def09d3d64a22796128626c114fce3eb-0'], 'contexts': ['tujuannya adalah agar objek objek dalam suatu kelompok serupa  terkait  satu sama lain  dan berbeda  atau tidak terkait  dengan objek objek dalam kelompok lain semakin besar kesamaan  atau homogenitas  dalam suatu kelompok dan semakin besar perbedaan antar kelompok  semakin baik atau lebih jelas pengelompokannya hasil clustering yang baik ditunjukkan dengan memaksimalkan jarak data antar cluster  inter cluster   meminimalkan jarak data dalam cluster  intra cluster clustering pada bidang pemasaran dapat digunakan untuk membantu marketer menemukan kelompok yang berbeda dalam basis pelanggan mereka  dan kemudian menggunakan pengetahuan ini untuk mengembangkan program pemasaran clustering pada bidang keamanan bank internet dapat digunakan untuk penemuan pola penipuan spam']}>,\n",
              " <MultiLabel: {'labels': [{'id': '06219393-9a9b-46f4-8098-94633a129c71', 'query': 'apa yang dimaksud dengan evaluasi clustering dengan label ', 'document': {'id': '94552c17c57e83d9de87fdeb1216a91f-0', 'content': 'evaluasi tanpa label dilakukan dengan mengukur baik atau tidaknya hasil clustering tanpa menggunakan data luar atau label evaluasi ini disebut juga sebagai unsupervised measure evaluation evaluasi dengan label dilakukan dengan mengukur baik atau tidaknya hasil clustering dengan menggunakan data luar atau label evaluasi ini disebut juga sebagai supervised measure evaluation beberapa metode untuk melakukan evaluasi tanpa label  inner indices unsupervised evaluation measure  di antaranya adalah kohesi dan separasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1088, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'evaluasi dengan label dilakukan dengan mengukur baik atau tidaknya hasil clustering dengan menggunakan data luar atau label ', 'type': 'extractive', 'score': 0.0, 'context': 'evaluasi tanpa label dilakukan dengan mengukur baik atau tidaknya hasil clustering tanpa menggunakan data luar atau label evaluasi ini disebut juga sebagai unsupervised measure evaluation evaluasi dengan label dilakukan dengan mengukur baik atau tidaknya hasil clustering dengan menggunakan data luar atau label evaluasi ini disebut juga sebagai supervised measure evaluation beberapa metode untuk melakukan evaluasi tanpa label  inner indices unsupervised evaluation measure  di antaranya adalah kohesi dan separasi', 'offsets_in_document': [{'start': 188, 'end': 312}], 'offsets_in_context': [{'start': 188, 'end': 312}], 'document_ids': ['94552c17c57e83d9de87fdeb1216a91f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan evaluasi clustering dengan label ', 'filters': None, 'id': '951aa8785ab19ed8e181213845c943cb', 'no_answer': False, 'answers': ['evaluasi dengan label dilakukan dengan mengukur baik atau tidaknya hasil clustering dengan menggunakan data luar atau label '], 'offsets_in_documents': [{'start': 188, 'end': 312}], 'offsets_in_contexts': [{'start': 188, 'end': 312}], 'document_ids': ['94552c17c57e83d9de87fdeb1216a91f-0'], 'contexts': ['evaluasi tanpa label dilakukan dengan mengukur baik atau tidaknya hasil clustering tanpa menggunakan data luar atau label evaluasi ini disebut juga sebagai unsupervised measure evaluation evaluasi dengan label dilakukan dengan mengukur baik atau tidaknya hasil clustering dengan menggunakan data luar atau label evaluasi ini disebut juga sebagai supervised measure evaluation beberapa metode untuk melakukan evaluasi tanpa label  inner indices unsupervised evaluation measure  di antaranya adalah kohesi dan separasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd01ebec1-a37a-4c37-aed6-aa4d04c4aa10', 'query': 'mengapa cluster yang baik bersifat lebih kohesif ', 'document': {'id': 'c8e4066d5d4a6433f09c8f9db9d9650b-0', 'content': 'kohesi adalah ukuran kedekatan data dalam suatu cluster kohesi didapatkan dengan menghitung rata rata jarak data dengan data lain dalam cluster yang sama cluster yang baik mempunyai jarak antar data yang rendah karena diharapkan mempunyai tingkat kesamaan yang tinggi yang berarti jaraknya lebih kecil sehingga dapat disebut lebih kohesif kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1089, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'cluster yang baik mempunyai jarak antar data yang rendah karena diharapkan mempunyai tingkat kesamaan yang tinggi yang berarti jaraknya lebih kecil sehingga dapat disebut lebih kohesif', 'type': 'extractive', 'score': 0.0, 'context': 'kohesi adalah ukuran kedekatan data dalam suatu cluster kohesi didapatkan dengan menghitung rata rata jarak data dengan data lain dalam cluster yang sama cluster yang baik mempunyai jarak antar data yang rendah karena diharapkan mempunyai tingkat kesamaan yang tinggi yang berarti jaraknya lebih kecil sehingga dapat disebut lebih kohesif kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain', 'offsets_in_document': [{'start': 154, 'end': 338}], 'offsets_in_context': [{'start': 154, 'end': 338}], 'document_ids': ['c8e4066d5d4a6433f09c8f9db9d9650b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'mengapa cluster yang baik bersifat lebih kohesif ', 'filters': None, 'id': '2ffd32734fe4295405dca75963a3ac56', 'no_answer': False, 'answers': ['cluster yang baik mempunyai jarak antar data yang rendah karena diharapkan mempunyai tingkat kesamaan yang tinggi yang berarti jaraknya lebih kecil sehingga dapat disebut lebih kohesif'], 'offsets_in_documents': [{'start': 154, 'end': 338}], 'offsets_in_contexts': [{'start': 154, 'end': 338}], 'document_ids': ['c8e4066d5d4a6433f09c8f9db9d9650b-0'], 'contexts': ['kohesi adalah ukuran kedekatan data dalam suatu cluster kohesi didapatkan dengan menghitung rata rata jarak data dengan data lain dalam cluster yang sama cluster yang baik mempunyai jarak antar data yang rendah karena diharapkan mempunyai tingkat kesamaan yang tinggi yang berarti jaraknya lebih kecil sehingga dapat disebut lebih kohesif kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2f585951-f889-4480-b952-09755dd9bb4b', 'query': 'bagaimana cara mengukur kohesi pada hasil clustering ', 'document': {'id': 'c8e4066d5d4a6433f09c8f9db9d9650b-0', 'content': 'kohesi adalah ukuran kedekatan data dalam suatu cluster kohesi didapatkan dengan menghitung rata rata jarak data dengan data lain dalam cluster yang sama cluster yang baik mempunyai jarak antar data yang rendah karena diharapkan mempunyai tingkat kesamaan yang tinggi yang berarti jaraknya lebih kecil sehingga dapat disebut lebih kohesif kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1089, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse ', 'type': 'extractive', 'score': 0.0, 'context': 'kohesi adalah ukuran kedekatan data dalam suatu cluster kohesi didapatkan dengan menghitung rata rata jarak data dengan data lain dalam cluster yang sama cluster yang baik mempunyai jarak antar data yang rendah karena diharapkan mempunyai tingkat kesamaan yang tinggi yang berarti jaraknya lebih kecil sehingga dapat disebut lebih kohesif kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain', 'offsets_in_document': [{'start': 339, 'end': 461}], 'offsets_in_context': [{'start': 339, 'end': 461}], 'document_ids': ['c8e4066d5d4a6433f09c8f9db9d9650b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mengukur kohesi pada hasil clustering ', 'filters': None, 'id': '6beefa4754dc8f822fe80c80bd15d53c', 'no_answer': False, 'answers': ['kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse '], 'offsets_in_documents': [{'start': 339, 'end': 461}], 'offsets_in_contexts': [{'start': 339, 'end': 461}], 'document_ids': ['c8e4066d5d4a6433f09c8f9db9d9650b-0'], 'contexts': ['kohesi adalah ukuran kedekatan data dalam suatu cluster kohesi didapatkan dengan menghitung rata rata jarak data dengan data lain dalam cluster yang sama cluster yang baik mempunyai jarak antar data yang rendah karena diharapkan mempunyai tingkat kesamaan yang tinggi yang berarti jaraknya lebih kecil sehingga dapat disebut lebih kohesif kohesi dapat diukur dengan within cluster sum of square  wcss  atau sering disebut juga sebagai sum of squared error  sse separasi mengukur seberapa berbeda distinct suatu objek pada suatu cluster dengan objek pada cluster lain']}>,\n",
              " <MultiLabel: {'labels': [{'id': '711cfbdf-18ca-4120-9681-565115856bc0', 'query': 'apa input dari analysis phase ', 'document': {'id': '1a0e471580a0ee31a4a2e314024ddd12-0', 'content': 'informasi ini kemudian digunakan untuk merencanakan pendekatan proyek dasar dan untuk melakukan studi kelayakan produk di bidang ekonomi  operasional dan teknis perencanaan untuk persyaratan jaminan kualitas dan identifikasi risiko yang terkait dengan proyek juga dilakukan dalam tahap perencanaan hasil dari studi kelayakan teknis adalah untuk menentukan berbagai pendekatan teknis yang dapat diikuti untuk mengimplementasikan proyek dengan sukses dengan risiko minimum outputnya studi kelayakan teknis   inisiasi proyek analysis phase inputnya studi kelayakan teknis dan pendanaan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1095, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'studi kelayakan teknis dan pendanaan', 'type': 'extractive', 'score': 0.0, 'context': 'informasi ini kemudian digunakan untuk merencanakan pendekatan proyek dasar dan untuk melakukan studi kelayakan produk di bidang ekonomi  operasional dan teknis perencanaan untuk persyaratan jaminan kualitas dan identifikasi risiko yang terkait dengan proyek juga dilakukan dalam tahap perencanaan hasil dari studi kelayakan teknis adalah untuk menentukan berbagai pendekatan teknis yang dapat diikuti untuk mengimplementasikan proyek dengan sukses dengan risiko minimum outputnya studi kelayakan teknis   inisiasi proyek analysis phase inputnya studi kelayakan teknis dan pendanaan', 'offsets_in_document': [{'start': 546, 'end': 582}], 'offsets_in_context': [{'start': 546, 'end': 582}], 'document_ids': ['1a0e471580a0ee31a4a2e314024ddd12-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa input dari analysis phase ', 'filters': None, 'id': '7f7ab7749a16c5c107badfa4cdd5ea3a', 'no_answer': False, 'answers': ['studi kelayakan teknis dan pendanaan'], 'offsets_in_documents': [{'start': 546, 'end': 582}], 'offsets_in_contexts': [{'start': 546, 'end': 582}], 'document_ids': ['1a0e471580a0ee31a4a2e314024ddd12-0'], 'contexts': ['informasi ini kemudian digunakan untuk merencanakan pendekatan proyek dasar dan untuk melakukan studi kelayakan produk di bidang ekonomi  operasional dan teknis perencanaan untuk persyaratan jaminan kualitas dan identifikasi risiko yang terkait dengan proyek juga dilakukan dalam tahap perencanaan hasil dari studi kelayakan teknis adalah untuk menentukan berbagai pendekatan teknis yang dapat diikuti untuk mengimplementasikan proyek dengan sukses dengan risiko minimum outputnya studi kelayakan teknis   inisiasi proyek analysis phase inputnya studi kelayakan teknis dan pendanaan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b68615c6-94c3-45af-9eb2-119dd9bdb762', 'query': 'apa fungsi shape dalam membuat data frame pada panda ', 'document': {'id': '3bb756f917cafdf4e2ab196b4f072a19-0', 'content': 'h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1121, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' me return representasi tuple dari dimensi', 'type': 'extractive', 'score': 0.0, 'context': 'h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data', 'offsets_in_document': [{'start': 375, 'end': 417}], 'offsets_in_context': [{'start': 375, 'end': 417}], 'document_ids': ['3bb756f917cafdf4e2ab196b4f072a19-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa fungsi shape dalam membuat data frame pada panda ', 'filters': None, 'id': 'a1037f22a21064c4af617cbc488faf1b', 'no_answer': False, 'answers': [' me return representasi tuple dari dimensi'], 'offsets_in_documents': [{'start': 375, 'end': 417}], 'offsets_in_contexts': [{'start': 375, 'end': 417}], 'document_ids': ['3bb756f917cafdf4e2ab196b4f072a19-0'], 'contexts': ['h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '1db68d24-eea5-4a51-a9d1-4659886e4216', 'query': 'apa fungsi size dalam membuat data frame pada panda ', 'document': {'id': '3bb756f917cafdf4e2ab196b4f072a19-0', 'content': 'h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1121, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mendifinisikan number dari elemen', 'type': 'extractive', 'score': 0.0, 'context': 'h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data', 'offsets_in_document': [{'start': 329, 'end': 362}], 'offsets_in_context': [{'start': 329, 'end': 362}], 'document_ids': ['3bb756f917cafdf4e2ab196b4f072a19-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa fungsi size dalam membuat data frame pada panda ', 'filters': None, 'id': 'ae2b9897c9a9bd111bfbe0727279e21c', 'no_answer': False, 'answers': ['mendifinisikan number dari elemen'], 'offsets_in_documents': [{'start': 329, 'end': 362}], 'offsets_in_contexts': [{'start': 329, 'end': 362}], 'document_ids': ['3bb756f917cafdf4e2ab196b4f072a19-0'], 'contexts': ['h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '1c527a3b-62b1-4eda-bdd8-40a4dc821808', 'query': 'apa maksud dari method std   pada pandas ', 'document': {'id': '3bb756f917cafdf4e2ab196b4f072a19-0', 'content': 'h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1121, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'std   sebagai standard deviation', 'type': 'extractive', 'score': 0.0, 'context': 'h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data', 'offsets_in_document': [{'start': 983, 'end': 1015}], 'offsets_in_context': [{'start': 983, 'end': 1015}], 'document_ids': ['3bb756f917cafdf4e2ab196b4f072a19-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa maksud dari method std   pada pandas ', 'filters': None, 'id': 'b03353655878dd5271212bbfa0b5d079', 'no_answer': False, 'answers': ['std   sebagai standard deviation'], 'offsets_in_documents': [{'start': 983, 'end': 1015}], 'offsets_in_contexts': [{'start': 983, 'end': 1015}], 'document_ids': ['3bb756f917cafdf4e2ab196b4f072a19-0'], 'contexts': ['h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data sedangkan untuk data frame methods dalam pandas  tidak seperti atribut karena methods memiliki kurung  dan semua atribut serta methods dapat terdaftar dengan fungsi dir     dir df untuk contoh methods yaitu head  n    tail  n   sebagai first atau last n rows  describe   sebagai generate descriptive statistics  hanya untuk kolom numeric   max    min   sebagai return max atau min values untuk semua kolom numerik  mean    median   untuk return mean atau median values untuk semua kolom numerik  std   sebagai standard deviation  sample  n   sebagai return random sample dari data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2d9cc381-05c9-4ce9-bf1c-9f10c62d4b31', 'query': 'apa saja yang termasuk dalam komponen kasus kendaraan otonom ', 'document': {'id': '26817bc66ff955e2854a9fecd959f7b2-0', 'content': 'business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1132, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi', 'type': 'extractive', 'score': 0.0, 'context': 'business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'offsets_in_document': [{'start': 300, 'end': 402}], 'offsets_in_context': [{'start': 300, 'end': 402}], 'document_ids': ['26817bc66ff955e2854a9fecd959f7b2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa saja yang termasuk dalam komponen kasus kendaraan otonom ', 'filters': None, 'id': '0971de2b330d00fd442e6e194a2868ef', 'no_answer': False, 'answers': ['komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi'], 'offsets_in_documents': [{'start': 300, 'end': 402}], 'offsets_in_contexts': [{'start': 300, 'end': 402}], 'document_ids': ['26817bc66ff955e2854a9fecd959f7b2-0'], 'contexts': ['business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '8f3cc2f2-f1a2-47f6-83a1-116722eb31d4', 'query': 'apakah masalah yang paling umum yang disebabkan oleh kendaraan otonom ', 'document': {'id': '26817bc66ff955e2854a9fecd959f7b2-0', 'content': 'business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1132, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': '1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'type': 'extractive', 'score': 0.0, 'context': 'business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'offsets_in_document': [{'start': 414, 'end': 680}], 'offsets_in_context': [{'start': 414, 'end': 680}], 'document_ids': ['26817bc66ff955e2854a9fecd959f7b2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apakah masalah yang paling umum yang disebabkan oleh kendaraan otonom ', 'filters': None, 'id': 'a82c3c38f1f1b4a8203df43e568b5559', 'no_answer': False, 'answers': ['1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi'], 'offsets_in_documents': [{'start': 414, 'end': 680}], 'offsets_in_contexts': [{'start': 414, 'end': 680}], 'document_ids': ['26817bc66ff955e2854a9fecd959f7b2-0'], 'contexts': ['business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '61d47c9b-c33e-4dbb-a746-157d7b30f77e', 'query': 'apakah hasil dari sensor merupakan output dalam tahapan desain alur sistem ', 'document': {'id': '26817bc66ff955e2854a9fecd959f7b2-0', 'content': 'business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1132, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan', 'type': 'extractive', 'score': 0.0, 'context': 'business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'offsets_in_document': [{'start': 213, 'end': 299}], 'offsets_in_context': [{'start': 213, 'end': 299}], 'document_ids': ['26817bc66ff955e2854a9fecd959f7b2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apakah hasil dari sensor merupakan output dalam tahapan desain alur sistem ', 'filters': None, 'id': 'bed385f3658043fcab552135fc5bed1d', 'no_answer': False, 'answers': ['inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan'], 'offsets_in_documents': [{'start': 213, 'end': 299}], 'offsets_in_contexts': [{'start': 213, 'end': 299}], 'document_ids': ['26817bc66ff955e2854a9fecd959f7b2-0'], 'contexts': ['business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'edc723a5-2291-48c7-8f5e-7cd8b6fba71d', 'query': 'berapa jumlah komponen yang termasuk dalam kasus kendaraan otonom ', 'document': {'id': '26817bc66ff955e2854a9fecd959f7b2-0', 'content': 'business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1132, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi', 'type': 'extractive', 'score': 0.0, 'context': 'business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi', 'offsets_in_document': [{'start': 300, 'end': 402}], 'offsets_in_context': [{'start': 300, 'end': 402}], 'document_ids': ['26817bc66ff955e2854a9fecd959f7b2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'berapa jumlah komponen yang termasuk dalam kasus kendaraan otonom ', 'filters': None, 'id': '5ea5d287202c44677cfbf2f8f9657259', 'no_answer': False, 'answers': ['komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi'], 'offsets_in_documents': [{'start': 300, 'end': 402}], 'offsets_in_contexts': [{'start': 300, 'end': 402}], 'document_ids': ['26817bc66ff955e2854a9fecd959f7b2-0'], 'contexts': ['business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator inputnya adalah hasil dari sistem sensor untuk mendeteksi lingkungan sekitar kendaraan komponen  deteksi dan klasifikasi objek  pengenalan pengemudi  sensor fusion  pemeliharaan dan privasi masalah   \\n1. salah dalam mengklasifikasikan objek \\n2. pengemudi yang mengantuk\\n3. penggunaan kamera yang dapat mengurangi jarak pandang dan ketajaman visual\\n4. mobil yang tidak bisa dikontrol pemeliharaannya\\n5. sistem navigasi dan kamera yang dapat melanggar privasi pengemudi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3ac62466-37a9-401b-868e-65fc4133c4e0', 'query': 'pendekatan apa yang dilakukan teorema bayes ketika kita memiliki beberapa alternatif hipotesis ', 'document': {'id': 'c967d2283d5767667d4c09d255d1bb56-0', 'content': 'hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1025, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' maxmimum a posteriori  map', 'type': 'extractive', 'score': 0.0, 'context': 'hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan', 'offsets_in_document': [{'start': 1007, 'end': 1034}], 'offsets_in_context': [{'start': 1007, 'end': 1034}], 'document_ids': ['c967d2283d5767667d4c09d255d1bb56-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'pendekatan apa yang dilakukan teorema bayes ketika kita memiliki beberapa alternatif hipotesis ', 'filters': None, 'id': 'bca250ef3f8b4bb66d9ca7057cf12ce3', 'no_answer': False, 'answers': [' maxmimum a posteriori  map'], 'offsets_in_documents': [{'start': 1007, 'end': 1034}], 'offsets_in_contexts': [{'start': 1007, 'end': 1034}], 'document_ids': ['c967d2283d5767667d4c09d255d1bb56-0'], 'contexts': ['hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e12a4bae-f035-4245-8249-fe3b821f209d', 'query': 'apa yang disebut bukti pengamatan d pada teorema bayes ', 'document': {'id': 'c967d2283d5767667d4c09d255d1bb56-0', 'content': 'hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1025, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku', 'type': 'extractive', 'score': 0.0, 'context': 'hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan', 'offsets_in_document': [{'start': 452, 'end': 564}], 'offsets_in_context': [{'start': 452, 'end': 564}], 'document_ids': ['c967d2283d5767667d4c09d255d1bb56-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang disebut bukti pengamatan d pada teorema bayes ', 'filters': None, 'id': 'cf97c6adb8df4e12368119bd5e41500c', 'no_answer': False, 'answers': ['mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku'], 'offsets_in_documents': [{'start': 452, 'end': 564}], 'offsets_in_contexts': [{'start': 452, 'end': 564}], 'document_ids': ['c967d2283d5767667d4c09d255d1bb56-0'], 'contexts': ['hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '9353d4f3-3b42-4a8d-a844-569cc15c1202', 'query': 'apa saja jenis pendekatan yang digunakan pada bayes learning ', 'document': {'id': 'c967d2283d5767667d4c09d255d1bb56-0', 'content': 'hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1025, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml ', 'type': 'extractive', 'score': 0.0, 'context': 'hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan', 'offsets_in_document': [{'start': 901, 'end': 1006}], 'offsets_in_context': [{'start': 901, 'end': 1006}], 'document_ids': ['c967d2283d5767667d4c09d255d1bb56-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa saja jenis pendekatan yang digunakan pada bayes learning ', 'filters': None, 'id': 'ba5dde2e4139be50b97e6fe08fae2d9c', 'no_answer': False, 'answers': ['terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml '], 'offsets_in_documents': [{'start': 901, 'end': 1006}], 'offsets_in_contexts': [{'start': 901, 'end': 1006}], 'document_ids': ['c967d2283d5767667d4c09d255d1bb56-0'], 'contexts': ['hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h probabilitas awal dari h  p h   menecerminkan pengetahuan awal kita tentang kemungkinan bahwa h adalah hipotesis yang benar  sebelum mengemati data   bukti pengamatan d  p d   mencerminkan probabilitas bahwa data pelatihan d akan diamati tanpa pengetahuan tentang hipotesis h yang berlaku  probabilitas kondisional pengamatan d  p d h   menunjukan probabilitas mengamati data d dalam suatu dunia dimana hipotesis h berlaku  probabilitas posterior h  p h d   mewakili probabilitas bahwa h berlaku berdasarkan data pelatihan yang diamati d  teorema bayes memungkinkan kita menghitung p h d  dengan cara p h d    p d h p h  p d terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '92e74301-b374-4fd2-87ad-c40457e2d008', 'query': 'setelah mengidentifikasi tujuan bisnis maka selanjutnya tugas apa yang dilakukan dalam business understanding ', 'document': {'id': '314138f01aa10fd61a7fa0c661d96895-0', 'content': ' solusi  \\n1. pelatihan model pembelajaran mesin yang lebih baik dengan menggunakan data yang lebih umum \\n2. mengimplementasikan face recognition\\n3. menggabungkan output sensor kamera dan radar serta lidar untuk memperoleh keakuratan yang lebih baik\\n4. menerapkan machine learning ke data yang diambil oleh perangkat yang terpasang dan memberikan indikator yang menunjukkan potensi kesalahan\\n5. dibutuhkan aturan yang meregulasi penggunaan data khusus untuk kendaraan bermotor\\ntugas tugas dalam business understanding \\n1. mengidentifikasi tujuan bisnis  \\na business background   \\n memahami latar belakang bisnis dari project  membuat bagan organisasi dan tanggung jawabnya\\n mengidentifikasi internal sponsor dan problem area\\n memahami motivasi project\\nb business goals   \\n mendeskripsikan masalah \\n menentukan semua pertanyaan bisnis dan manfaat yang diharapkan\\nc business success criteria  \\n dokumentasikan kriteria keberhasilan project\\n setiap tujuan bisnis memiliki kriteria untuk sukses\\n2. menilai dan memahami situasi anda\\na inventaris sumber daya yang ada \\n hardware yang dibutuhkan \\n sumber data dan penyimpanan pengetahuan serta sumber daya personal \\nb', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1133, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': '2. menilai dan memahami situasi anda', 'type': 'extractive', 'score': 0.0, 'context': ' solusi  \\n1. pelatihan model pembelajaran mesin yang lebih baik dengan menggunakan data yang lebih umum \\n2. mengimplementasikan face recognition\\n3. menggabungkan output sensor kamera dan radar serta lidar untuk memperoleh keakuratan yang lebih baik\\n4. menerapkan machine learning ke data yang diambil oleh perangkat yang terpasang dan memberikan indikator yang menunjukkan potensi kesalahan\\n5. dibutuhkan aturan yang meregulasi penggunaan data khusus untuk kendaraan bermotor\\ntugas tugas dalam business understanding \\n1. mengidentifikasi tujuan bisnis  \\na business background   \\n memahami latar belakang bisnis dari project  membuat bagan organisasi dan tanggung jawabnya\\n mengidentifikasi internal sponsor dan problem area\\n memahami motivasi project\\nb business goals   \\n mendeskripsikan masalah \\n menentukan semua pertanyaan bisnis dan manfaat yang diharapkan\\nc business success criteria  \\n dokumentasikan kriteria keberhasilan project\\n setiap tujuan bisnis memiliki kriteria untuk sukses\\n2. menilai dan memahami situasi anda\\na inventaris sumber daya yang ada \\n hardware yang dibutuhkan \\n sumber data dan penyimpanan pengetahuan serta sumber daya personal \\nb', 'offsets_in_document': [{'start': 990, 'end': 1026}], 'offsets_in_context': [{'start': 990, 'end': 1026}], 'document_ids': ['314138f01aa10fd61a7fa0c661d96895-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'setelah mengidentifikasi tujuan bisnis maka selanjutnya tugas apa yang dilakukan dalam business understanding ', 'filters': None, 'id': '5e2dd81293c89ce0c8863ea772ee2494', 'no_answer': False, 'answers': ['2. menilai dan memahami situasi anda'], 'offsets_in_documents': [{'start': 990, 'end': 1026}], 'offsets_in_contexts': [{'start': 990, 'end': 1026}], 'document_ids': ['314138f01aa10fd61a7fa0c661d96895-0'], 'contexts': [' solusi  \\n1. pelatihan model pembelajaran mesin yang lebih baik dengan menggunakan data yang lebih umum \\n2. mengimplementasikan face recognition\\n3. menggabungkan output sensor kamera dan radar serta lidar untuk memperoleh keakuratan yang lebih baik\\n4. menerapkan machine learning ke data yang diambil oleh perangkat yang terpasang dan memberikan indikator yang menunjukkan potensi kesalahan\\n5. dibutuhkan aturan yang meregulasi penggunaan data khusus untuk kendaraan bermotor\\ntugas tugas dalam business understanding \\n1. mengidentifikasi tujuan bisnis  \\na business background   \\n memahami latar belakang bisnis dari project  membuat bagan organisasi dan tanggung jawabnya\\n mengidentifikasi internal sponsor dan problem area\\n memahami motivasi project\\nb business goals   \\n mendeskripsikan masalah \\n menentukan semua pertanyaan bisnis dan manfaat yang diharapkan\\nc business success criteria  \\n dokumentasikan kriteria keberhasilan project\\n setiap tujuan bisnis memiliki kriteria untuk sukses\\n2. menilai dan memahami situasi anda\\na inventaris sumber daya yang ada \\n hardware yang dibutuhkan \\n sumber data dan penyimpanan pengetahuan serta sumber daya personal \\nb']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f2bcffea-2a91-46ed-be7b-d8a6e9ea1048', 'query': 'bagaimana cara mengidentifikasi tujuan bisnis dalam business understanding ', 'document': {'id': '314138f01aa10fd61a7fa0c661d96895-0', 'content': ' solusi  \\n1. pelatihan model pembelajaran mesin yang lebih baik dengan menggunakan data yang lebih umum \\n2. mengimplementasikan face recognition\\n3. menggabungkan output sensor kamera dan radar serta lidar untuk memperoleh keakuratan yang lebih baik\\n4. menerapkan machine learning ke data yang diambil oleh perangkat yang terpasang dan memberikan indikator yang menunjukkan potensi kesalahan\\n5. dibutuhkan aturan yang meregulasi penggunaan data khusus untuk kendaraan bermotor\\ntugas tugas dalam business understanding \\n1. mengidentifikasi tujuan bisnis  \\na business background   \\n memahami latar belakang bisnis dari project  membuat bagan organisasi dan tanggung jawabnya\\n mengidentifikasi internal sponsor dan problem area\\n memahami motivasi project\\nb business goals   \\n mendeskripsikan masalah \\n menentukan semua pertanyaan bisnis dan manfaat yang diharapkan\\nc business success criteria  \\n dokumentasikan kriteria keberhasilan project\\n setiap tujuan bisnis memiliki kriteria untuk sukses\\n2. menilai dan memahami situasi anda\\na inventaris sumber daya yang ada \\n hardware yang dibutuhkan \\n sumber data dan penyimpanan pengetahuan serta sumber daya personal \\nb', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1133, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': '1. mengidentifikasi tujuan bisnis  \\na business background   \\n memahami latar belakang bisnis dari project  membuat bagan organisasi dan tanggung jawabnya\\n mengidentifikasi internal sponsor dan problem area\\n memahami motivasi project\\nb business goals   \\n mendeskripsikan masalah \\n menentukan semua pertanyaan bisnis dan manfaat yang diharapkan\\nc business success criteria  \\n dokumentasikan kriteria keberhasilan project\\n setiap tujuan bisnis memiliki kriteria untuk sukses', 'type': 'extractive', 'score': 0.0, 'context': ' solusi  \\n1. pelatihan model pembelajaran mesin yang lebih baik dengan menggunakan data yang lebih umum \\n2. mengimplementasikan face recognition\\n3. menggabungkan output sensor kamera dan radar serta lidar untuk memperoleh keakuratan yang lebih baik\\n4. menerapkan machine learning ke data yang diambil oleh perangkat yang terpasang dan memberikan indikator yang menunjukkan potensi kesalahan\\n5. dibutuhkan aturan yang meregulasi penggunaan data khusus untuk kendaraan bermotor\\ntugas tugas dalam business understanding \\n1. mengidentifikasi tujuan bisnis  \\na business background   \\n memahami latar belakang bisnis dari project  membuat bagan organisasi dan tanggung jawabnya\\n mengidentifikasi internal sponsor dan problem area\\n memahami motivasi project\\nb business goals   \\n mendeskripsikan masalah \\n menentukan semua pertanyaan bisnis dan manfaat yang diharapkan\\nc business success criteria  \\n dokumentasikan kriteria keberhasilan project\\n setiap tujuan bisnis memiliki kriteria untuk sukses\\n2. menilai dan memahami situasi anda\\na inventaris sumber daya yang ada \\n hardware yang dibutuhkan \\n sumber data dan penyimpanan pengetahuan serta sumber daya personal \\nb', 'offsets_in_document': [{'start': 518, 'end': 989}], 'offsets_in_context': [{'start': 518, 'end': 989}], 'document_ids': ['314138f01aa10fd61a7fa0c661d96895-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mengidentifikasi tujuan bisnis dalam business understanding ', 'filters': None, 'id': '13861c841f33ee81f7eb1c9ca80a744c', 'no_answer': False, 'answers': ['1. mengidentifikasi tujuan bisnis  \\na business background   \\n memahami latar belakang bisnis dari project  membuat bagan organisasi dan tanggung jawabnya\\n mengidentifikasi internal sponsor dan problem area\\n memahami motivasi project\\nb business goals   \\n mendeskripsikan masalah \\n menentukan semua pertanyaan bisnis dan manfaat yang diharapkan\\nc business success criteria  \\n dokumentasikan kriteria keberhasilan project\\n setiap tujuan bisnis memiliki kriteria untuk sukses'], 'offsets_in_documents': [{'start': 518, 'end': 989}], 'offsets_in_contexts': [{'start': 518, 'end': 989}], 'document_ids': ['314138f01aa10fd61a7fa0c661d96895-0'], 'contexts': [' solusi  \\n1. pelatihan model pembelajaran mesin yang lebih baik dengan menggunakan data yang lebih umum \\n2. mengimplementasikan face recognition\\n3. menggabungkan output sensor kamera dan radar serta lidar untuk memperoleh keakuratan yang lebih baik\\n4. menerapkan machine learning ke data yang diambil oleh perangkat yang terpasang dan memberikan indikator yang menunjukkan potensi kesalahan\\n5. dibutuhkan aturan yang meregulasi penggunaan data khusus untuk kendaraan bermotor\\ntugas tugas dalam business understanding \\n1. mengidentifikasi tujuan bisnis  \\na business background   \\n memahami latar belakang bisnis dari project  membuat bagan organisasi dan tanggung jawabnya\\n mengidentifikasi internal sponsor dan problem area\\n memahami motivasi project\\nb business goals   \\n mendeskripsikan masalah \\n menentukan semua pertanyaan bisnis dan manfaat yang diharapkan\\nc business success criteria  \\n dokumentasikan kriteria keberhasilan project\\n setiap tujuan bisnis memiliki kriteria untuk sukses\\n2. menilai dan memahami situasi anda\\na inventaris sumber daya yang ada \\n hardware yang dibutuhkan \\n sumber data dan penyimpanan pengetahuan serta sumber daya personal \\nb']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd17f3570-61f2-4af1-b7b0-f37fdfb8bea3', 'query': 'mengapa dalam proses data preparataion dapat berbeda berbeda atau bervariasi ', 'document': {'id': 'aeb41314a61a23afa46aaff963905d2c-0', 'content': 'proses data preparation dapat bervariasi karena tergantung pada tujuan analisis  keahlian pengguna data  dan pertanyaan yang ingin dijawab aspek dalam data preparation meliputi data completeness  data correctness  data quantity  dan data usability dalam data completeness  penting untuk memastikan apakah data yang ada sudah cukup untuk merepresentasikan pola yang ingin diteliti  dan apakah ada tambahan data yang diperlukan dari sumber lain data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1136, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'karena tergantung pada tujuan analisis  keahlian pengguna data  dan pertanyaan yang ingin dijawab', 'type': 'extractive', 'score': 0.0, 'context': 'proses data preparation dapat bervariasi karena tergantung pada tujuan analisis  keahlian pengguna data  dan pertanyaan yang ingin dijawab aspek dalam data preparation meliputi data completeness  data correctness  data quantity  dan data usability dalam data completeness  penting untuk memastikan apakah data yang ada sudah cukup untuk merepresentasikan pola yang ingin diteliti  dan apakah ada tambahan data yang diperlukan dari sumber lain data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi', 'offsets_in_document': [{'start': 41, 'end': 138}], 'offsets_in_context': [{'start': 41, 'end': 138}], 'document_ids': ['aeb41314a61a23afa46aaff963905d2c-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'mengapa dalam proses data preparataion dapat berbeda berbeda atau bervariasi ', 'filters': None, 'id': '3a64c2e6f8eb31e3f99a95cf0e2c164a', 'no_answer': False, 'answers': ['karena tergantung pada tujuan analisis  keahlian pengguna data  dan pertanyaan yang ingin dijawab'], 'offsets_in_documents': [{'start': 41, 'end': 138}], 'offsets_in_contexts': [{'start': 41, 'end': 138}], 'document_ids': ['aeb41314a61a23afa46aaff963905d2c-0'], 'contexts': ['proses data preparation dapat bervariasi karena tergantung pada tujuan analisis  keahlian pengguna data  dan pertanyaan yang ingin dijawab aspek dalam data preparation meliputi data completeness  data correctness  data quantity  dan data usability dalam data completeness  penting untuk memastikan apakah data yang ada sudah cukup untuk merepresentasikan pola yang ingin diteliti  dan apakah ada tambahan data yang diperlukan dari sumber lain data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e249d3d6-bb8f-46d8-aaf5-e51e54ef3191', 'query': 'apa yang perlu diperhatikan dalam sumber data ', 'document': {'id': 'ee48b9413d49a340c6bf9a9c41e7d150-0', 'content': 'data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll dokumentasi data understanding meliputi beberapa hal  yaitu data collection report  data description report  data exploration report  dan data quality report data collection report mencakup beberapa aspek penting yang terkait dengan sumber data  atribut data  kecukupan data  dan penggabungan beberapa data yang bervariasi sumber data perlu memperhatikan keberadaan data yang dibutuhkan  apakah data berbayar  dan adakah data tambahan yang diperlukan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1139, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'apakah data berbayar  dan adakah data tambahan yang diperlukan', 'type': 'extractive', 'score': 0.0, 'context': 'data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll dokumentasi data understanding meliputi beberapa hal  yaitu data collection report  data description report  data exploration report  dan data quality report data collection report mencakup beberapa aspek penting yang terkait dengan sumber data  atribut data  kecukupan data  dan penggabungan beberapa data yang bervariasi sumber data perlu memperhatikan keberadaan data yang dibutuhkan  apakah data berbayar  dan adakah data tambahan yang diperlukan', 'offsets_in_document': [{'start': 823, 'end': 885}], 'offsets_in_context': [{'start': 823, 'end': 885}], 'document_ids': ['ee48b9413d49a340c6bf9a9c41e7d150-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang perlu diperhatikan dalam sumber data ', 'filters': None, 'id': 'e3628be1cbe505e961d536ed856ca775', 'no_answer': False, 'answers': ['apakah data berbayar  dan adakah data tambahan yang diperlukan'], 'offsets_in_documents': [{'start': 823, 'end': 885}], 'offsets_in_contexts': [{'start': 823, 'end': 885}], 'document_ids': ['ee48b9413d49a340c6bf9a9c41e7d150-0'], 'contexts': ['data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll dokumentasi data understanding meliputi beberapa hal  yaitu data collection report  data description report  data exploration report  dan data quality report data collection report mencakup beberapa aspek penting yang terkait dengan sumber data  atribut data  kecukupan data  dan penggabungan beberapa data yang bervariasi sumber data perlu memperhatikan keberadaan data yang dibutuhkan  apakah data berbayar  dan adakah data tambahan yang diperlukan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '39e1a850-caa2-467f-97be-de6774d2b4b4', 'query': 'sebutan lain q2 adalah apa ', 'document': {'id': '2782c706c505bd67ec2f5d05f7c9f444-0', 'content': 'simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik jika nilai simpangan baku kecil  data secara umum terkumpul dekat dengan nilai rerata aritmetik median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1147, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'median', 'type': 'extractive', 'score': 0.0, 'context': 'simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik jika nilai simpangan baku kecil  data secara umum terkumpul dekat dengan nilai rerata aritmetik median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data', 'offsets_in_document': [{'start': 246, 'end': 252}], 'offsets_in_context': [{'start': 246, 'end': 252}], 'document_ids': ['2782c706c505bd67ec2f5d05f7c9f444-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'sebutan lain q2 adalah apa ', 'filters': None, 'id': 'a9bca8264f11212899a8ddcba907e71e', 'no_answer': False, 'answers': ['median'], 'offsets_in_documents': [{'start': 246, 'end': 252}], 'offsets_in_contexts': [{'start': 246, 'end': 252}], 'document_ids': ['2782c706c505bd67ec2f5d05f7c9f444-0'], 'contexts': ['simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik jika nilai simpangan baku kecil  data secara umum terkumpul dekat dengan nilai rerata aritmetik median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '08b7cf84-5695-496c-a772-c93bd7c071fb', 'query': 'median dikenal juga dengan sebutan apa ', 'document': {'id': '2782c706c505bd67ec2f5d05f7c9f444-0', 'content': 'simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik jika nilai simpangan baku kecil  data secara umum terkumpul dekat dengan nilai rerata aritmetik median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1147, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' q2', 'type': 'extractive', 'score': 0.0, 'context': 'simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik jika nilai simpangan baku kecil  data secara umum terkumpul dekat dengan nilai rerata aritmetik median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data', 'offsets_in_document': [{'start': 294, 'end': 297}], 'offsets_in_context': [{'start': 294, 'end': 297}], 'document_ids': ['2782c706c505bd67ec2f5d05f7c9f444-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'median dikenal juga dengan sebutan apa ', 'filters': None, 'id': '71267fa457b7b3ef83a7fafa6e698126', 'no_answer': False, 'answers': [' q2'], 'offsets_in_documents': [{'start': 294, 'end': 297}], 'offsets_in_contexts': [{'start': 294, 'end': 297}], 'document_ids': ['2782c706c505bd67ec2f5d05f7c9f444-0'], 'contexts': ['simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik jika nilai simpangan baku kecil  data secara umum terkumpul dekat dengan nilai rerata aritmetik median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '62e556ce-a4eb-42a1-8496-4720bb66e6e2', 'query': 'mengidentifikasi subset dari data dilakukan pada dokumentasi apa ', 'document': {'id': 'e59fa8f0c42bf7079b71e1eb4178c3d0-0', 'content': 'data collection report mencakup beberapa aspek penting yang terkait dengan sumber data  atribut data  kecukupan data  dan penggabungan beberapa data yang bervariasi sumber data perlu memperhatikan keberadaan data yang dibutuhkan  apakah data berbayar  dan adakah data tambahan yang diperlukan data description report mencakup beberapa aspek penting  seperti gambaran data yang berisi jumlah data dan tipe nilai  kuantitas data  format dan metode mendapatkannya   kualitas data  karakteristik dan tipe data   penentuan statistik dasar dari atribut kunci  dan atribut yang menjadi prioritas data exploration report mencakup beberapa aspek penting  yaitu hipotesis data  atribut yang  menjanjikan  untuk dianalisis lebih lanjut  eksplorasi karakteristik baru dari data  bagaimana mengubah initial hipotesis  mengidentifikasi subset dari data untuk digunakan nantinya  serta perbandingan dengan tujuan data quality report mencakup beberapa aspek penting  yaitu missing data  kesalahan data  menghitung kesalahan  atribut yang hilang dan kosong  noise  memeriksa keberadaan redudansi data  pertimbangan untuk data pencicilan   outlier  serta bagaimana data disimpan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1152, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data exploration report ', 'type': 'extractive', 'score': 0.0, 'context': 'data collection report mencakup beberapa aspek penting yang terkait dengan sumber data  atribut data  kecukupan data  dan penggabungan beberapa data yang bervariasi sumber data perlu memperhatikan keberadaan data yang dibutuhkan  apakah data berbayar  dan adakah data tambahan yang diperlukan data description report mencakup beberapa aspek penting  seperti gambaran data yang berisi jumlah data dan tipe nilai  kuantitas data  format dan metode mendapatkannya   kualitas data  karakteristik dan tipe data   penentuan statistik dasar dari atribut kunci  dan atribut yang menjadi prioritas data exploration report mencakup beberapa aspek penting  yaitu hipotesis data  atribut yang  menjanjikan  untuk dianalisis lebih lanjut  eksplorasi karakteristik baru dari data  bagaimana mengubah initial hipotesis  mengidentifikasi subset dari data untuk digunakan nantinya  serta perbandingan dengan tujuan data quality report mencakup beberapa aspek penting  yaitu missing data  kesalahan data  menghitung kesalahan  atribut yang hilang dan kosong  noise  memeriksa keberadaan redudansi data  pertimbangan untuk data pencicilan   outlier  serta bagaimana data disimpan', 'offsets_in_document': [{'start': 589, 'end': 613}], 'offsets_in_context': [{'start': 589, 'end': 613}], 'document_ids': ['e59fa8f0c42bf7079b71e1eb4178c3d0-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'mengidentifikasi subset dari data dilakukan pada dokumentasi apa ', 'filters': None, 'id': '94a1358987abc2761a980ae4afbf7373', 'no_answer': False, 'answers': ['data exploration report '], 'offsets_in_documents': [{'start': 589, 'end': 613}], 'offsets_in_contexts': [{'start': 589, 'end': 613}], 'document_ids': ['e59fa8f0c42bf7079b71e1eb4178c3d0-0'], 'contexts': ['data collection report mencakup beberapa aspek penting yang terkait dengan sumber data  atribut data  kecukupan data  dan penggabungan beberapa data yang bervariasi sumber data perlu memperhatikan keberadaan data yang dibutuhkan  apakah data berbayar  dan adakah data tambahan yang diperlukan data description report mencakup beberapa aspek penting  seperti gambaran data yang berisi jumlah data dan tipe nilai  kuantitas data  format dan metode mendapatkannya   kualitas data  karakteristik dan tipe data   penentuan statistik dasar dari atribut kunci  dan atribut yang menjadi prioritas data exploration report mencakup beberapa aspek penting  yaitu hipotesis data  atribut yang  menjanjikan  untuk dianalisis lebih lanjut  eksplorasi karakteristik baru dari data  bagaimana mengubah initial hipotesis  mengidentifikasi subset dari data untuk digunakan nantinya  serta perbandingan dengan tujuan data quality report mencakup beberapa aspek penting  yaitu missing data  kesalahan data  menghitung kesalahan  atribut yang hilang dan kosong  noise  memeriksa keberadaan redudansi data  pertimbangan untuk data pencicilan   outlier  serta bagaimana data disimpan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '939d11a8-4a45-446b-97fd-d10b71212ae4', 'query': 'bagaimana cara mengidentifikasi dan melakukan perbaikan error dalam dataset ', 'document': {'id': 'aa43704fde004442ff92801970df976e-0', 'content': 'terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat tugas dalam data preparation meliputi data cleaning  transformasi data  dan reduksi dimensi data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1181, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'menghapus duplikasi  menangani missing data  dan outlier', 'type': 'extractive', 'score': 0.0, 'context': 'terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat tugas dalam data preparation meliputi data cleaning  transformasi data  dan reduksi dimensi data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis', 'offsets_in_document': [{'start': 610, 'end': 666}], 'offsets_in_context': [{'start': 610, 'end': 666}], 'document_ids': ['aa43704fde004442ff92801970df976e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mengidentifikasi dan melakukan perbaikan error dalam dataset ', 'filters': None, 'id': '69fa5b43ea37965b46bba980d81c16a8', 'no_answer': False, 'answers': ['menghapus duplikasi  menangani missing data  dan outlier'], 'offsets_in_documents': [{'start': 610, 'end': 666}], 'offsets_in_contexts': [{'start': 610, 'end': 666}], 'document_ids': ['aa43704fde004442ff92801970df976e-0'], 'contexts': ['terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat tugas dalam data preparation meliputi data cleaning  transformasi data  dan reduksi dimensi data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'de504f3e-464e-4eb7-9555-3db1bf24589a', 'query': 'apa tujuan dari data preparation ', 'document': {'id': 'aa43704fde004442ff92801970df976e-0', 'content': 'terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat tugas dalam data preparation meliputi data cleaning  transformasi data  dan reduksi dimensi data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1181, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat ', 'type': 'extractive', 'score': 0.0, 'context': 'terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat tugas dalam data preparation meliputi data cleaning  transformasi data  dan reduksi dimensi data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis', 'offsets_in_document': [{'start': 196, 'end': 437}], 'offsets_in_context': [{'start': 196, 'end': 437}], 'document_ids': ['aa43704fde004442ff92801970df976e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa tujuan dari data preparation ', 'filters': None, 'id': '4b27c82f46cc113e6b93bed630051947', 'no_answer': False, 'answers': ['meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat '], 'offsets_in_documents': [{'start': 196, 'end': 437}], 'offsets_in_contexts': [{'start': 196, 'end': 437}], 'document_ids': ['aa43704fde004442ff92801970df976e-0'], 'contexts': ['terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat tugas dalam data preparation meliputi data cleaning  transformasi data  dan reduksi dimensi data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis']}>,\n",
              " <MultiLabel: {'labels': [{'id': '374c1da5-a25e-40c7-befd-b8e60c0d85e4', 'query': 'apa yang dimaksud dengan data preparation ', 'document': {'id': '7621966f9c34f7fab7846f47c4fdca5a-0', 'content': ' data preparation adalah serangkaian proses pembersihan dan transformasi raw data yang dilakukan sebelum proses analisis  modelling  reporting untuk menjadikan data clean  akurat  lengkap dan realiable data preparation merupakan tahapan penting karena data diperoleh dari berbagai sumber yang umumnya didesain dalam format tertentu untuk menjalankan fungsi tertentu dalam aplikasi  beberapa algoritma machine learning memerlukan data dalam format tertentu  seperti format numerik selain itu  data harus bersih  akurat  lengkap  dan telah diberi label dengan baik sebelum dilakukan proses analisis terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1185, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data preparation adalah serangkaian proses pembersihan dan transformasi raw data yang dilakukan sebelum proses analisis  modelling  reporting untuk menjadikan data clean  akurat  lengkap dan realiable ', 'type': 'extractive', 'score': 0.0, 'context': ' data preparation adalah serangkaian proses pembersihan dan transformasi raw data yang dilakukan sebelum proses analisis  modelling  reporting untuk menjadikan data clean  akurat  lengkap dan realiable data preparation merupakan tahapan penting karena data diperoleh dari berbagai sumber yang umumnya didesain dalam format tertentu untuk menjalankan fungsi tertentu dalam aplikasi  beberapa algoritma machine learning memerlukan data dalam format tertentu  seperti format numerik selain itu  data harus bersih  akurat  lengkap  dan telah diberi label dengan baik sebelum dilakukan proses analisis terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat', 'offsets_in_document': [{'start': 1, 'end': 202}], 'offsets_in_context': [{'start': 1, 'end': 202}], 'document_ids': ['7621966f9c34f7fab7846f47c4fdca5a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan data preparation ', 'filters': None, 'id': 'e87e6539162f95fdf64c5e7c87b2660d', 'no_answer': False, 'answers': ['data preparation adalah serangkaian proses pembersihan dan transformasi raw data yang dilakukan sebelum proses analisis  modelling  reporting untuk menjadikan data clean  akurat  lengkap dan realiable '], 'offsets_in_documents': [{'start': 1, 'end': 202}], 'offsets_in_contexts': [{'start': 1, 'end': 202}], 'document_ids': ['7621966f9c34f7fab7846f47c4fdca5a-0'], 'contexts': [' data preparation adalah serangkaian proses pembersihan dan transformasi raw data yang dilakukan sebelum proses analisis  modelling  reporting untuk menjadikan data clean  akurat  lengkap dan realiable data preparation merupakan tahapan penting karena data diperoleh dari berbagai sumber yang umumnya didesain dalam format tertentu untuk menjalankan fungsi tertentu dalam aplikasi  beberapa algoritma machine learning memerlukan data dalam format tertentu  seperti format numerik selain itu  data harus bersih  akurat  lengkap  dan telah diberi label dengan baik sebelum dilakukan proses analisis terakhir  kegagalan dalam analisis data maupun proyek machine learning sering disebabkan oleh data yang tidak berkualitas  bukan dari algoritma maupun vendornya tujuan dari data preparation yaitu meningkatkan keseluruhan kualitas data dan proses analisisnya  mempercepat proses analisis pengambilan keputusan  menggabungkan sejumlah dataset yang terpisah ke dalam satu format  mendapatkan hasil analisis data yang berkualitas dan akurat']}>,\n",
              " <MultiLabel: {'labels': [{'id': '8d98e7ae-27e9-43f6-a94d-12b86642b9e7', 'query': 'apa penyebab dari outlier ', 'document': {'id': 'e094192c966e237863ff48565d5a4969-0', 'content': 'selain itu  terdapat penanganan outlier dalam tugas data cleaning tidak ada definisi dan metode yang pasti untuk mengidentifikasi outlier karena bergantung pada dataset yang spesifik domain expert dapat dilibatkan untuk membantu dalam menginterpretasikan dan menentukan apakah sampel merupakan outlier atau tidak outlier dapat disebabkan oleh data entry error  measurement error  sampling error  data corruption  serta pengamatan outlier yang sebenarnya keberadaan outlier dapat menyebabkan masalah saat memodelkan  terutama dalam model linear  dan dapat mempengaruhi evaluasi seperti perhitungan mse  mean squared error', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1189, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data entry error  measurement error  sampling error  data corruption  serta pengamatan outlier yang sebenarnya ', 'type': 'extractive', 'score': 0.0, 'context': 'selain itu  terdapat penanganan outlier dalam tugas data cleaning tidak ada definisi dan metode yang pasti untuk mengidentifikasi outlier karena bergantung pada dataset yang spesifik domain expert dapat dilibatkan untuk membantu dalam menginterpretasikan dan menentukan apakah sampel merupakan outlier atau tidak outlier dapat disebabkan oleh data entry error  measurement error  sampling error  data corruption  serta pengamatan outlier yang sebenarnya keberadaan outlier dapat menyebabkan masalah saat memodelkan  terutama dalam model linear  dan dapat mempengaruhi evaluasi seperti perhitungan mse  mean squared error', 'offsets_in_document': [{'start': 343, 'end': 454}], 'offsets_in_context': [{'start': 343, 'end': 454}], 'document_ids': ['e094192c966e237863ff48565d5a4969-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa penyebab dari outlier ', 'filters': None, 'id': '81f149e20c037f811fb1e2990fb1de6e', 'no_answer': False, 'answers': ['data entry error  measurement error  sampling error  data corruption  serta pengamatan outlier yang sebenarnya '], 'offsets_in_documents': [{'start': 343, 'end': 454}], 'offsets_in_contexts': [{'start': 343, 'end': 454}], 'document_ids': ['e094192c966e237863ff48565d5a4969-0'], 'contexts': ['selain itu  terdapat penanganan outlier dalam tugas data cleaning tidak ada definisi dan metode yang pasti untuk mengidentifikasi outlier karena bergantung pada dataset yang spesifik domain expert dapat dilibatkan untuk membantu dalam menginterpretasikan dan menentukan apakah sampel merupakan outlier atau tidak outlier dapat disebabkan oleh data entry error  measurement error  sampling error  data corruption  serta pengamatan outlier yang sebenarnya keberadaan outlier dapat menyebabkan masalah saat memodelkan  terutama dalam model linear  dan dapat mempengaruhi evaluasi seperti perhitungan mse  mean squared error']}>,\n",
              " <MultiLabel: {'labels': [{'id': '6ee438fe-c45c-4dde-b28d-63de51f455b8', 'query': 'apa yang dimaksud dengan high dimensionality data ', 'document': {'id': 'a11a97695335b6fdae7eaf9ba3b5600a-0', 'content': ' banyak data domain saat ini yang melibatkan banyak fitur  high dimensionality data   contohnya  1  dokumen  terdiri atas ratusan ribuan kata  bigram   2  citra  terdiri atas ribuan jutaan piksel   3  genomics  terdiri atas ribuan gen  basa dna  dsb data berdimensi tinggi berpotensi berisi redundant features  irrelevent features ataupun noise sehingga memunculkan permasalahan seperti  1  fitur data yang redundan atau tidak relevan dengan konteks permasalahan dapat menurunkan performa algoritma machine learning   2  biaya komputasi biasanya akan meningkat secara eksponensial   3  lebih sulit untuk diinterpretasikan atau divisualisasikan   4  curse of dimensionality  saat dimensi data bertambah besar  maka jumlah sampel data yang tersedia juga harus ditambah tetapi jika dimensi data besar  namun jumlah sampel data terbatas maka akan terjadi curse of dimensionality yang dapat meningkatkan risiko overfitting reduksi dimensi menjadi solusi untuk permasalahan dimana ketika dimensi data meningkat  performa classifier juga meningkat hingga jumlah fitur yang optimal tertentu dan jika dimensi data terus ditambah  tanpa ada peningkatan jumlah sampel pelatihan  maka performa classifier juga akan menurun reduksi dimensi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1218, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' data domain saat ini yang melibatkan banyak fitur', 'type': 'extractive', 'score': 0.0, 'context': ' banyak data domain saat ini yang melibatkan banyak fitur  high dimensionality data   contohnya  1  dokumen  terdiri atas ratusan ribuan kata  bigram   2  citra  terdiri atas ribuan jutaan piksel   3  genomics  terdiri atas ribuan gen  basa dna  dsb data berdimensi tinggi berpotensi berisi redundant features  irrelevent features ataupun noise sehingga memunculkan permasalahan seperti  1  fitur data yang redundan atau tidak relevan dengan konteks permasalahan dapat menurunkan performa algoritma machine learning   2  biaya komputasi biasanya akan meningkat secara eksponensial   3  lebih sulit untuk diinterpretasikan atau divisualisasikan   4  curse of dimensionality  saat dimensi data bertambah besar  maka jumlah sampel data yang tersedia juga harus ditambah tetapi jika dimensi data besar  namun jumlah sampel data terbatas maka akan terjadi curse of dimensionality yang dapat meningkatkan risiko overfitting reduksi dimensi menjadi solusi untuk permasalahan dimana ketika dimensi data meningkat  performa classifier juga meningkat hingga jumlah fitur yang optimal tertentu dan jika dimensi data terus ditambah  tanpa ada peningkatan jumlah sampel pelatihan  maka performa classifier juga akan menurun reduksi dimensi', 'offsets_in_document': [{'start': 7, 'end': 57}], 'offsets_in_context': [{'start': 7, 'end': 57}], 'document_ids': ['a11a97695335b6fdae7eaf9ba3b5600a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan high dimensionality data ', 'filters': None, 'id': '83acc2e6dcf764aa36f5f0fc3d798175', 'no_answer': False, 'answers': [' data domain saat ini yang melibatkan banyak fitur'], 'offsets_in_documents': [{'start': 7, 'end': 57}], 'offsets_in_contexts': [{'start': 7, 'end': 57}], 'document_ids': ['a11a97695335b6fdae7eaf9ba3b5600a-0'], 'contexts': [' banyak data domain saat ini yang melibatkan banyak fitur  high dimensionality data   contohnya  1  dokumen  terdiri atas ratusan ribuan kata  bigram   2  citra  terdiri atas ribuan jutaan piksel   3  genomics  terdiri atas ribuan gen  basa dna  dsb data berdimensi tinggi berpotensi berisi redundant features  irrelevent features ataupun noise sehingga memunculkan permasalahan seperti  1  fitur data yang redundan atau tidak relevan dengan konteks permasalahan dapat menurunkan performa algoritma machine learning   2  biaya komputasi biasanya akan meningkat secara eksponensial   3  lebih sulit untuk diinterpretasikan atau divisualisasikan   4  curse of dimensionality  saat dimensi data bertambah besar  maka jumlah sampel data yang tersedia juga harus ditambah tetapi jika dimensi data besar  namun jumlah sampel data terbatas maka akan terjadi curse of dimensionality yang dapat meningkatkan risiko overfitting reduksi dimensi menjadi solusi untuk permasalahan dimana ketika dimensi data meningkat  performa classifier juga meningkat hingga jumlah fitur yang optimal tertentu dan jika dimensi data terus ditambah  tanpa ada peningkatan jumlah sampel pelatihan  maka performa classifier juga akan menurun reduksi dimensi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'baecce5f-9c1c-45f4-97cb-21826d34e6e6', 'query': 'bagaimana cara kerja quantile discretization transform ', 'document': {'id': 'dbffb574ff4c8796e1920ae8dc5d09ca-0', 'content': 'dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  uniform   parameter encode  ordinal  dan menentukan jumlah bin pada parameter n_bin k means cluster discretization transform dilakukan dengan cara membentuk k cluster untuk setiap variabel input  lalu memasukkan setiap nilai ke sebuah kluster yang sesuai jika distribusi nilai variabel tidak terlalu kompleks  biasanya jumlah kluster k dapat diset dengan nilai antara 3 5. dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  kmeans  dan menentukan jumlah bin pada parameter n_bin pada quantile discretization transform akan membagi setiap observasi untuk setiap variabel input ke dalam k grup bin  dimana setiap grup berisi jumlah anggota yang hampir sama jika distribusi nilai input variabel tidak terlalu kompleks  jumlah bin dapat diset dengan nilai kecil antara 5 10. dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  quantile  dan menentukan jumlah bin pada parameter n_bin', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1222, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'quantile discretization transform akan membagi setiap observasi untuk setiap variabel input ke dalam k grup bin  dimana setiap grup berisi jumlah anggota yang hampir sama', 'type': 'extractive', 'score': 0.0, 'context': 'dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  uniform   parameter encode  ordinal  dan menentukan jumlah bin pada parameter n_bin k means cluster discretization transform dilakukan dengan cara membentuk k cluster untuk setiap variabel input  lalu memasukkan setiap nilai ke sebuah kluster yang sesuai jika distribusi nilai variabel tidak terlalu kompleks  biasanya jumlah kluster k dapat diset dengan nilai antara 3 5. dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  kmeans  dan menentukan jumlah bin pada parameter n_bin pada quantile discretization transform akan membagi setiap observasi untuk setiap variabel input ke dalam k grup bin  dimana setiap grup berisi jumlah anggota yang hampir sama jika distribusi nilai input variabel tidak terlalu kompleks  jumlah bin dapat diset dengan nilai kecil antara 5 10. dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  quantile  dan menentukan jumlah bin pada parameter n_bin', 'offsets_in_document': [{'start': 667, 'end': 837}], 'offsets_in_context': [{'start': 667, 'end': 837}], 'document_ids': ['dbffb574ff4c8796e1920ae8dc5d09ca-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara kerja quantile discretization transform ', 'filters': None, 'id': '49c590c2301c32ed9305603c92d1975e', 'no_answer': False, 'answers': ['quantile discretization transform akan membagi setiap observasi untuk setiap variabel input ke dalam k grup bin  dimana setiap grup berisi jumlah anggota yang hampir sama'], 'offsets_in_documents': [{'start': 667, 'end': 837}], 'offsets_in_contexts': [{'start': 667, 'end': 837}], 'document_ids': ['dbffb574ff4c8796e1920ae8dc5d09ca-0'], 'contexts': ['dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  uniform   parameter encode  ordinal  dan menentukan jumlah bin pada parameter n_bin k means cluster discretization transform dilakukan dengan cara membentuk k cluster untuk setiap variabel input  lalu memasukkan setiap nilai ke sebuah kluster yang sesuai jika distribusi nilai variabel tidak terlalu kompleks  biasanya jumlah kluster k dapat diset dengan nilai antara 3 5. dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  kmeans  dan menentukan jumlah bin pada parameter n_bin pada quantile discretization transform akan membagi setiap observasi untuk setiap variabel input ke dalam k grup bin  dimana setiap grup berisi jumlah anggota yang hampir sama jika distribusi nilai input variabel tidak terlalu kompleks  jumlah bin dapat diset dengan nilai kecil antara 5 10. dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  quantile  dan menentukan jumlah bin pada parameter n_bin']}>,\n",
              " <MultiLabel: {'labels': [{'id': '08d92e2b-db39-4124-8369-2561a6dbbe7f', 'query': 'Untuk apa intuisi diperlukan dalam sains data?', 'document': {'id': '1049fd72bf7b902267853c716b0359e2-0', 'content': 'langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan seorang ilmuwan data perlu memiliki pemahaman yang baik tentang kebutuhan bisnis dan mampu mengembangkan analitik yang sesuai dengan mereka intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan keingintahuan juga menjadi kunci dalam sains data  karena bidang ini terus berkembang dengan metode dan teknologi baru yang terus muncul', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1005, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan', 'type': 'extractive', 'score': 0.0, 'context': 'langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan seorang ilmuwan data perlu memiliki pemahaman yang baik tentang kebutuhan bisnis dan mampu mengembangkan analitik yang sesuai dengan mereka intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan keingintahuan juga menjadi kunci dalam sains data  karena bidang ini terus berkembang dengan metode dan teknologi baru yang terus muncul', 'offsets_in_document': [{'start': 408, 'end': 542}], 'offsets_in_context': [{'start': 408, 'end': 542}], 'document_ids': ['1049fd72bf7b902267853c716b0359e2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Untuk apa intuisi diperlukan dalam sains data?', 'filters': None, 'id': 'f19aafb95d392053ade3586766cc8a14', 'no_answer': False, 'answers': ['intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan'], 'offsets_in_documents': [{'start': 408, 'end': 542}], 'offsets_in_contexts': [{'start': 408, 'end': 542}], 'document_ids': ['1049fd72bf7b902267853c716b0359e2-0'], 'contexts': ['langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan seorang ilmuwan data perlu memiliki pemahaman yang baik tentang kebutuhan bisnis dan mampu mengembangkan analitik yang sesuai dengan mereka intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan keingintahuan juga menjadi kunci dalam sains data  karena bidang ini terus berkembang dengan metode dan teknologi baru yang terus muncul']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e60cf293-f568-4538-830c-9bd5460e6267', 'query': 'apa saja jenis supervised learning?', 'document': {'id': '4ae034a8f4d523330287dca657e3ca6f-0', 'content': 'variable ouput y berperan sebagai teacher yang membimbing proses pembelajaran  setelah fungsi f didapatkan  maka fungsi tersebut dapat digunakan untuk memetakan  memprediksi  input data baru  x  dengan tepat  fungsi f yang dihasilkan dari proses pembelajaran disebut juga sebagai model  classification model terdapat 3 jenis supervised learning yaitu 1  berdasarkan tipe ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu regresi dan klasifikasi  regresi  jika ouput y bernilai kontinu dan klasifikasi  jika ouput y bernilai diskret kategori 2  berdasarkan jumlah ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu binary classification dan multiclass classification  binary classification  target artibut hanya terdiri dari 2 kategori kelas dan multiclass classification  target atribut terdiri atas lebih dari dua kategori kelas 3  berdasarkan separability   linearity  pada jenis ini supervised learning dibagi menjadi 4 yaitu separable  kelompok yang berbeda menempati daerah pada ruang dimensi yang berbeda  non separable  kelompok yang berbeda bercampur dalam daerah yang sama dalam ruang dimensi  linearly separable  kelompok yang berbeda dapat dipisahkan dengan sebuah garis lurus  non linearly separable  kelompok yang berbeda tidak dapat dipisahkan dengan sebuah garis lurus', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1021, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' terdapat 3 jenis supervised learning yaitu 1  berdasarkan tipe ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu regresi dan klasifikasi  regresi  jika ouput y bernilai kontinu dan klasifikasi  jika ouput y bernilai diskret kategori 2  berdasarkan jumlah ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu binary classification dan multiclass classification  binary classification  target artibut hanya terdiri dari 2 kategori kelas dan multiclass classification  target atribut terdiri atas lebih dari dua kategori kelas 3  berdasarkan separability   linearity  pada jenis ini supervised learning dibagi menjadi 4 yaitu separable  kelompok yang berbeda menempati daerah pada ruang dimensi yang berbeda  non separable  kelompok yang berbeda bercampur dalam daerah yang sama dalam ruang dimensi  linearly separable  kelompok yang berbeda dapat dipisahkan dengan sebuah garis lurus  non linearly separable  kelompok yang berbeda tidak dapat dipisahkan dengan sebuah garis lurus', 'type': 'extractive', 'score': 0.0, 'context': 'variable ouput y berperan sebagai teacher yang membimbing proses pembelajaran  setelah fungsi f didapatkan  maka fungsi tersebut dapat digunakan untuk memetakan  memprediksi  input data baru  x  dengan tepat  fungsi f yang dihasilkan dari proses pembelajaran disebut juga sebagai model  classification model terdapat 3 jenis supervised learning yaitu 1  berdasarkan tipe ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu regresi dan klasifikasi  regresi  jika ouput y bernilai kontinu dan klasifikasi  jika ouput y bernilai diskret kategori 2  berdasarkan jumlah ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu binary classification dan multiclass classification  binary classification  target artibut hanya terdiri dari 2 kategori kelas dan multiclass classification  target atribut terdiri atas lebih dari dua kategori kelas 3  berdasarkan separability   linearity  pada jenis ini supervised learning dibagi menjadi 4 yaitu separable  kelompok yang berbeda menempati daerah pada ruang dimensi yang berbeda  non separable  kelompok yang berbeda bercampur dalam daerah yang sama dalam ruang dimensi  linearly separable  kelompok yang berbeda dapat dipisahkan dengan sebuah garis lurus  non linearly separable  kelompok yang berbeda tidak dapat dipisahkan dengan sebuah garis lurus', 'offsets_in_document': [{'start': 307, 'end': 1318}], 'offsets_in_context': [{'start': 307, 'end': 1318}], 'document_ids': ['4ae034a8f4d523330287dca657e3ca6f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa saja jenis supervised learning?', 'filters': None, 'id': 'bd761be17b9cff921f108f5edbc23f50', 'no_answer': False, 'answers': [' terdapat 3 jenis supervised learning yaitu 1  berdasarkan tipe ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu regresi dan klasifikasi  regresi  jika ouput y bernilai kontinu dan klasifikasi  jika ouput y bernilai diskret kategori 2  berdasarkan jumlah ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu binary classification dan multiclass classification  binary classification  target artibut hanya terdiri dari 2 kategori kelas dan multiclass classification  target atribut terdiri atas lebih dari dua kategori kelas 3  berdasarkan separability   linearity  pada jenis ini supervised learning dibagi menjadi 4 yaitu separable  kelompok yang berbeda menempati daerah pada ruang dimensi yang berbeda  non separable  kelompok yang berbeda bercampur dalam daerah yang sama dalam ruang dimensi  linearly separable  kelompok yang berbeda dapat dipisahkan dengan sebuah garis lurus  non linearly separable  kelompok yang berbeda tidak dapat dipisahkan dengan sebuah garis lurus'], 'offsets_in_documents': [{'start': 307, 'end': 1318}], 'offsets_in_contexts': [{'start': 307, 'end': 1318}], 'document_ids': ['4ae034a8f4d523330287dca657e3ca6f-0'], 'contexts': ['variable ouput y berperan sebagai teacher yang membimbing proses pembelajaran  setelah fungsi f didapatkan  maka fungsi tersebut dapat digunakan untuk memetakan  memprediksi  input data baru  x  dengan tepat  fungsi f yang dihasilkan dari proses pembelajaran disebut juga sebagai model  classification model terdapat 3 jenis supervised learning yaitu 1  berdasarkan tipe ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu regresi dan klasifikasi  regresi  jika ouput y bernilai kontinu dan klasifikasi  jika ouput y bernilai diskret kategori 2  berdasarkan jumlah ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu binary classification dan multiclass classification  binary classification  target artibut hanya terdiri dari 2 kategori kelas dan multiclass classification  target atribut terdiri atas lebih dari dua kategori kelas 3  berdasarkan separability   linearity  pada jenis ini supervised learning dibagi menjadi 4 yaitu separable  kelompok yang berbeda menempati daerah pada ruang dimensi yang berbeda  non separable  kelompok yang berbeda bercampur dalam daerah yang sama dalam ruang dimensi  linearly separable  kelompok yang berbeda dapat dipisahkan dengan sebuah garis lurus  non linearly separable  kelompok yang berbeda tidak dapat dipisahkan dengan sebuah garis lurus']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3554c168-0b67-4520-a266-b012b29667ed', 'query': 'kapan karnel linear digunakan pada svm?', 'document': {'id': 'f00af1d3da42717f830a8abd8a2d164c-0', 'content': 'svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik   invers multi kuadratik  additive karnel linier digunakan ketika data yang akan diklasifikasi dapat terpisah dengan sebuah garis hyperplane karnel non linear digunakan ketika data hanya dapat dipisahkan dengan garis lengkung atau sebuah bidang pada ruang dimensi tinggi selanjutnya terdapat model assement merupakan proses mengestimasi dan mengevaluasi kinerja berbagai model untuk memilih yang terbaik dan model selection merupakan proses memilih model terbaik setelah melalui evaluasi performa dan mempertimbangkan kesalahan prediksi pada data baru', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1033, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'karnel linier digunakan ketika data yang akan diklasifikasi dapat terpisah dengan sebuah garis hyperplane', 'type': 'extractive', 'score': 0.0, 'context': 'svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik   invers multi kuadratik  additive karnel linier digunakan ketika data yang akan diklasifikasi dapat terpisah dengan sebuah garis hyperplane karnel non linear digunakan ketika data hanya dapat dipisahkan dengan garis lengkung atau sebuah bidang pada ruang dimensi tinggi selanjutnya terdapat model assement merupakan proses mengestimasi dan mengevaluasi kinerja berbagai model untuk memilih yang terbaik dan model selection merupakan proses memilih model terbaik setelah melalui evaluasi performa dan mempertimbangkan kesalahan prediksi pada data baru', 'offsets_in_document': [{'start': 351, 'end': 456}], 'offsets_in_context': [{'start': 351, 'end': 456}], 'document_ids': ['f00af1d3da42717f830a8abd8a2d164c-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'kapan karnel linear digunakan pada svm?', 'filters': None, 'id': 'a9fb4c4881fe50efb007fe598d2c9f03', 'no_answer': False, 'answers': ['karnel linier digunakan ketika data yang akan diklasifikasi dapat terpisah dengan sebuah garis hyperplane'], 'offsets_in_documents': [{'start': 351, 'end': 456}], 'offsets_in_contexts': [{'start': 351, 'end': 456}], 'document_ids': ['f00af1d3da42717f830a8abd8a2d164c-0'], 'contexts': ['svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik   invers multi kuadratik  additive karnel linier digunakan ketika data yang akan diklasifikasi dapat terpisah dengan sebuah garis hyperplane karnel non linear digunakan ketika data hanya dapat dipisahkan dengan garis lengkung atau sebuah bidang pada ruang dimensi tinggi selanjutnya terdapat model assement merupakan proses mengestimasi dan mengevaluasi kinerja berbagai model untuk memilih yang terbaik dan model selection merupakan proses memilih model terbaik setelah melalui evaluasi performa dan mempertimbangkan kesalahan prediksi pada data baru']}>,\n",
              " <MultiLabel: {'labels': [{'id': '61a0955b-2e8d-48dc-affa-31c0c9227d7b', 'query': 'fungsi apa yang mencegah masalah dari fungsi relu?', 'document': {'id': 'af01e0ca4cad0849cd7515a25429b208-0', 'content': 'fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif fungsi parametric relu memiliki keuntungan yaitu memungkinkan kemiringan negatif dipelajari   tidak seperti relu yang bocor  fungsi ini menyediakan kemiringan bagian negatif dari berfungsi sebagai argumen oleh karena itu  dimungkinkan untuk melakukan backpropagation dan belajar yang paling tepat nilai α dan kekurangannya mungkin tampil berbeda untuk masalah yang berbeda fungsi softmax memiliki keuntungan mampu menangani banyak kelas hanya satu kelas di kelas lainnya fungsi aktivasi yaitu menormalkan output untuk masing masing kelas antara 0 dan 1  dan membaginya dengan jumlah mereka  memberikan probabilitas nilai input berada di kelas tertentu serta berguna untuk neuron keluaran  biasanya softmax hanya digunakan untuk lapisan output  untuk', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1041, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu ', 'type': 'extractive', 'score': 0.0, 'context': 'fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif fungsi parametric relu memiliki keuntungan yaitu memungkinkan kemiringan negatif dipelajari   tidak seperti relu yang bocor  fungsi ini menyediakan kemiringan bagian negatif dari berfungsi sebagai argumen oleh karena itu  dimungkinkan untuk melakukan backpropagation dan belajar yang paling tepat nilai α dan kekurangannya mungkin tampil berbeda untuk masalah yang berbeda fungsi softmax memiliki keuntungan mampu menangani banyak kelas hanya satu kelas di kelas lainnya fungsi aktivasi yaitu menormalkan output untuk masing masing kelas antara 0 dan 1  dan membaginya dengan jumlah mereka  memberikan probabilitas nilai input berada di kelas tertentu serta berguna untuk neuron keluaran  biasanya softmax hanya digunakan untuk lapisan output  untuk', 'offsets_in_document': [{'start': 309, 'end': 378}], 'offsets_in_context': [{'start': 309, 'end': 378}], 'document_ids': ['af01e0ca4cad0849cd7515a25429b208-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'fungsi apa yang mencegah masalah dari fungsi relu?', 'filters': None, 'id': 'f34b0f9278115a0f8588055c5283ad97', 'no_answer': False, 'answers': ['fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu '], 'offsets_in_documents': [{'start': 309, 'end': 378}], 'offsets_in_contexts': [{'start': 309, 'end': 378}], 'document_ids': ['af01e0ca4cad0849cd7515a25429b208-0'], 'contexts': ['fungsi relu memiliki keuntungan efisien secara komputasi dan memungkinkan jaringan untuk menyatu sangat cepat dan salah satu kekurangannya adalah masalah dying relu  ketika input mendekati nol  ataunegatif  gradien fungsi menjadi nol  jaringan tidak dapat melakukan backpropagation dan tidak bisa mempelajari fungsi leaky relu memiliki keuntungan mencegah kematian masalah relu variasi relu ini memiliki kemiringan positif kecil di area negatif  sehingga memungkinkan backpropagation  bahkan untuk nilai input negatif dan kekurangannya hasil tidak konsisten  relu yang bocor tidak tersedia prediksi yang konsisten untuk nilai input negatif fungsi parametric relu memiliki keuntungan yaitu memungkinkan kemiringan negatif dipelajari   tidak seperti relu yang bocor  fungsi ini menyediakan kemiringan bagian negatif dari berfungsi sebagai argumen oleh karena itu  dimungkinkan untuk melakukan backpropagation dan belajar yang paling tepat nilai α dan kekurangannya mungkin tampil berbeda untuk masalah yang berbeda fungsi softmax memiliki keuntungan mampu menangani banyak kelas hanya satu kelas di kelas lainnya fungsi aktivasi yaitu menormalkan output untuk masing masing kelas antara 0 dan 1  dan membaginya dengan jumlah mereka  memberikan probabilitas nilai input berada di kelas tertentu serta berguna untuk neuron keluaran  biasanya softmax hanya digunakan untuk lapisan output  untuk']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c5f183c8-cb1a-4bcf-aeae-0f079bcccc1d', 'query': 'siapa yang menemukan fungsi swish?', 'document': {'id': 'e8b31c19c5fc3f24abc51847f95e2401-0', 'content': 'oleh karena itu  dimungkinkan untuk melakukan backpropagation dan belajar yang paling tepat nilai α dan kekurangannya mungkin tampil berbeda untuk masalah yang berbeda fungsi softmax memiliki keuntungan mampu menangani banyak kelas hanya satu kelas di kelas lainnya fungsi aktivasi yaitu menormalkan output untuk masing masing kelas antara 0 dan 1  dan membaginya dengan jumlah mereka  memberikan probabilitas nilai input berada di kelas tertentu serta berguna untuk neuron keluaran  biasanya softmax hanya digunakan untuk lapisan output  untuk jaringan saraf yang perlu mengklasifikasikan input menjadi beberapa kategori fungsi swish yaitu fungsi aktivasi mandiri yang baru ditemukan oleh para peneliti di google menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1046, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' fungsi swish yaitu fungsi aktivasi mandiri yang baru ditemukan oleh para peneliti di google', 'type': 'extractive', 'score': 0.0, 'context': 'oleh karena itu  dimungkinkan untuk melakukan backpropagation dan belajar yang paling tepat nilai α dan kekurangannya mungkin tampil berbeda untuk masalah yang berbeda fungsi softmax memiliki keuntungan mampu menangani banyak kelas hanya satu kelas di kelas lainnya fungsi aktivasi yaitu menormalkan output untuk masing masing kelas antara 0 dan 1  dan membaginya dengan jumlah mereka  memberikan probabilitas nilai input berada di kelas tertentu serta berguna untuk neuron keluaran  biasanya softmax hanya digunakan untuk lapisan output  untuk jaringan saraf yang perlu mengklasifikasikan input menjadi beberapa kategori fungsi swish yaitu fungsi aktivasi mandiri yang baru ditemukan oleh para peneliti di google menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi', 'offsets_in_document': [{'start': 621, 'end': 713}], 'offsets_in_context': [{'start': 621, 'end': 713}], 'document_ids': ['e8b31c19c5fc3f24abc51847f95e2401-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'siapa yang menemukan fungsi swish?', 'filters': None, 'id': 'aabbbd81e9c4227eda67a63245e3aa90', 'no_answer': False, 'answers': [' fungsi swish yaitu fungsi aktivasi mandiri yang baru ditemukan oleh para peneliti di google'], 'offsets_in_documents': [{'start': 621, 'end': 713}], 'offsets_in_contexts': [{'start': 621, 'end': 713}], 'document_ids': ['e8b31c19c5fc3f24abc51847f95e2401-0'], 'contexts': ['oleh karena itu  dimungkinkan untuk melakukan backpropagation dan belajar yang paling tepat nilai α dan kekurangannya mungkin tampil berbeda untuk masalah yang berbeda fungsi softmax memiliki keuntungan mampu menangani banyak kelas hanya satu kelas di kelas lainnya fungsi aktivasi yaitu menormalkan output untuk masing masing kelas antara 0 dan 1  dan membaginya dengan jumlah mereka  memberikan probabilitas nilai input berada di kelas tertentu serta berguna untuk neuron keluaran  biasanya softmax hanya digunakan untuk lapisan output  untuk jaringan saraf yang perlu mengklasifikasikan input menjadi beberapa kategori fungsi swish yaitu fungsi aktivasi mandiri yang baru ditemukan oleh para peneliti di google menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '5893a9f3-40ca-4592-a553-2efcf329c650', 'query': 'apa saja karakteristik kualitas data yang harus diperhatikan?', 'document': {'id': '5d87129eb8b4bfcfe2a3a7605c673085-0', 'content': 'untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus tugas terakhir dari cleaning data yaitu validasi data dan qa  quality assurance beberapa pertanyaan perlu dijawab untuk memastikan kevalidan data  seperti apakah datanya masuk akal  mengikuti aturan domainnya  membuktikan atau menyangkal working theory  serta dapat digunakan untuk mendapatkan insight baru selain itu  terdapat juga 5 karakteristik kualitas data yang harus diperhatikan  validity  kesesuaian dengan aturan bisnis   accuracy  ketepatan nilai data   completeness  kelengkapan semua data yang diperlukan   consistency  konsistensi dalam satu set atau beberapa set data   dan uniformity  penggunaan satuan ukuran yang sama', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1154, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' 5 karakteristik kualitas data yang harus diperhatikan  validity  kesesuaian dengan aturan bisnis   accuracy  ketepatan nilai data   completeness  kelengkapan semua data yang diperlukan   consistency  konsistensi dalam satu set atau beberapa set data   dan uniformity  penggunaan satuan ukuran yang sama', 'type': 'extractive', 'score': 0.0, 'context': 'untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus tugas terakhir dari cleaning data yaitu validasi data dan qa  quality assurance beberapa pertanyaan perlu dijawab untuk memastikan kevalidan data  seperti apakah datanya masuk akal  mengikuti aturan domainnya  membuktikan atau menyangkal working theory  serta dapat digunakan untuk mendapatkan insight baru selain itu  terdapat juga 5 karakteristik kualitas data yang harus diperhatikan  validity  kesesuaian dengan aturan bisnis   accuracy  ketepatan nilai data   completeness  kelengkapan semua data yang diperlukan   consistency  konsistensi dalam satu set atau beberapa set data   dan uniformity  penggunaan satuan ukuran yang sama', 'offsets_in_document': [{'start': 830, 'end': 1133}], 'offsets_in_context': [{'start': 830, 'end': 1133}], 'document_ids': ['5d87129eb8b4bfcfe2a3a7605c673085-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa saja karakteristik kualitas data yang harus diperhatikan?', 'filters': None, 'id': '29268852dbacac71830629c5647e3b53', 'no_answer': False, 'answers': [' 5 karakteristik kualitas data yang harus diperhatikan  validity  kesesuaian dengan aturan bisnis   accuracy  ketepatan nilai data   completeness  kelengkapan semua data yang diperlukan   consistency  konsistensi dalam satu set atau beberapa set data   dan uniformity  penggunaan satuan ukuran yang sama'], 'offsets_in_documents': [{'start': 830, 'end': 1133}], 'offsets_in_contexts': [{'start': 830, 'end': 1133}], 'document_ids': ['5d87129eb8b4bfcfe2a3a7605c673085-0'], 'contexts': ['untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus tugas terakhir dari cleaning data yaitu validasi data dan qa  quality assurance beberapa pertanyaan perlu dijawab untuk memastikan kevalidan data  seperti apakah datanya masuk akal  mengikuti aturan domainnya  membuktikan atau menyangkal working theory  serta dapat digunakan untuk mendapatkan insight baru selain itu  terdapat juga 5 karakteristik kualitas data yang harus diperhatikan  validity  kesesuaian dengan aturan bisnis   accuracy  ketepatan nilai data   completeness  kelengkapan semua data yang diperlukan   consistency  konsistensi dalam satu set atau beberapa set data   dan uniformity  penggunaan satuan ukuran yang sama']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e2fbc31e-5f63-4650-876f-754798efb763', 'query': 'Apa saja langkah-langkah penggunaan data?', 'document': {'id': '173dd459009093b961c71b030bb8376a-0', 'content': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 999, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan', 'type': 'extractive', 'score': 0.0, 'context': 'data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data', 'offsets_in_document': [{'start': 398, 'end': 661}], 'offsets_in_context': [{'start': 398, 'end': 661}], 'document_ids': ['173dd459009093b961c71b030bb8376a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Apa saja langkah-langkah penggunaan data?', 'filters': None, 'id': 'ee481f9bf25905b9d1a570a2ccb80339', 'no_answer': False, 'answers': ['penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan'], 'offsets_in_documents': [{'start': 398, 'end': 661}], 'offsets_in_contexts': [{'start': 398, 'end': 661}], 'document_ids': ['173dd459009093b961c71b030bb8376a-0'], 'contexts': ['data science menjadi penting dalam era digital saat ini karena dapat mengolah data secara efisien dan menghasilkan wawasan yang berharga dari data tersebut dengan demikian  data science memanfaatkan berbagai elemen untuk menghasilkan pemahaman yang mendalam tentang data data sceience juga diterapkan di berbagai industri dan bidang lainnya dengan menerapkan pengetahuan tentang data yang mendalam penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '4838d4fc-b8df-409f-a9ad-08eec119e1a3', 'query': 'Apa saja latar belakang yang harus dimiliki oleh seorang saintis data?', 'document': {'id': 'f47f5429bdec68f03ea19bfba9146f1e-0', 'content': 'kemampuan pemrograman  terutama dalam bahasa python atau r  penting dalam mengelola dan memproses data secara efisien pemahaman matematika juga diperlukan dalam pemodelan dan interpretasi data terakhir  pengetahuan dalam bidang database membantu dalam mengelola data secara efektif sebagai kunci dalam memanfaatkan potensi big data  saintis data membawa struktur ke dalamnya  menemukan pola menarik  dan memberikan saran kepada eksekutif tentang implikasi untuk produk  proses  dan pengambilan keputusan yang lebih baik dengan demikian  seorang saintis data perlu memiliki latar belakang dalam pembelajaran mesin  statistik  pemrograman  terutama python atau r   matematika  dan bidang database', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1004, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'seorang saintis data perlu memiliki latar belakang dalam pembelajaran mesin  statistik  pemrograman  terutama python atau r   matematika  dan bidang database', 'type': 'extractive', 'score': 0.0, 'context': 'kemampuan pemrograman  terutama dalam bahasa python atau r  penting dalam mengelola dan memproses data secara efisien pemahaman matematika juga diperlukan dalam pemodelan dan interpretasi data terakhir  pengetahuan dalam bidang database membantu dalam mengelola data secara efektif sebagai kunci dalam memanfaatkan potensi big data  saintis data membawa struktur ke dalamnya  menemukan pola menarik  dan memberikan saran kepada eksekutif tentang implikasi untuk produk  proses  dan pengambilan keputusan yang lebih baik dengan demikian  seorang saintis data perlu memiliki latar belakang dalam pembelajaran mesin  statistik  pemrograman  terutama python atau r   matematika  dan bidang database', 'offsets_in_document': [{'start': 537, 'end': 694}], 'offsets_in_context': [{'start': 537, 'end': 694}], 'document_ids': ['f47f5429bdec68f03ea19bfba9146f1e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Apa saja latar belakang yang harus dimiliki oleh seorang saintis data?', 'filters': None, 'id': 'acff505eed8fc682f53259ce566eacb4', 'no_answer': False, 'answers': ['seorang saintis data perlu memiliki latar belakang dalam pembelajaran mesin  statistik  pemrograman  terutama python atau r   matematika  dan bidang database'], 'offsets_in_documents': [{'start': 537, 'end': 694}], 'offsets_in_contexts': [{'start': 537, 'end': 694}], 'document_ids': ['f47f5429bdec68f03ea19bfba9146f1e-0'], 'contexts': ['kemampuan pemrograman  terutama dalam bahasa python atau r  penting dalam mengelola dan memproses data secara efisien pemahaman matematika juga diperlukan dalam pemodelan dan interpretasi data terakhir  pengetahuan dalam bidang database membantu dalam mengelola data secara efektif sebagai kunci dalam memanfaatkan potensi big data  saintis data membawa struktur ke dalamnya  menemukan pola menarik  dan memberikan saran kepada eksekutif tentang implikasi untuk produk  proses  dan pengambilan keputusan yang lebih baik dengan demikian  seorang saintis data perlu memiliki latar belakang dalam pembelajaran mesin  statistik  pemrograman  terutama python atau r   matematika  dan bidang database']}>,\n",
              " <MultiLabel: {'labels': [{'id': '300cc460-4e8b-40e4-8c17-0e7e72c8c08c', 'query': 'Apa saja komponen utama dalam sains data?', 'document': {'id': 'c5af9df498176a2d1af0d319a7178a7e-0', 'content': 'praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang selama proses ini  ilmuwan data menggunakan berbagai teknik dan alat untuk mengoptimalkan penggunaan data dan menghasilkan wawasan yang berharga komponen utama dalam sains data meliputi eksplorasi data  pemodelan  pengujian model  dan implementasi model eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1007, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' komponen utama dalam sains data meliputi eksplorasi data  pemodelan  pengujian model  dan implementasi model', 'type': 'extractive', 'score': 0.0, 'context': 'praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang selama proses ini  ilmuwan data menggunakan berbagai teknik dan alat untuk mengoptimalkan penggunaan data dan menghasilkan wawasan yang berharga komponen utama dalam sains data meliputi eksplorasi data  pemodelan  pengujian model  dan implementasi model eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut', 'offsets_in_document': [{'start': 304, 'end': 413}], 'offsets_in_context': [{'start': 304, 'end': 413}], 'document_ids': ['c5af9df498176a2d1af0d319a7178a7e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Apa saja komponen utama dalam sains data?', 'filters': None, 'id': '09c22e6edf149c5c196798c7674a2710', 'no_answer': False, 'answers': [' komponen utama dalam sains data meliputi eksplorasi data  pemodelan  pengujian model  dan implementasi model'], 'offsets_in_documents': [{'start': 304, 'end': 413}], 'offsets_in_contexts': [{'start': 304, 'end': 413}], 'document_ids': ['c5af9df498176a2d1af0d319a7178a7e-0'], 'contexts': ['praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang selama proses ini  ilmuwan data menggunakan berbagai teknik dan alat untuk mengoptimalkan penggunaan data dan menghasilkan wawasan yang berharga komponen utama dalam sains data meliputi eksplorasi data  pemodelan  pengujian model  dan implementasi model eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b2091ac5-c24e-483f-98fa-57fcf9359ddf', 'query': 'Bagaimana aplikasi sains data dalam bidang pemasaran?', 'document': {'id': '946bf2f9b3a989a92d678fc08f24bba1-0', 'content': 'untuk visualisasi data  r  tableau  dan mentah menjadi pilihan yang populer untuk membuat visualisasi yang menarik dan informatif sedangkan dalam pembelajaran mesin  alat seperti percikan  azure ml studio  dan mahout digunakan untuk mengembangkan dan menerapkan model pembelajaran mesin dengan menggunakan berbagai alat ini  ilmuwan data dapat memanfaatkan teknologi terkini untuk memproses  menganalisis  dan memvisualisasikan data secara efektif sains data memiliki berbagai aplikasi yang relevan dalam berbagai industri dalam bidang pemasaran  sains data dapat digunakan untuk mengembangkan strategi harga berdasarkan analisis data  meningkatkan keuntungan perusahaan  dan mengelompokkan pelanggan berdasarkan kebutuhan mereka', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1011, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'dalam bidang pemasaran  sains data dapat digunakan untuk mengembangkan strategi harga berdasarkan analisis data  meningkatkan keuntungan perusahaan  dan mengelompokkan pelanggan berdasarkan kebutuhan mereka', 'type': 'extractive', 'score': 0.0, 'context': 'untuk visualisasi data  r  tableau  dan mentah menjadi pilihan yang populer untuk membuat visualisasi yang menarik dan informatif sedangkan dalam pembelajaran mesin  alat seperti percikan  azure ml studio  dan mahout digunakan untuk mengembangkan dan menerapkan model pembelajaran mesin dengan menggunakan berbagai alat ini  ilmuwan data dapat memanfaatkan teknologi terkini untuk memproses  menganalisis  dan memvisualisasikan data secara efektif sains data memiliki berbagai aplikasi yang relevan dalam berbagai industri dalam bidang pemasaran  sains data dapat digunakan untuk mengembangkan strategi harga berdasarkan analisis data  meningkatkan keuntungan perusahaan  dan mengelompokkan pelanggan berdasarkan kebutuhan mereka', 'offsets_in_document': [{'start': 523, 'end': 729}], 'offsets_in_context': [{'start': 523, 'end': 729}], 'document_ids': ['946bf2f9b3a989a92d678fc08f24bba1-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Bagaimana aplikasi sains data dalam bidang pemasaran?', 'filters': None, 'id': '12c9e1a344c616f8c35f251b86df1188', 'no_answer': False, 'answers': ['dalam bidang pemasaran  sains data dapat digunakan untuk mengembangkan strategi harga berdasarkan analisis data  meningkatkan keuntungan perusahaan  dan mengelompokkan pelanggan berdasarkan kebutuhan mereka'], 'offsets_in_documents': [{'start': 523, 'end': 729}], 'offsets_in_contexts': [{'start': 523, 'end': 729}], 'document_ids': ['946bf2f9b3a989a92d678fc08f24bba1-0'], 'contexts': ['untuk visualisasi data  r  tableau  dan mentah menjadi pilihan yang populer untuk membuat visualisasi yang menarik dan informatif sedangkan dalam pembelajaran mesin  alat seperti percikan  azure ml studio  dan mahout digunakan untuk mengembangkan dan menerapkan model pembelajaran mesin dengan menggunakan berbagai alat ini  ilmuwan data dapat memanfaatkan teknologi terkini untuk memproses  menganalisis  dan memvisualisasikan data secara efektif sains data memiliki berbagai aplikasi yang relevan dalam berbagai industri dalam bidang pemasaran  sains data dapat digunakan untuk mengembangkan strategi harga berdasarkan analisis data  meningkatkan keuntungan perusahaan  dan mengelompokkan pelanggan berdasarkan kebutuhan mereka']}>,\n",
              " <MultiLabel: {'labels': [{'id': '19ab4bf7-be9c-4ebd-8921-0054d87b2d7e', 'query': 'apa contoh kekurangan dari penggunaan sains data?', 'document': {'id': '585d486f1d50c9bf67a20d9f0d5d1723-0', 'content': 'hal ini memungkinkan kita untuk mendapatkan wawasan yang berharga dari data historis menggunakan alat yang kuat dengan menggunakan sains data  bisnis dapat dioptimalkan  keputusan masa depan yang lebih baik dapat dibuat  dan target pelanggan dapat dipilih dengan lebih baik selain itu  sains data juga membantu konsumen mencari produk yang lebih baik melalui sistem rekomendasi berbasis data namun  penggunaan sains data juga memiliki beberapa kekurangan salah satunya adalah pelanggaran privasi pelanggan saat profil pelanggan digunakan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1016, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' penggunaan sains data juga memiliki beberapa kekurangan salah satunya adalah pelanggaran privasi pelanggan saat profil pelanggan digunakan', 'type': 'extractive', 'score': 0.0, 'context': 'hal ini memungkinkan kita untuk mendapatkan wawasan yang berharga dari data historis menggunakan alat yang kuat dengan menggunakan sains data  bisnis dapat dioptimalkan  keputusan masa depan yang lebih baik dapat dibuat  dan target pelanggan dapat dipilih dengan lebih baik selain itu  sains data juga membantu konsumen mencari produk yang lebih baik melalui sistem rekomendasi berbasis data namun  penggunaan sains data juga memiliki beberapa kekurangan salah satunya adalah pelanggaran privasi pelanggan saat profil pelanggan digunakan', 'offsets_in_document': [{'start': 398, 'end': 537}], 'offsets_in_context': [{'start': 398, 'end': 537}], 'document_ids': ['585d486f1d50c9bf67a20d9f0d5d1723-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa contoh kekurangan dari penggunaan sains data?', 'filters': None, 'id': '61a667dabb5e6838d753c4fa7ea95334', 'no_answer': False, 'answers': [' penggunaan sains data juga memiliki beberapa kekurangan salah satunya adalah pelanggaran privasi pelanggan saat profil pelanggan digunakan'], 'offsets_in_documents': [{'start': 398, 'end': 537}], 'offsets_in_contexts': [{'start': 398, 'end': 537}], 'document_ids': ['585d486f1d50c9bf67a20d9f0d5d1723-0'], 'contexts': ['hal ini memungkinkan kita untuk mendapatkan wawasan yang berharga dari data historis menggunakan alat yang kuat dengan menggunakan sains data  bisnis dapat dioptimalkan  keputusan masa depan yang lebih baik dapat dibuat  dan target pelanggan dapat dipilih dengan lebih baik selain itu  sains data juga membantu konsumen mencari produk yang lebih baik melalui sistem rekomendasi berbasis data namun  penggunaan sains data juga memiliki beberapa kekurangan salah satunya adalah pelanggaran privasi pelanggan saat profil pelanggan digunakan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0ab0ce63-f46b-418a-83b4-c0e462297bf3', 'query': 'bagaimana proses komputasi pada algoritma knn?', 'document': {'id': 'c724692e05e3d00809fc31358c084b5e-0', 'content': '3  berdasarkan separability   linearity  pada jenis ini supervised learning dibagi menjadi 4 yaitu separable  kelompok yang berbeda menempati daerah pada ruang dimensi yang berbeda  non separable  kelompok yang berbeda bercampur dalam daerah yang sama dalam ruang dimensi  linearly separable  kelompok yang berbeda dapat dipisahkan dengan sebuah garis lurus  non linearly separable  kelompok yang berbeda tidak dapat dipisahkan dengan sebuah garis lurus selanjutnya ada beberapa algoritma supervised learning untuk classification task  yaitu 1  k nearest neighbors  algoritma ini juga bisa disebut dengan algortima knn dan algoritma ini termasuk kedalam tipe lazy learning pada algortima ini proses komputasi ditunda sampai prediksi klasifikasi dilakukan algoritma ini mempunyai teknik yang sederhana karena tidak diperlukan pengetahuan awal mengenai bagaimana distribusi data cara kerja dari algortima ini yang pertama algortima ini mengklasifikasikan data baru  test tuple  berdasarkan kelas data dari k tetangga terdekat  nearest neighbors', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1020, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' pada algortima ini proses komputasi ditunda sampai prediksi klasifikasi dilakukan', 'type': 'extractive', 'score': 0.0, 'context': '3  berdasarkan separability   linearity  pada jenis ini supervised learning dibagi menjadi 4 yaitu separable  kelompok yang berbeda menempati daerah pada ruang dimensi yang berbeda  non separable  kelompok yang berbeda bercampur dalam daerah yang sama dalam ruang dimensi  linearly separable  kelompok yang berbeda dapat dipisahkan dengan sebuah garis lurus  non linearly separable  kelompok yang berbeda tidak dapat dipisahkan dengan sebuah garis lurus selanjutnya ada beberapa algoritma supervised learning untuk classification task  yaitu 1  k nearest neighbors  algoritma ini juga bisa disebut dengan algortima knn dan algoritma ini termasuk kedalam tipe lazy learning pada algortima ini proses komputasi ditunda sampai prediksi klasifikasi dilakukan algoritma ini mempunyai teknik yang sederhana karena tidak diperlukan pengetahuan awal mengenai bagaimana distribusi data cara kerja dari algortima ini yang pertama algortima ini mengklasifikasikan data baru  test tuple  berdasarkan kelas data dari k tetangga terdekat  nearest neighbors', 'offsets_in_document': [{'start': 672, 'end': 754}], 'offsets_in_context': [{'start': 672, 'end': 754}], 'document_ids': ['c724692e05e3d00809fc31358c084b5e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana proses komputasi pada algoritma knn?', 'filters': None, 'id': '5d97e6e73e97e8a804200dbfacf389d2', 'no_answer': False, 'answers': [' pada algortima ini proses komputasi ditunda sampai prediksi klasifikasi dilakukan'], 'offsets_in_documents': [{'start': 672, 'end': 754}], 'offsets_in_contexts': [{'start': 672, 'end': 754}], 'document_ids': ['c724692e05e3d00809fc31358c084b5e-0'], 'contexts': ['3  berdasarkan separability   linearity  pada jenis ini supervised learning dibagi menjadi 4 yaitu separable  kelompok yang berbeda menempati daerah pada ruang dimensi yang berbeda  non separable  kelompok yang berbeda bercampur dalam daerah yang sama dalam ruang dimensi  linearly separable  kelompok yang berbeda dapat dipisahkan dengan sebuah garis lurus  non linearly separable  kelompok yang berbeda tidak dapat dipisahkan dengan sebuah garis lurus selanjutnya ada beberapa algoritma supervised learning untuk classification task  yaitu 1  k nearest neighbors  algoritma ini juga bisa disebut dengan algortima knn dan algoritma ini termasuk kedalam tipe lazy learning pada algortima ini proses komputasi ditunda sampai prediksi klasifikasi dilakukan algoritma ini mempunyai teknik yang sederhana karena tidak diperlukan pengetahuan awal mengenai bagaimana distribusi data cara kerja dari algortima ini yang pertama algortima ini mengklasifikasikan data baru  test tuple  berdasarkan kelas data dari k tetangga terdekat  nearest neighbors']}>,\n",
              " <MultiLabel: {'labels': [{'id': '61f75df5-c153-4243-9e92-54ffc8528036', 'query': 'apa yang digunakan algoritma berbasis  tree  dari beberapa decision tree untuk membuat keputusan ', 'document': {'id': 'aed4b416ba30c2a2127631566a5e39ad-0', 'content': 'sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan algoritma random forest menggabungkan keluaran dari beberapa decision tree yang didapatkan secara ensemble untuk menghasilkan keluaran akhir dengan menggabungkan beberapa decision tree yang tidak berkorelasi  seringkali peningkatan akurasi model yang signifikan dapat dicapai decision tree yang memiliki kelemahan overfitting dapat dikurangi dengan menggunakan algoritma random forest', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1026, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'fitur kualitas ', 'type': 'extractive', 'score': 0.0, 'context': 'sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan algoritma random forest menggabungkan keluaran dari beberapa decision tree yang didapatkan secara ensemble untuk menghasilkan keluaran akhir dengan menggabungkan beberapa decision tree yang tidak berkorelasi  seringkali peningkatan akurasi model yang signifikan dapat dicapai decision tree yang memiliki kelemahan overfitting dapat dikurangi dengan menggunakan algoritma random forest', 'offsets_in_document': [{'start': 250, 'end': 265}], 'offsets_in_context': [{'start': 250, 'end': 265}], 'document_ids': ['aed4b416ba30c2a2127631566a5e39ad-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang digunakan algoritma berbasis  tree  dari beberapa decision tree untuk membuat keputusan ', 'filters': None, 'id': '18878a2068cf6f8c72dc6b7b4d2425f1', 'no_answer': False, 'answers': ['fitur kualitas '], 'offsets_in_documents': [{'start': 250, 'end': 265}], 'offsets_in_contexts': [{'start': 250, 'end': 265}], 'document_ids': ['aed4b416ba30c2a2127631566a5e39ad-0'], 'contexts': ['sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan algoritma random forest menggabungkan keluaran dari beberapa decision tree yang didapatkan secara ensemble untuk menghasilkan keluaran akhir dengan menggabungkan beberapa decision tree yang tidak berkorelasi  seringkali peningkatan akurasi model yang signifikan dapat dicapai decision tree yang memiliki kelemahan overfitting dapat dikurangi dengan menggunakan algoritma random forest']}>,\n",
              " <MultiLabel: {'labels': [{'id': '249ebfb7-baaf-40ca-89b3-021dea1fe3fc', 'query': 'apa yang dimaksud dengan random forest ', 'document': {'id': 'aed4b416ba30c2a2127631566a5e39ad-0', 'content': 'sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan algoritma random forest menggabungkan keluaran dari beberapa decision tree yang didapatkan secara ensemble untuk menghasilkan keluaran akhir dengan menggabungkan beberapa decision tree yang tidak berkorelasi  seringkali peningkatan akurasi model yang signifikan dapat dicapai decision tree yang memiliki kelemahan overfitting dapat dikurangi dengan menggunakan algoritma random forest', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1026, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan', 'type': 'extractive', 'score': 0.0, 'context': 'sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan algoritma random forest menggabungkan keluaran dari beberapa decision tree yang didapatkan secara ensemble untuk menghasilkan keluaran akhir dengan menggabungkan beberapa decision tree yang tidak berkorelasi  seringkali peningkatan akurasi model yang signifikan dapat dicapai decision tree yang memiliki kelemahan overfitting dapat dikurangi dengan menggunakan algoritma random forest', 'offsets_in_document': [{'start': 182, 'end': 316}], 'offsets_in_context': [{'start': 182, 'end': 316}], 'document_ids': ['aed4b416ba30c2a2127631566a5e39ad-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan random forest ', 'filters': None, 'id': 'bd4470eea8f32666f88e0ced99d67e3c', 'no_answer': False, 'answers': [' random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan'], 'offsets_in_documents': [{'start': 182, 'end': 316}], 'offsets_in_contexts': [{'start': 182, 'end': 316}], 'document_ids': ['aed4b416ba30c2a2127631566a5e39ad-0'], 'contexts': ['sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan algoritma random forest menggabungkan keluaran dari beberapa decision tree yang didapatkan secara ensemble untuk menghasilkan keluaran akhir dengan menggabungkan beberapa decision tree yang tidak berkorelasi  seringkali peningkatan akurasi model yang signifikan dapat dicapai decision tree yang memiliki kelemahan overfitting dapat dikurangi dengan menggunakan algoritma random forest']}>,\n",
              " <MultiLabel: {'labels': [{'id': '460960c3-cef3-4ab7-b4a8-2e56fd43d0d9', 'query': 'kapan kondisi berhenti dicapai pada algoritma decision tree?', 'document': {'id': 'e273de42cf736e9d5a5f220a121cfbbb-0', 'content': 'setiap pemisahan menghasilkan cabang baru dalam pohon keputusan  yang mewakili kondisi yang harus dipenuhi proses ini dilakukan secara berulang hingga mencapai kondisi berhenti  seperti mencapai tingkat kedalaman maksimum atau tidak ada lagi pemisahan yang memberikan peningkatan signifikan dalam pemisahan kelas pada akhirnya  model decision tree dapat digunakan untuk memprediksi kelas dari data uji dengan mengikuti serangkaian kondisi yang ditentukan oleh pohon keputusan 3  naive bayes teorema bayes merupakan teorema yang memberikan cara untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati berbagai data yang diberikan hipotesis dan data yang diamati itu sendiri', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1031, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kondisi berhenti  seperti mencapai tingkat kedalaman maksimum atau tidak ada lagi pemisahan yang memberikan peningkatan signifikan dalam pemisahan kelas', 'type': 'extractive', 'score': 0.0, 'context': 'setiap pemisahan menghasilkan cabang baru dalam pohon keputusan  yang mewakili kondisi yang harus dipenuhi proses ini dilakukan secara berulang hingga mencapai kondisi berhenti  seperti mencapai tingkat kedalaman maksimum atau tidak ada lagi pemisahan yang memberikan peningkatan signifikan dalam pemisahan kelas pada akhirnya  model decision tree dapat digunakan untuk memprediksi kelas dari data uji dengan mengikuti serangkaian kondisi yang ditentukan oleh pohon keputusan 3  naive bayes teorema bayes merupakan teorema yang memberikan cara untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati berbagai data yang diberikan hipotesis dan data yang diamati itu sendiri', 'offsets_in_document': [{'start': 160, 'end': 312}], 'offsets_in_context': [{'start': 160, 'end': 312}], 'document_ids': ['e273de42cf736e9d5a5f220a121cfbbb-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'kapan kondisi berhenti dicapai pada algoritma decision tree?', 'filters': None, 'id': '2adccf16c66919e2a0faf0ba4fdcf06d', 'no_answer': False, 'answers': ['kondisi berhenti  seperti mencapai tingkat kedalaman maksimum atau tidak ada lagi pemisahan yang memberikan peningkatan signifikan dalam pemisahan kelas'], 'offsets_in_documents': [{'start': 160, 'end': 312}], 'offsets_in_contexts': [{'start': 160, 'end': 312}], 'document_ids': ['e273de42cf736e9d5a5f220a121cfbbb-0'], 'contexts': ['setiap pemisahan menghasilkan cabang baru dalam pohon keputusan  yang mewakili kondisi yang harus dipenuhi proses ini dilakukan secara berulang hingga mencapai kondisi berhenti  seperti mencapai tingkat kedalaman maksimum atau tidak ada lagi pemisahan yang memberikan peningkatan signifikan dalam pemisahan kelas pada akhirnya  model decision tree dapat digunakan untuk memprediksi kelas dari data uji dengan mengikuti serangkaian kondisi yang ditentukan oleh pohon keputusan 3  naive bayes teorema bayes merupakan teorema yang memberikan cara untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati berbagai data yang diberikan hipotesis dan data yang diamati itu sendiri']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3fa42fa5-18a7-42f6-8e5f-71f32435b81b', 'query': 'apa yang disebut probabilitas awal dari h pada teorema bayes ', 'document': {'id': 'f707fac5c73aa98852fbf0be03881cba-0', 'content': '3  naive bayes teorema bayes merupakan teorema yang memberikan cara untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati berbagai data yang diberikan hipotesis dan data yang diamati itu sendiri naive bayes menggunakan teorema bayes untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati data yang diberikan hipotesis  dan data yang diamati itu sendiri hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1023, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'pengetahuan awal ', 'type': 'extractive', 'score': 0.0, 'context': '3  naive bayes teorema bayes merupakan teorema yang memberikan cara untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati berbagai data yang diberikan hipotesis dan data yang diamati itu sendiri naive bayes menggunakan teorema bayes untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati data yang diberikan hipotesis  dan data yang diamati itu sendiri hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h', 'offsets_in_document': [{'start': 651, 'end': 668}], 'offsets_in_context': [{'start': 651, 'end': 668}], 'document_ids': ['f707fac5c73aa98852fbf0be03881cba-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang disebut probabilitas awal dari h pada teorema bayes ', 'filters': None, 'id': '68f2dfab1e619964e95bcfb454ffbb15', 'no_answer': False, 'answers': ['pengetahuan awal '], 'offsets_in_documents': [{'start': 651, 'end': 668}], 'offsets_in_contexts': [{'start': 651, 'end': 668}], 'document_ids': ['f707fac5c73aa98852fbf0be03881cba-0'], 'contexts': ['3  naive bayes teorema bayes merupakan teorema yang memberikan cara untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati berbagai data yang diberikan hipotesis dan data yang diamati itu sendiri naive bayes menggunakan teorema bayes untuk menghitung probabilitas suatu hipotesis berdasarkan probabilitas awalnya  probabilitas mengamati data yang diberikan hipotesis  dan data yang diamati itu sendiri hal ini membantu dalam menentukan hipotesis yang paling mungkin berdasarkan data pelatihan yang diamati tujuan teorema bayes adalah untuk menentukan hipotesis yang paling mungkin  diberikan data d dan pengetahuan awal tentang probabilitas awal dari berbagai hipotesis dalam h']}>,\n",
              " <MultiLabel: {'labels': [{'id': '38fbab81-1e5c-434c-8244-210fce277eb3', 'query': 'Apa saja praktik yang dilakukan oleh ilmuwan data?', 'document': {'id': 'cd462bb44ba7af67c313a979d3de14f-0', 'content': 'sebagai kunci dalam memanfaatkan potensi big data  saintis data membawa struktur ke dalamnya  menemukan pola menarik  dan memberikan saran kepada eksekutif tentang implikasi untuk produk  proses  dan pengambilan keputusan yang lebih baik dengan demikian  seorang saintis data perlu memiliki latar belakang dalam pembelajaran mesin  statistik  pemrograman  terutama python atau r   matematika  dan bidang database sains data telah mengalami evolusi yang signifikan  mulai dari fokus pada pengolahan data dan analisis statistik tradisional hingga pendekatan yang lebih holistik menggunakan kombinasi statistik  pembelajaran mesin  ilmu komputer  dan keahlian domain praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang selama proses ini  ilmuwan data menggunakan berbagai teknik dan alat untuk mengoptimalkan penggunaan data dan menghasilkan wawasan yang berharga', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1002, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang', 'type': 'extractive', 'score': 0.0, 'context': 'sebagai kunci dalam memanfaatkan potensi big data  saintis data membawa struktur ke dalamnya  menemukan pola menarik  dan memberikan saran kepada eksekutif tentang implikasi untuk produk  proses  dan pengambilan keputusan yang lebih baik dengan demikian  seorang saintis data perlu memiliki latar belakang dalam pembelajaran mesin  statistik  pemrograman  terutama python atau r   matematika  dan bidang database sains data telah mengalami evolusi yang signifikan  mulai dari fokus pada pengolahan data dan analisis statistik tradisional hingga pendekatan yang lebih holistik menggunakan kombinasi statistik  pembelajaran mesin  ilmu komputer  dan keahlian domain praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang selama proses ini  ilmuwan data menggunakan berbagai teknik dan alat untuk mengoptimalkan penggunaan data dan menghasilkan wawasan yang berharga', 'offsets_in_document': [{'start': 664, 'end': 823}], 'offsets_in_context': [{'start': 664, 'end': 823}], 'document_ids': ['cd462bb44ba7af67c313a979d3de14f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Apa saja praktik yang dilakukan oleh ilmuwan data?', 'filters': None, 'id': '0fee78114c8ca54b3ebcc7bf1d4ef189', 'no_answer': False, 'answers': ['praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang'], 'offsets_in_documents': [{'start': 664, 'end': 823}], 'offsets_in_contexts': [{'start': 664, 'end': 823}], 'document_ids': ['cd462bb44ba7af67c313a979d3de14f-0'], 'contexts': ['sebagai kunci dalam memanfaatkan potensi big data  saintis data membawa struktur ke dalamnya  menemukan pola menarik  dan memberikan saran kepada eksekutif tentang implikasi untuk produk  proses  dan pengambilan keputusan yang lebih baik dengan demikian  seorang saintis data perlu memiliki latar belakang dalam pembelajaran mesin  statistik  pemrograman  terutama python atau r   matematika  dan bidang database sains data telah mengalami evolusi yang signifikan  mulai dari fokus pada pengolahan data dan analisis statistik tradisional hingga pendekatan yang lebih holistik menggunakan kombinasi statistik  pembelajaran mesin  ilmu komputer  dan keahlian domain praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang selama proses ini  ilmuwan data menggunakan berbagai teknik dan alat untuk mengoptimalkan penggunaan data dan menghasilkan wawasan yang berharga']}>,\n",
              " <MultiLabel: {'labels': [{'id': '724e33dc-9dd6-453b-a6b3-7b36231edb0d', 'query': 'apa yang dilakukan dalam langkah evaluasi dalam praktik ilmuwan data ', 'document': {'id': '354e1db6deb787ef69b3110357593063-0', 'content': 'eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut setelah itu  model diuji menggunakan data uji untuk memeriksa keakuratannya dan melakukan perubahan jika diperlukan langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1009, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi', 'type': 'extractive', 'score': 0.0, 'context': 'eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut setelah itu  model diuji menggunakan data uji untuk memeriksa keakuratannya dan melakukan perubahan jika diperlukan langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan', 'offsets_in_document': [{'start': 448, 'end': 561}], 'offsets_in_context': [{'start': 448, 'end': 561}], 'document_ids': ['354e1db6deb787ef69b3110357593063-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang dilakukan dalam langkah evaluasi dalam praktik ilmuwan data ', 'filters': None, 'id': '67bef658985c7ca0c4dd17ed340bd487', 'no_answer': False, 'answers': ['menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi'], 'offsets_in_documents': [{'start': 448, 'end': 561}], 'offsets_in_contexts': [{'start': 448, 'end': 561}], 'document_ids': ['354e1db6deb787ef69b3110357593063-0'], 'contexts': ['eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut setelah itu  model diuji menggunakan data uji untuk memeriksa keakuratannya dan melakukan perubahan jika diperlukan langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b0fc6b5c-e350-4178-87e6-cac5620275de', 'query': 'mengapa penting menguji model dalam sains data ', 'document': {'id': '354e1db6deb787ef69b3110357593063-0', 'content': 'eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut setelah itu  model diuji menggunakan data uji untuk memeriksa keakuratannya dan melakukan perubahan jika diperlukan langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1009, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'memeriksa keakuratannya dan melakukan perubahan jika diperlukan ', 'type': 'extractive', 'score': 0.0, 'context': 'eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut setelah itu  model diuji menggunakan data uji untuk memeriksa keakuratannya dan melakukan perubahan jika diperlukan langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan', 'offsets_in_document': [{'start': 360, 'end': 424}], 'offsets_in_context': [{'start': 360, 'end': 424}], 'document_ids': ['354e1db6deb787ef69b3110357593063-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'mengapa penting menguji model dalam sains data ', 'filters': None, 'id': '1278772320b0863da63824910d28bdc8', 'no_answer': False, 'answers': ['memeriksa keakuratannya dan melakukan perubahan jika diperlukan '], 'offsets_in_documents': [{'start': 360, 'end': 424}], 'offsets_in_contexts': [{'start': 360, 'end': 424}], 'document_ids': ['354e1db6deb787ef69b3110357593063-0'], 'contexts': ['eksplorasi data melibatkan pengambilan sampel dan transformasi data  serta memeriksa hubungan antara fitur fitur dalam data menggunakan metode statistik selanjutnya  pemodelan melibatkan pemilihan model yang sesuai  seperti menggunakan algoritma machine learning  dan memasukkan data ke dalam model tersebut setelah itu  model diuji menggunakan data uji untuk memeriksa keakuratannya dan melakukan perubahan jika diperlukan langkah terakhir adalah menerapkan model yang memberikan hasil terbaik sesuai dengan pengujian  dan menyebarkannya di lingkungan produksi untuk digunakan dalam konteks bisnis karakteristik penting dalam sains data meliputi pemahaman bisnis  intuisi  dan keingintahuan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c884bdb5-7e9f-4592-af64-3b7a7fb3fb9b', 'query': 'bagaimana aplikasi sains data dalam berbagai indiustri?', 'document': {'id': '89ee0b414578c0ae199d05215f4d18ee-0', 'content': 'pemerintah juga dapat menggunakan sains data untuk menyusun kebijakan yang lebih baik berdasarkan data yang tersedia contoh aplikasi sains data dalam pemasaran dan iklan meliputi penggunaan analisis prediktif dan deskriptif untuk mengelompokkan pelanggan  serta analisis media sosial untuk memahami perilaku dan pola pikir pelanggan di sektor keuangan dan perbankan  sains data digunakan dalam analisis risiko  manajemen data pelanggan  dan deteksi penipuan dalam ritel dan e commerce  sains data dapat digunakan untuk sistem rekomendasi  analisis prediktif  analisis sentimen  dan lainnya secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1013, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif', 'type': 'extractive', 'score': 0.0, 'context': 'pemerintah juga dapat menggunakan sains data untuk menyusun kebijakan yang lebih baik berdasarkan data yang tersedia contoh aplikasi sains data dalam pemasaran dan iklan meliputi penggunaan analisis prediktif dan deskriptif untuk mengelompokkan pelanggan  serta analisis media sosial untuk memahami perilaku dan pola pikir pelanggan di sektor keuangan dan perbankan  sains data digunakan dalam analisis risiko  manajemen data pelanggan  dan deteksi penipuan dalam ritel dan e commerce  sains data dapat digunakan untuk sistem rekomendasi  analisis prediktif  analisis sentimen  dan lainnya secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif', 'offsets_in_document': [{'start': 590, 'end': 783}], 'offsets_in_context': [{'start': 590, 'end': 783}], 'document_ids': ['89ee0b414578c0ae199d05215f4d18ee-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana aplikasi sains data dalam berbagai indiustri?', 'filters': None, 'id': '5bd966c7a1b6092b4cd868eb6b3fbafe', 'no_answer': False, 'answers': ['secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif'], 'offsets_in_documents': [{'start': 590, 'end': 783}], 'offsets_in_contexts': [{'start': 590, 'end': 783}], 'document_ids': ['89ee0b414578c0ae199d05215f4d18ee-0'], 'contexts': ['pemerintah juga dapat menggunakan sains data untuk menyusun kebijakan yang lebih baik berdasarkan data yang tersedia contoh aplikasi sains data dalam pemasaran dan iklan meliputi penggunaan analisis prediktif dan deskriptif untuk mengelompokkan pelanggan  serta analisis media sosial untuk memahami perilaku dan pola pikir pelanggan di sektor keuangan dan perbankan  sains data digunakan dalam analisis risiko  manajemen data pelanggan  dan deteksi penipuan dalam ritel dan e commerce  sains data dapat digunakan untuk sistem rekomendasi  analisis prediktif  analisis sentimen  dan lainnya secara keseluruhan  sains data memiliki aplikasi yang luas dan beragam dalam berbagai industri  membantu dalam pengambilan keputusan yang lebih baik dan memanfaatkan potensi data secara efektif']}>,\n",
              " <MultiLabel: {'labels': [{'id': '7ab25988-cc75-4013-9a60-7c180ba7d512', 'query': 'apa kegunaan fungsi f pada supervised learning?', 'document': {'id': 'c6903a90d23f12b803c8668c079e2f83-0', 'content': 'jenis metode pembelajaran dibagi menjadi tiga yaitu supervised learning pembelajaran terbimbing   unsupervised learning pembelajaran tidak terbimbing   reinforcement learning kali ini kita akan membahas lebih detail tentang metode pembelajaran supervised learning supervised learning menggunakan dataset yang terdiri atas pasangan variable input x  dan output y  dimana supervised learning melakukan pembelajaran terhadap variable input x  dan output y  untuk mendapatkan fungsi f x  yang tepat yang dapat memetakan input x menjadi y variable ouput y berperan sebagai teacher yang membimbing proses pembelajaran  setelah fungsi f didapatkan  maka fungsi tersebut dapat digunakan untuk memetakan  memprediksi  input data baru  x  dengan tepat  fungsi f yang dihasilkan dari proses pembelajaran disebut juga sebagai model  classification model terdapat 3 jenis supervised learning yaitu 1  berdasarkan tipe ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu regresi dan klasifikasi  regresi  jika ouput y bernilai kontinu dan klasifikasi  jika ouput y bernilai diskret kategori', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1018, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' fungsi tersebut dapat digunakan untuk memetakan  memprediksi  input data baru  x  dengan tepat', 'type': 'extractive', 'score': 0.0, 'context': 'jenis metode pembelajaran dibagi menjadi tiga yaitu supervised learning pembelajaran terbimbing   unsupervised learning pembelajaran tidak terbimbing   reinforcement learning kali ini kita akan membahas lebih detail tentang metode pembelajaran supervised learning supervised learning menggunakan dataset yang terdiri atas pasangan variable input x  dan output y  dimana supervised learning melakukan pembelajaran terhadap variable input x  dan output y  untuk mendapatkan fungsi f x  yang tepat yang dapat memetakan input x menjadi y variable ouput y berperan sebagai teacher yang membimbing proses pembelajaran  setelah fungsi f didapatkan  maka fungsi tersebut dapat digunakan untuk memetakan  memprediksi  input data baru  x  dengan tepat  fungsi f yang dihasilkan dari proses pembelajaran disebut juga sebagai model  classification model terdapat 3 jenis supervised learning yaitu 1  berdasarkan tipe ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu regresi dan klasifikasi  regresi  jika ouput y bernilai kontinu dan klasifikasi  jika ouput y bernilai diskret kategori', 'offsets_in_document': [{'start': 646, 'end': 741}], 'offsets_in_context': [{'start': 646, 'end': 741}], 'document_ids': ['c6903a90d23f12b803c8668c079e2f83-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa kegunaan fungsi f pada supervised learning?', 'filters': None, 'id': 'fd2c70d9786deb79c3700e2c26767652', 'no_answer': False, 'answers': [' fungsi tersebut dapat digunakan untuk memetakan  memprediksi  input data baru  x  dengan tepat'], 'offsets_in_documents': [{'start': 646, 'end': 741}], 'offsets_in_contexts': [{'start': 646, 'end': 741}], 'document_ids': ['c6903a90d23f12b803c8668c079e2f83-0'], 'contexts': ['jenis metode pembelajaran dibagi menjadi tiga yaitu supervised learning pembelajaran terbimbing   unsupervised learning pembelajaran tidak terbimbing   reinforcement learning kali ini kita akan membahas lebih detail tentang metode pembelajaran supervised learning supervised learning menggunakan dataset yang terdiri atas pasangan variable input x  dan output y  dimana supervised learning melakukan pembelajaran terhadap variable input x  dan output y  untuk mendapatkan fungsi f x  yang tepat yang dapat memetakan input x menjadi y variable ouput y berperan sebagai teacher yang membimbing proses pembelajaran  setelah fungsi f didapatkan  maka fungsi tersebut dapat digunakan untuk memetakan  memprediksi  input data baru  x  dengan tepat  fungsi f yang dihasilkan dari proses pembelajaran disebut juga sebagai model  classification model terdapat 3 jenis supervised learning yaitu 1  berdasarkan tipe ouputnya  pada jenis ini supervised learning dibagi menjadi 2 yaitu regresi dan klasifikasi  regresi  jika ouput y bernilai kontinu dan klasifikasi  jika ouput y bernilai diskret kategori']}>,\n",
              " <MultiLabel: {'labels': [{'id': '6744f5a3-8bfd-478d-b88b-0f1aa0a7ac86', 'query': 'mengapa stratified sampling digunakan dalam stratified k folds cross validation ', 'document': {'id': 'd294435a23a57ffa9a0c3384742c4552-0', 'content': 'dalam setiap iterasi  salah satu subset digunakan sebagai data validasi  sementara k 1 subset lainnya digunakan sebagai data pelatihan proses ini diulang k kali  sehingga setiap subset digunakan sebagai data validasi tepat satu kali proses pemilihan data untuk masing masing fold dilakukan menggunakan random sampling 2  stratified k folds cross validation  prinsipnya sama dengan k folds cross validation  hanya saja perbedaannya pada proses pemilihan data untuk masing masing fold dilakukan menggunakan stratified sampling 3  leave one out cross validation  merupakan bentuk khusus dari k folds cross validation  dimana k n  untuk n adalah total jumlah data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1028, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' proses pemilihan data untuk masing masing fold dilakukan menggunakan stratified sampling', 'type': 'extractive', 'score': 0.0, 'context': 'dalam setiap iterasi  salah satu subset digunakan sebagai data validasi  sementara k 1 subset lainnya digunakan sebagai data pelatihan proses ini diulang k kali  sehingga setiap subset digunakan sebagai data validasi tepat satu kali proses pemilihan data untuk masing masing fold dilakukan menggunakan random sampling 2  stratified k folds cross validation  prinsipnya sama dengan k folds cross validation  hanya saja perbedaannya pada proses pemilihan data untuk masing masing fold dilakukan menggunakan stratified sampling 3  leave one out cross validation  merupakan bentuk khusus dari k folds cross validation  dimana k n  untuk n adalah total jumlah data', 'offsets_in_document': [{'start': 435, 'end': 524}], 'offsets_in_context': [{'start': 435, 'end': 524}], 'document_ids': ['d294435a23a57ffa9a0c3384742c4552-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'mengapa stratified sampling digunakan dalam stratified k folds cross validation ', 'filters': None, 'id': '1f17abd97b04d866a13ffe3700995d1d', 'no_answer': False, 'answers': [' proses pemilihan data untuk masing masing fold dilakukan menggunakan stratified sampling'], 'offsets_in_documents': [{'start': 435, 'end': 524}], 'offsets_in_contexts': [{'start': 435, 'end': 524}], 'document_ids': ['d294435a23a57ffa9a0c3384742c4552-0'], 'contexts': ['dalam setiap iterasi  salah satu subset digunakan sebagai data validasi  sementara k 1 subset lainnya digunakan sebagai data pelatihan proses ini diulang k kali  sehingga setiap subset digunakan sebagai data validasi tepat satu kali proses pemilihan data untuk masing masing fold dilakukan menggunakan random sampling 2  stratified k folds cross validation  prinsipnya sama dengan k folds cross validation  hanya saja perbedaannya pada proses pemilihan data untuk masing masing fold dilakukan menggunakan stratified sampling 3  leave one out cross validation  merupakan bentuk khusus dari k folds cross validation  dimana k n  untuk n adalah total jumlah data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '069b027f-3148-421e-8f73-e809715d1649', 'query': 'apa itu confusion matrix?', 'document': {'id': 'cd0251bc61de952056bf3f1b8a32839e-0', 'content': '2  stratified k folds cross validation  prinsipnya sama dengan k folds cross validation  hanya saja perbedaannya pada proses pemilihan data untuk masing masing fold dilakukan menggunakan stratified sampling 3  leave one out cross validation  merupakan bentuk khusus dari k folds cross validation  dimana k n  untuk n adalah total jumlah data dilakukan n eksperimen dimana training set terdiri dari n  1 samples data serta 1 buah sampel data untuk validation set teknik ini tidak menjamin distribusi sample data yang sama untuk masing masing kelas selanjutnya ada confusion matrix  matriks yang digunakan untuk merangkum hasil klasifikasi yang benar atau salah yang dihasilkan oleh sebuah metode model klasifikasi pada suatu dataset', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1034, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' confusion matrix  matriks yang digunakan untuk merangkum hasil klasifikasi yang benar atau salah yang dihasilkan oleh sebuah metode', 'type': 'extractive', 'score': 0.0, 'context': '2  stratified k folds cross validation  prinsipnya sama dengan k folds cross validation  hanya saja perbedaannya pada proses pemilihan data untuk masing masing fold dilakukan menggunakan stratified sampling 3  leave one out cross validation  merupakan bentuk khusus dari k folds cross validation  dimana k n  untuk n adalah total jumlah data dilakukan n eksperimen dimana training set terdiri dari n  1 samples data serta 1 buah sampel data untuk validation set teknik ini tidak menjamin distribusi sample data yang sama untuk masing masing kelas selanjutnya ada confusion matrix  matriks yang digunakan untuk merangkum hasil klasifikasi yang benar atau salah yang dihasilkan oleh sebuah metode model klasifikasi pada suatu dataset', 'offsets_in_document': [{'start': 562, 'end': 694}], 'offsets_in_context': [{'start': 562, 'end': 694}], 'document_ids': ['cd0251bc61de952056bf3f1b8a32839e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa itu confusion matrix?', 'filters': None, 'id': '02900f5df84644c8bbe42adf677392b7', 'no_answer': False, 'answers': [' confusion matrix  matriks yang digunakan untuk merangkum hasil klasifikasi yang benar atau salah yang dihasilkan oleh sebuah metode'], 'offsets_in_documents': [{'start': 562, 'end': 694}], 'offsets_in_contexts': [{'start': 562, 'end': 694}], 'document_ids': ['cd0251bc61de952056bf3f1b8a32839e-0'], 'contexts': ['2  stratified k folds cross validation  prinsipnya sama dengan k folds cross validation  hanya saja perbedaannya pada proses pemilihan data untuk masing masing fold dilakukan menggunakan stratified sampling 3  leave one out cross validation  merupakan bentuk khusus dari k folds cross validation  dimana k n  untuk n adalah total jumlah data dilakukan n eksperimen dimana training set terdiri dari n  1 samples data serta 1 buah sampel data untuk validation set teknik ini tidak menjamin distribusi sample data yang sama untuk masing masing kelas selanjutnya ada confusion matrix  matriks yang digunakan untuk merangkum hasil klasifikasi yang benar atau salah yang dihasilkan oleh sebuah metode model klasifikasi pada suatu dataset']}>,\n",
              " <MultiLabel: {'labels': [{'id': '570ca197-5677-4316-8165-e6402d180fe2', 'query': 'apa tahap pertama dalam proyek analitik data ', 'document': {'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0', 'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 996, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'business understanding adalah tahap pertama dalam proyek analitik data', 'type': 'extractive', 'score': 0.0, 'context': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'offsets_in_document': [{'start': 1, 'end': 71}], 'offsets_in_context': [{'start': 1, 'end': 71}], 'document_ids': ['7b13cd5e078eaf6a4ae6e39eaafe5672-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa tahap pertama dalam proyek analitik data ', 'filters': None, 'id': '7032311a241fdf629188559f0671e0c6', 'no_answer': False, 'answers': ['business understanding adalah tahap pertama dalam proyek analitik data'], 'offsets_in_documents': [{'start': 1, 'end': 71}], 'offsets_in_contexts': [{'start': 1, 'end': 71}], 'document_ids': ['7b13cd5e078eaf6a4ae6e39eaafe5672-0'], 'contexts': [' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya']}>,\n",
              " <MultiLabel: {'labels': [{'id': '4d0ebc99-5ce5-4487-8708-c5c1f7590302', 'query': 'mengapa proyek data sains juga disebut sebagai proyek bisnis ', 'document': {'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0', 'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 996, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis ', 'type': 'extractive', 'score': 0.0, 'context': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'offsets_in_document': [{'start': 157, 'end': 267}], 'offsets_in_context': [{'start': 157, 'end': 267}], 'document_ids': ['7b13cd5e078eaf6a4ae6e39eaafe5672-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'mengapa proyek data sains juga disebut sebagai proyek bisnis ', 'filters': None, 'id': 'c7140e283f3faa6aa7fd9e1ea5aad078', 'no_answer': False, 'answers': ['selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis '], 'offsets_in_documents': [{'start': 157, 'end': 267}], 'offsets_in_contexts': [{'start': 157, 'end': 267}], 'document_ids': ['7b13cd5e078eaf6a4ae6e39eaafe5672-0'], 'contexts': [' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f6014928-cd43-4f12-9834-cc1a8baddb93', 'query': 'selain harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis  apakah hal penting yang menunjukkan bahwa proyek data sains adalah proyek bisnis ', 'document': {'id': '7b13cd5e078eaf6a4ae6e39eaafe5672-0', 'content': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 996, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' memiliki visi yang selaras dengan bisnis', 'type': 'extractive', 'score': 0.0, 'context': ' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya', 'offsets_in_document': [{'start': 225, 'end': 266}], 'offsets_in_context': [{'start': 225, 'end': 266}], 'document_ids': ['7b13cd5e078eaf6a4ae6e39eaafe5672-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'selain harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis  apakah hal penting yang menunjukkan bahwa proyek data sains adalah proyek bisnis ', 'filters': None, 'id': 'be6c1e701ce6b4ded2932742683520fd', 'no_answer': False, 'answers': [' memiliki visi yang selaras dengan bisnis'], 'offsets_in_documents': [{'start': 225, 'end': 266}], 'offsets_in_contexts': [{'start': 225, 'end': 266}], 'document_ids': ['7b13cd5e078eaf6a4ae6e39eaafe5672-0'], 'contexts': [' business understanding adalah tahap pertama dalam proyek analitik data penting untuk diingat bahwa proyek data science adalah proyek bisnis  sehingga harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis dan memiliki visi yang selaras dengan bisnis fokus utama tahap ini adalah menentukan masalah apa yang akan dicoba diselesaikan tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3b793a1a-a904-4f51-96c3-f4d50b33cebb', 'query': 'algortima apa yang menggunakan tree sebagai model classifiernya ', 'document': {'id': '60a71be744408bf3bcc944104dd91478-0', 'content': 'algoritma ini mempunyai teknik yang sederhana karena tidak diperlukan pengetahuan awal mengenai bagaimana distribusi data cara kerja dari algortima ini yang pertama algortima ini mengklasifikasikan data baru  test tuple  berdasarkan kelas data dari k tetangga terdekat  nearest neighbors tetangga terdekat adalah training data yang memiliki tingkat kemiripan  similarity  tertinggi atau jarak  distance  terdekat terhadap test tuple 2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1029, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'decision tree  ', 'type': 'extractive', 'score': 0.0, 'context': 'algoritma ini mempunyai teknik yang sederhana karena tidak diperlukan pengetahuan awal mengenai bagaimana distribusi data cara kerja dari algortima ini yang pertama algortima ini mengklasifikasikan data baru  test tuple  berdasarkan kelas data dari k tetangga terdekat  nearest neighbors tetangga terdekat adalah training data yang memiliki tingkat kemiripan  similarity  tertinggi atau jarak  distance  terdekat terhadap test tuple 2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node', 'offsets_in_document': [{'start': 436, 'end': 451}], 'offsets_in_context': [{'start': 436, 'end': 451}], 'document_ids': ['60a71be744408bf3bcc944104dd91478-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'algortima apa yang menggunakan tree sebagai model classifiernya ', 'filters': None, 'id': '6dbd87445f73cac8cc18af2bc41c56af', 'no_answer': False, 'answers': ['decision tree  '], 'offsets_in_documents': [{'start': 436, 'end': 451}], 'offsets_in_contexts': [{'start': 436, 'end': 451}], 'document_ids': ['60a71be744408bf3bcc944104dd91478-0'], 'contexts': ['algoritma ini mempunyai teknik yang sederhana karena tidak diperlukan pengetahuan awal mengenai bagaimana distribusi data cara kerja dari algortima ini yang pertama algortima ini mengklasifikasikan data baru  test tuple  berdasarkan kelas data dari k tetangga terdekat  nearest neighbors tetangga terdekat adalah training data yang memiliki tingkat kemiripan  similarity  tertinggi atau jarak  distance  terdekat terhadap test tuple 2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node']}>,\n",
              " <MultiLabel: {'labels': [{'id': '7f82f2c2-87a7-4867-8539-6740f8be8176', 'query': 'Apa alat yang digunakan untuk analisis data?', 'document': {'id': '41394d2d19942e654d7ff3c2158b0ea4-0', 'content': 'ada berbagai alat yang tersedia bagi ilmuwan data dalam melaksanakan tugas tugas mereka untuk analisis data  alat yang umum digunakan meliputi percikan  python  dan sas dalam hal gudang data  alat seperti hadoop  sql  dan sarang lebah digunakan untuk mengelola dan menyimpan data dalam skala besar untuk visualisasi data  r  tableau  dan mentah menjadi pilihan yang populer untuk membuat visualisasi yang menarik dan informatif sedangkan dalam pembelajaran mesin  alat seperti percikan  azure ml studio  dan mahout digunakan untuk mengembangkan dan menerapkan model pembelajaran mesin', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1010, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'untuk analisis data  alat yang umum digunakan meliputi percikan  python  dan sas', 'type': 'extractive', 'score': 0.0, 'context': 'ada berbagai alat yang tersedia bagi ilmuwan data dalam melaksanakan tugas tugas mereka untuk analisis data  alat yang umum digunakan meliputi percikan  python  dan sas dalam hal gudang data  alat seperti hadoop  sql  dan sarang lebah digunakan untuk mengelola dan menyimpan data dalam skala besar untuk visualisasi data  r  tableau  dan mentah menjadi pilihan yang populer untuk membuat visualisasi yang menarik dan informatif sedangkan dalam pembelajaran mesin  alat seperti percikan  azure ml studio  dan mahout digunakan untuk mengembangkan dan menerapkan model pembelajaran mesin', 'offsets_in_document': [{'start': 88, 'end': 168}], 'offsets_in_context': [{'start': 88, 'end': 168}], 'document_ids': ['41394d2d19942e654d7ff3c2158b0ea4-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Apa alat yang digunakan untuk analisis data?', 'filters': None, 'id': '6477cb234b7205428562adc30c559583', 'no_answer': False, 'answers': ['untuk analisis data  alat yang umum digunakan meliputi percikan  python  dan sas'], 'offsets_in_documents': [{'start': 88, 'end': 168}], 'offsets_in_contexts': [{'start': 88, 'end': 168}], 'document_ids': ['41394d2d19942e654d7ff3c2158b0ea4-0'], 'contexts': ['ada berbagai alat yang tersedia bagi ilmuwan data dalam melaksanakan tugas tugas mereka untuk analisis data  alat yang umum digunakan meliputi percikan  python  dan sas dalam hal gudang data  alat seperti hadoop  sql  dan sarang lebah digunakan untuk mengelola dan menyimpan data dalam skala besar untuk visualisasi data  r  tableau  dan mentah menjadi pilihan yang populer untuk membuat visualisasi yang menarik dan informatif sedangkan dalam pembelajaran mesin  alat seperti percikan  azure ml studio  dan mahout digunakan untuk mengembangkan dan menerapkan model pembelajaran mesin']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd76c453d-09c1-4dcc-8f51-720205b282a4', 'query': 'apa yang melambangkan kelas atau label data pada konteks machine learning  supervised learning  ', 'document': {'id': 'c761a07cd2afd393d2d5d840969564eb-0', 'content': 'terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan mendapatakan hipotesis yang paling mungkin  h  dari sekumpulan hipotesis kandidat  h  berdasarkan data observasi  d  dan maximuum likelihood ml  berlaku ketika dalam beberapa kasus kita akan mengamsumsikan bahwa setiap hipotesis h dalam h mempunyai probabilitas prior yang sama dalam konteks machine learning  supervised learning  perlu diingat bahwa d merupakan data training atau data pembelajaran dan h adalah kelas atau label data 4  support vector machine  linear classifiers dalam support vector machine  svm  adalah salah satu metode pengklasifikasi yang digunakan untuk memisahkan dua kelas data dengan menggunakan garis linear atau hyperplane svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik  ', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1024, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'h adalah kelas atau label', 'type': 'extractive', 'score': 0.0, 'context': 'terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan mendapatakan hipotesis yang paling mungkin  h  dari sekumpulan hipotesis kandidat  h  berdasarkan data observasi  d  dan maximuum likelihood ml  berlaku ketika dalam beberapa kasus kita akan mengamsumsikan bahwa setiap hipotesis h dalam h mempunyai probabilitas prior yang sama dalam konteks machine learning  supervised learning  perlu diingat bahwa d merupakan data training atau data pembelajaran dan h adalah kelas atau label data 4  support vector machine  linear classifiers dalam support vector machine  svm  adalah salah satu metode pengklasifikasi yang digunakan untuk memisahkan dua kelas data dengan menggunakan garis linear atau hyperplane svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik  ', 'offsets_in_document': [{'start': 646, 'end': 671}], 'offsets_in_context': [{'start': 646, 'end': 671}], 'document_ids': ['c761a07cd2afd393d2d5d840969564eb-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang melambangkan kelas atau label data pada konteks machine learning  supervised learning  ', 'filters': None, 'id': 'd2d421142de0d7615aa358c2a1df6a8a', 'no_answer': False, 'answers': ['h adalah kelas atau label'], 'offsets_in_documents': [{'start': 646, 'end': 671}], 'offsets_in_contexts': [{'start': 646, 'end': 671}], 'document_ids': ['c761a07cd2afd393d2d5d840969564eb-0'], 'contexts': ['terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan mendapatakan hipotesis yang paling mungkin  h  dari sekumpulan hipotesis kandidat  h  berdasarkan data observasi  d  dan maximuum likelihood ml  berlaku ketika dalam beberapa kasus kita akan mengamsumsikan bahwa setiap hipotesis h dalam h mempunyai probabilitas prior yang sama dalam konteks machine learning  supervised learning  perlu diingat bahwa d merupakan data training atau data pembelajaran dan h adalah kelas atau label data 4  support vector machine  linear classifiers dalam support vector machine  svm  adalah salah satu metode pengklasifikasi yang digunakan untuk memisahkan dua kelas data dengan menggunakan garis linear atau hyperplane svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik  ']}>,\n",
              " <MultiLabel: {'labels': [{'id': '280e6d87-8259-47e9-959f-0795c4a47c73', 'query': 'apa yang melambangkan data training atau data pembelajaran pada konteks machine learning  supervised learning  ', 'document': {'id': 'c761a07cd2afd393d2d5d840969564eb-0', 'content': 'terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan mendapatakan hipotesis yang paling mungkin  h  dari sekumpulan hipotesis kandidat  h  berdasarkan data observasi  d  dan maximuum likelihood ml  berlaku ketika dalam beberapa kasus kita akan mengamsumsikan bahwa setiap hipotesis h dalam h mempunyai probabilitas prior yang sama dalam konteks machine learning  supervised learning  perlu diingat bahwa d merupakan data training atau data pembelajaran dan h adalah kelas atau label data 4  support vector machine  linear classifiers dalam support vector machine  svm  adalah salah satu metode pengklasifikasi yang digunakan untuk memisahkan dua kelas data dengan menggunakan garis linear atau hyperplane svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik  ', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1024, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'd merupakan data training atau data pembelajaran', 'type': 'extractive', 'score': 0.0, 'context': 'terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan mendapatakan hipotesis yang paling mungkin  h  dari sekumpulan hipotesis kandidat  h  berdasarkan data observasi  d  dan maximuum likelihood ml  berlaku ketika dalam beberapa kasus kita akan mengamsumsikan bahwa setiap hipotesis h dalam h mempunyai probabilitas prior yang sama dalam konteks machine learning  supervised learning  perlu diingat bahwa d merupakan data training atau data pembelajaran dan h adalah kelas atau label data 4  support vector machine  linear classifiers dalam support vector machine  svm  adalah salah satu metode pengklasifikasi yang digunakan untuk memisahkan dua kelas data dengan menggunakan garis linear atau hyperplane svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik  ', 'offsets_in_document': [{'start': 593, 'end': 641}], 'offsets_in_context': [{'start': 593, 'end': 641}], 'document_ids': ['c761a07cd2afd393d2d5d840969564eb-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang melambangkan data training atau data pembelajaran pada konteks machine learning  supervised learning  ', 'filters': None, 'id': 'b83a76308c6c3e2820968a047ccd3ebf', 'no_answer': False, 'answers': ['d merupakan data training atau data pembelajaran'], 'offsets_in_documents': [{'start': 593, 'end': 641}], 'offsets_in_contexts': [{'start': 593, 'end': 641}], 'document_ids': ['c761a07cd2afd393d2d5d840969564eb-0'], 'contexts': ['terdapat dua pendekatan pada bayes learning yaitu maxmimum a posteriori  map  dan maximuum likelihood ml   maxmimum a posteriori  map   berlaku untuk kondisi dimana ketika kita memiliki beberapa alternatif hipotesis  lalu map memiliki tujuan mendapatakan hipotesis yang paling mungkin  h  dari sekumpulan hipotesis kandidat  h  berdasarkan data observasi  d  dan maximuum likelihood ml  berlaku ketika dalam beberapa kasus kita akan mengamsumsikan bahwa setiap hipotesis h dalam h mempunyai probabilitas prior yang sama dalam konteks machine learning  supervised learning  perlu diingat bahwa d merupakan data training atau data pembelajaran dan h adalah kelas atau label data 4  support vector machine  linear classifiers dalam support vector machine  svm  adalah salah satu metode pengklasifikasi yang digunakan untuk memisahkan dua kelas data dengan menggunakan garis linear atau hyperplane svm bertujuan untuk menemukan hyperplane terbaik yang memaksimalkan margin  yaitu jarak terdekat antara hyperplane dan titik data dari kelas yang berbeda beberapa macam fungsi kernel support vector machine  svm   linear  polinomial of degree d  polinomial of degree up to d  gaussian rbf  sigmoid  tangen hiperbolik  ']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e59e2ba3-2ef8-4341-a34b-5289527dd954', 'query': 'Kenapa kemampuan pemrograman harus dikuasai oleh seorang saintis data?', 'document': {'id': '3dbfb2af00a74c750aa5682987cee773-0', 'content': 'seorang saintis data memiliki keahlian dalam beberapa latar belakang yang penting mereka harus memahami konsep pembelajaran mesin untuk menghasilkan model prediktif yang akurat selain itu  pengetahuan statistik diperlukan untuk menganalisis data dan menarik kesimpulan yang valid kemampuan pemrograman  terutama dalam bahasa python atau r  penting dalam mengelola dan memproses data secara efisien pemahaman matematika juga diperlukan dalam pemodelan dan interpretasi data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1000, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kemampuan pemrograman  terutama dalam bahasa python atau r  penting dalam mengelola dan memproses data secara efisien ', 'type': 'extractive', 'score': 0.0, 'context': 'seorang saintis data memiliki keahlian dalam beberapa latar belakang yang penting mereka harus memahami konsep pembelajaran mesin untuk menghasilkan model prediktif yang akurat selain itu  pengetahuan statistik diperlukan untuk menganalisis data dan menarik kesimpulan yang valid kemampuan pemrograman  terutama dalam bahasa python atau r  penting dalam mengelola dan memproses data secara efisien pemahaman matematika juga diperlukan dalam pemodelan dan interpretasi data', 'offsets_in_document': [{'start': 280, 'end': 398}], 'offsets_in_context': [{'start': 280, 'end': 398}], 'document_ids': ['3dbfb2af00a74c750aa5682987cee773-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Kenapa kemampuan pemrograman harus dikuasai oleh seorang saintis data?', 'filters': None, 'id': '70fee558492772bbd81b53439bfe300b', 'no_answer': False, 'answers': ['kemampuan pemrograman  terutama dalam bahasa python atau r  penting dalam mengelola dan memproses data secara efisien '], 'offsets_in_documents': [{'start': 280, 'end': 398}], 'offsets_in_contexts': [{'start': 280, 'end': 398}], 'document_ids': ['3dbfb2af00a74c750aa5682987cee773-0'], 'contexts': ['seorang saintis data memiliki keahlian dalam beberapa latar belakang yang penting mereka harus memahami konsep pembelajaran mesin untuk menghasilkan model prediktif yang akurat selain itu  pengetahuan statistik diperlukan untuk menganalisis data dan menarik kesimpulan yang valid kemampuan pemrograman  terutama dalam bahasa python atau r  penting dalam mengelola dan memproses data secara efisien pemahaman matematika juga diperlukan dalam pemodelan dan interpretasi data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '4cb945b8-15bc-41c2-8c95-8ead4f183b47', 'query': 'siapa yang akan menggunakan informasi yang dihasilkan dari penggunaan data ', 'document': {'id': '7abb9417739996ba5c148ac651c51b01-0', 'content': 'penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data setelah itu  data dievaluasi dan ditafsirkan untuk menghasilkan informasi yang berharga bagi para pengambilan keputusan selanjutnya  informasi tersebut dapat diimplementasikan dalam bentuk produk atau digunakan dalam proses pengambilan keputusan dengan menggunakan pendekatan ini  data dapat memberikan wawasan yang berharga dan mendukung pengambilan keputusan yang lebih baik setelah data dievaluasi dan ditafsirkan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1001, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'para pengambilan keputusan', 'type': 'extractive', 'score': 0.0, 'context': 'penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data setelah itu  data dievaluasi dan ditafsirkan untuk menghasilkan informasi yang berharga bagi para pengambilan keputusan selanjutnya  informasi tersebut dapat diimplementasikan dalam bentuk produk atau digunakan dalam proses pengambilan keputusan dengan menggunakan pendekatan ini  data dapat memberikan wawasan yang berharga dan mendukung pengambilan keputusan yang lebih baik setelah data dievaluasi dan ditafsirkan', 'offsets_in_document': [{'start': 430, 'end': 456}], 'offsets_in_context': [{'start': 430, 'end': 456}], 'document_ids': ['7abb9417739996ba5c148ac651c51b01-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'siapa yang akan menggunakan informasi yang dihasilkan dari penggunaan data ', 'filters': None, 'id': 'e6317fee97b05c24fd25dff23a19c328', 'no_answer': False, 'answers': ['para pengambilan keputusan'], 'offsets_in_documents': [{'start': 430, 'end': 456}], 'offsets_in_contexts': [{'start': 430, 'end': 456}], 'document_ids': ['7abb9417739996ba5c148ac651c51b01-0'], 'contexts': ['penggunaan data melibatkan langkah langkah seperti analisis eksplorasi data untuk memahami karakteristik dan pola data  pembangunan model pengetahuan untuk menggambarkan hubungan antarvariabel  dan pengembangan model prediktif untuk membuat prediksi di masa depan semua langkah langkah tersebut dilakukan di dalam proses penggunaan data setelah itu  data dievaluasi dan ditafsirkan untuk menghasilkan informasi yang berharga bagi para pengambilan keputusan selanjutnya  informasi tersebut dapat diimplementasikan dalam bentuk produk atau digunakan dalam proses pengambilan keputusan dengan menggunakan pendekatan ini  data dapat memberikan wawasan yang berharga dan mendukung pengambilan keputusan yang lebih baik setelah data dievaluasi dan ditafsirkan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '74ad6aa3-32c1-4faf-a66b-01a2a39cd3c9', 'query': 'apa yang dimaksud dengan decision tree ', 'document': {'id': '78cdc899ce51d0c4d8f92800b030249f-0', 'content': '2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node depth node merupakan jumlah langkah minimum yang diperlukan untuk mencapai leaf node dari root node terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1022, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' algortima ini membuat model classifier dalam bentuk tree  pohon ', 'type': 'extractive', 'score': 0.0, 'context': '2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node depth node merupakan jumlah langkah minimum yang diperlukan untuk mencapai leaf node dari root node terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi', 'offsets_in_document': [{'start': 17, 'end': 82}], 'offsets_in_context': [{'start': 17, 'end': 82}], 'document_ids': ['78cdc899ce51d0c4d8f92800b030249f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan decision tree ', 'filters': None, 'id': 'ea29c7ab579c48049ca584d14328bf51', 'no_answer': False, 'answers': [' algortima ini membuat model classifier dalam bentuk tree  pohon '], 'offsets_in_documents': [{'start': 17, 'end': 82}], 'offsets_in_contexts': [{'start': 17, 'end': 82}], 'document_ids': ['78cdc899ce51d0c4d8f92800b030249f-0'], 'contexts': ['2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node depth node merupakan jumlah langkah minimum yang diperlukan untuk mencapai leaf node dari root node terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '5645ee01-166f-4996-bc0e-9831f96452b9', 'query': 'terdiri dari apa saja tree pada decision tree ', 'document': {'id': '78cdc899ce51d0c4d8f92800b030249f-0', 'content': '2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node depth node merupakan jumlah langkah minimum yang diperlukan untuk mencapai leaf node dari root node terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1022, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node depth node merupakan jumlah langkah minimum yang diperlukan untuk mencapai leaf node dari root node ', 'type': 'extractive', 'score': 0.0, 'context': '2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node depth node merupakan jumlah langkah minimum yang diperlukan untuk mencapai leaf node dari root node terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi', 'offsets_in_document': [{'start': 89, 'end': 502}], 'offsets_in_context': [{'start': 89, 'end': 502}], 'document_ids': ['78cdc899ce51d0c4d8f92800b030249f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'terdiri dari apa saja tree pada decision tree ', 'filters': None, 'id': 'e97139c0d94ef19826e839974fc9bd3d', 'no_answer': False, 'answers': ['terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node depth node merupakan jumlah langkah minimum yang diperlukan untuk mencapai leaf node dari root node '], 'offsets_in_documents': [{'start': 89, 'end': 502}], 'offsets_in_contexts': [{'start': 89, 'end': 502}], 'document_ids': ['78cdc899ce51d0c4d8f92800b030249f-0'], 'contexts': ['2  decision tree  algortima ini membuat model classifier dalam bentuk tree  pohon   yang terdiri dari  root node  fitur yang menjadi kriteria pertama pada tree yang dibangun   leaf node  label kelas   internal node  node diantara root node dan leaf node  dan branch  nilai yang mungkin muncul dari fitur terkait yang dipresentasikan pada node  root internal pada tree tersebut juga terdapat depth node depth node merupakan jumlah langkah minimum yang diperlukan untuk mencapai leaf node dari root node terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0081266f-b574-4650-ae3f-b8da6866464d', 'query': 'Bagaimana aplikasi sains data dalam bidang perbankan?', 'document': {'id': '4f4d9c00a8dcebd7db5bfed7dbf17e19-0', 'content': 'sains data memiliki berbagai aplikasi yang relevan dalam berbagai industri dalam bidang pemasaran  sains data dapat digunakan untuk mengembangkan strategi harga berdasarkan analisis data  meningkatkan keuntungan perusahaan  dan mengelompokkan pelanggan berdasarkan kebutuhan mereka di sektor kesehatan  data dapat digunakan untuk mencegah dan memantau masalah kesehatan  sementara di industri perbankan dan keuangan  sains data dapat membantu dalam deteksi penipuan dan pengelolaan aset bank pemerintah juga dapat menggunakan sains data untuk menyusun kebijakan yang lebih baik berdasarkan data yang tersedia contoh aplikasi sains data dalam pemasaran dan iklan meliputi penggunaan analisis prediktif dan deskriptif untuk mengelompokkan pelanggan  serta analisis media sosial untuk memahami perilaku dan pola pikir pelanggan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1012, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' di industri perbankan dan keuangan  sains data dapat membantu dalam deteksi penipuan dan pengelolaan aset bank', 'type': 'extractive', 'score': 0.0, 'context': 'sains data memiliki berbagai aplikasi yang relevan dalam berbagai industri dalam bidang pemasaran  sains data dapat digunakan untuk mengembangkan strategi harga berdasarkan analisis data  meningkatkan keuntungan perusahaan  dan mengelompokkan pelanggan berdasarkan kebutuhan mereka di sektor kesehatan  data dapat digunakan untuk mencegah dan memantau masalah kesehatan  sementara di industri perbankan dan keuangan  sains data dapat membantu dalam deteksi penipuan dan pengelolaan aset bank pemerintah juga dapat menggunakan sains data untuk menyusun kebijakan yang lebih baik berdasarkan data yang tersedia contoh aplikasi sains data dalam pemasaran dan iklan meliputi penggunaan analisis prediktif dan deskriptif untuk mengelompokkan pelanggan  serta analisis media sosial untuk memahami perilaku dan pola pikir pelanggan', 'offsets_in_document': [{'start': 380, 'end': 491}], 'offsets_in_context': [{'start': 380, 'end': 491}], 'document_ids': ['4f4d9c00a8dcebd7db5bfed7dbf17e19-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'Bagaimana aplikasi sains data dalam bidang perbankan?', 'filters': None, 'id': '21bc9681ffedc453b46e68f1b268d0c4', 'no_answer': False, 'answers': [' di industri perbankan dan keuangan  sains data dapat membantu dalam deteksi penipuan dan pengelolaan aset bank'], 'offsets_in_documents': [{'start': 380, 'end': 491}], 'offsets_in_contexts': [{'start': 380, 'end': 491}], 'document_ids': ['4f4d9c00a8dcebd7db5bfed7dbf17e19-0'], 'contexts': ['sains data memiliki berbagai aplikasi yang relevan dalam berbagai industri dalam bidang pemasaran  sains data dapat digunakan untuk mengembangkan strategi harga berdasarkan analisis data  meningkatkan keuntungan perusahaan  dan mengelompokkan pelanggan berdasarkan kebutuhan mereka di sektor kesehatan  data dapat digunakan untuk mencegah dan memantau masalah kesehatan  sementara di industri perbankan dan keuangan  sains data dapat membantu dalam deteksi penipuan dan pengelolaan aset bank pemerintah juga dapat menggunakan sains data untuk menyusun kebijakan yang lebih baik berdasarkan data yang tersedia contoh aplikasi sains data dalam pemasaran dan iklan meliputi penggunaan analisis prediktif dan deskriptif untuk mengelompokkan pelanggan  serta analisis media sosial untuk memahami perilaku dan pola pikir pelanggan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f610b4db-520d-4627-927f-383819c2bc23', 'query': 'apa saja industi yang menggunakan sains data?', 'document': {'id': '40a4a29f4cf9498a684b0772b2e64060-0', 'content': 'namun  penggunaan sains data juga memiliki beberapa kekurangan salah satunya adalah pelanggaran privasi pelanggan saat profil pelanggan digunakan informasi pribadi mereka  seperti transaksi dan pembelian  dapat terlihat oleh perusahaan yang menggunakan sains data selain itu  informasi yang diperoleh melalui sains data juga dapat digunakan dengan cara yang merugikan  baik terhadap kelompok  individu  negara  maupun komunitas tertentu secara keseluruhan  sains data telah memberikan dampak besar pada berbagai industri utama  seperti perawatan kesehatan  e commerce  keuangan  perbankan  dan transportasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1017, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'sains data telah memberikan dampak besar pada berbagai industri utama  seperti perawatan kesehatan  e commerce  keuangan  perbankan  dan transportasi', 'type': 'extractive', 'score': 0.0, 'context': 'namun  penggunaan sains data juga memiliki beberapa kekurangan salah satunya adalah pelanggaran privasi pelanggan saat profil pelanggan digunakan informasi pribadi mereka  seperti transaksi dan pembelian  dapat terlihat oleh perusahaan yang menggunakan sains data selain itu  informasi yang diperoleh melalui sains data juga dapat digunakan dengan cara yang merugikan  baik terhadap kelompok  individu  negara  maupun komunitas tertentu secara keseluruhan  sains data telah memberikan dampak besar pada berbagai industri utama  seperti perawatan kesehatan  e commerce  keuangan  perbankan  dan transportasi', 'offsets_in_document': [{'start': 457, 'end': 606}], 'offsets_in_context': [{'start': 457, 'end': 606}], 'document_ids': ['40a4a29f4cf9498a684b0772b2e64060-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa saja industi yang menggunakan sains data?', 'filters': None, 'id': 'c95dca61294c5f6f9ff36143a66b40bf', 'no_answer': False, 'answers': ['sains data telah memberikan dampak besar pada berbagai industri utama  seperti perawatan kesehatan  e commerce  keuangan  perbankan  dan transportasi'], 'offsets_in_documents': [{'start': 457, 'end': 606}], 'offsets_in_contexts': [{'start': 457, 'end': 606}], 'document_ids': ['40a4a29f4cf9498a684b0772b2e64060-0'], 'contexts': ['namun  penggunaan sains data juga memiliki beberapa kekurangan salah satunya adalah pelanggaran privasi pelanggan saat profil pelanggan digunakan informasi pribadi mereka  seperti transaksi dan pembelian  dapat terlihat oleh perusahaan yang menggunakan sains data selain itu  informasi yang diperoleh melalui sains data juga dapat digunakan dengan cara yang merugikan  baik terhadap kelompok  individu  negara  maupun komunitas tertentu secara keseluruhan  sains data telah memberikan dampak besar pada berbagai industri utama  seperti perawatan kesehatan  e commerce  keuangan  perbankan  dan transportasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '393e125c-4b7d-4e4f-ad86-c095022b107c', 'query': 'bagaimana algoritma decision tree bekerja?', 'document': {'id': '3b804ca1122eae40d2521523646866c-0', 'content': 'terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi algoritma decision tree bekerja dengan memilih fitur yang paling informatif dan membagi dataset berdasarkan nilai fitur tersebut setiap pemisahan menghasilkan cabang baru dalam pohon keputusan  yang mewakili kondisi yang harus dipenuhi proses ini dilakukan secara berulang hingga mencapai kondisi berhenti  seperti mencapai tingkat kedalaman maksimum atau tidak ada lagi pemisahan yang memberikan peningkatan signifikan dalam pemisahan kelas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1032, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'algoritma decision tree bekerja dengan memilih fitur yang paling informatif dan membagi dataset berdasarkan nilai fitur tersebut', 'type': 'extractive', 'score': 0.0, 'context': 'terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi algoritma decision tree bekerja dengan memilih fitur yang paling informatif dan membagi dataset berdasarkan nilai fitur tersebut setiap pemisahan menghasilkan cabang baru dalam pohon keputusan  yang mewakili kondisi yang harus dipenuhi proses ini dilakukan secara berulang hingga mencapai kondisi berhenti  seperti mencapai tingkat kedalaman maksimum atau tidak ada lagi pemisahan yang memberikan peningkatan signifikan dalam pemisahan kelas', 'offsets_in_document': [{'start': 145, 'end': 273}], 'offsets_in_context': [{'start': 145, 'end': 273}], 'document_ids': ['3b804ca1122eae40d2521523646866c-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'bagaimana algoritma decision tree bekerja?', 'filters': None, 'id': 'de7049f8349cdb5fc4d73014e09cb92f', 'no_answer': False, 'answers': ['algoritma decision tree bekerja dengan memilih fitur yang paling informatif dan membagi dataset berdasarkan nilai fitur tersebut'], 'offsets_in_documents': [{'start': 145, 'end': 273}], 'offsets_in_contexts': [{'start': 145, 'end': 273}], 'document_ids': ['3b804ca1122eae40d2521523646866c-0'], 'contexts': ['terdapat 4 jenis depth node yaitu id3  c4.5  cart  dii pada depth node attribute selection method atau splitting rules yang digunakan bervariasi algoritma decision tree bekerja dengan memilih fitur yang paling informatif dan membagi dataset berdasarkan nilai fitur tersebut setiap pemisahan menghasilkan cabang baru dalam pohon keputusan  yang mewakili kondisi yang harus dipenuhi proses ini dilakukan secara berulang hingga mencapai kondisi berhenti  seperti mencapai tingkat kedalaman maksimum atau tidak ada lagi pemisahan yang memberikan peningkatan signifikan dalam pemisahan kelas']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c6fc9bd6-0061-472b-b23f-d40b9af6dc01', 'query': 'apa saja yang diperlukan pada proses training ', 'document': {'id': 'cded9cad675cf5dc21ec011791bdac3d-0', 'content': 'menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi learning atau training bertujuan untuk melatih jst proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1037, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'proses training memerlukan data latih yang terdiri atas pasangan input dan output', 'type': 'extractive', 'score': 0.0, 'context': 'menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi learning atau training bertujuan untuk melatih jst proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat', 'offsets_in_document': [{'start': 316, 'end': 397}], 'offsets_in_context': [{'start': 316, 'end': 397}], 'document_ids': ['cded9cad675cf5dc21ec011791bdac3d-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa saja yang diperlukan pada proses training ', 'filters': None, 'id': 'd16b68d710160a3c8190f60bbb73c047', 'no_answer': False, 'answers': ['proses training memerlukan data latih yang terdiri atas pasangan input dan output'], 'offsets_in_documents': [{'start': 316, 'end': 397}], 'offsets_in_contexts': [{'start': 316, 'end': 397}], 'document_ids': ['cded9cad675cf5dc21ec011791bdac3d-0'], 'contexts': ['menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi learning atau training bertujuan untuk melatih jst proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat']}>,\n",
              " <MultiLabel: {'labels': [{'id': '5a484e81-bb88-4403-a47f-b5d6a6551b8f', 'query': 'apa yang dapat dilakukan untuk melatih jaringan syaraf tiruan ', 'document': {'id': 'cded9cad675cf5dc21ec011791bdac3d-0', 'content': 'menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi learning atau training bertujuan untuk melatih jst proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1037, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' learning atau training ', 'type': 'extractive', 'score': 0.0, 'context': 'menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi learning atau training bertujuan untuk melatih jst proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat', 'offsets_in_document': [{'start': 264, 'end': 288}], 'offsets_in_context': [{'start': 264, 'end': 288}], 'document_ids': ['cded9cad675cf5dc21ec011791bdac3d-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa yang dapat dilakukan untuk melatih jaringan syaraf tiruan ', 'filters': None, 'id': 'c09ed042c3119f6e90b9e32058d0e09b', 'no_answer': False, 'answers': [' learning atau training '], 'offsets_in_documents': [{'start': 264, 'end': 288}], 'offsets_in_contexts': [{'start': 264, 'end': 288}], 'document_ids': ['cded9cad675cf5dc21ec011791bdac3d-0'], 'contexts': ['menurut kertas mereka  itu berkinerja lebih baik daripada relu dengan tingkat yang sama efisiensi komputasi dalam percobaan di imagenet dengan model identik yang menjalankan relu dan swish  yang baru fungsi mencapai akurasi klasifikasi top  1 0 6 0 9  lebih tinggi learning atau training bertujuan untuk melatih jst proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat']}>,\n",
              " <MultiLabel: {'labels': [{'id': '53ee45e5-b918-4ba4-946e-75a22b785f1d', 'query': 'apa saja yang termasuk performance matrics?', 'document': {'id': '69068109e51f1d3e2c678865632736d2-0', 'content': 'teknik ini tidak menjamin distribusi sample data yang sama untuk masing masing kelas selanjutnya ada confusion matrix  matriks yang digunakan untuk merangkum hasil klasifikasi yang benar atau salah yang dihasilkan oleh sebuah metode model klasifikasi pada suatu dataset terdiri dari baris dan kolom yang berkaitan dengan informasi kelas sebenarnya  true classes gold standard actual class  dan kelas yang diprediksi  predicted classes confusion matrix bukanlah metrik performa  tetapi digunakan sebagai referensi untuk menghitung metrik performa terdapat beberapa contoh dari performance matrics  accuracy  error rate  recall  sensitivity tpr   precision  f measure  fpr  false positive rate  false alarm  fnr false negative rate   specificity', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1027, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'contoh dari performance matrics  accuracy  error rate  recall  sensitivity tpr   precision  f measure  fpr  false positive rate  false alarm  fnr false negative rate   specificity', 'type': 'extractive', 'score': 0.0, 'context': 'teknik ini tidak menjamin distribusi sample data yang sama untuk masing masing kelas selanjutnya ada confusion matrix  matriks yang digunakan untuk merangkum hasil klasifikasi yang benar atau salah yang dihasilkan oleh sebuah metode model klasifikasi pada suatu dataset terdiri dari baris dan kolom yang berkaitan dengan informasi kelas sebenarnya  true classes gold standard actual class  dan kelas yang diprediksi  predicted classes confusion matrix bukanlah metrik performa  tetapi digunakan sebagai referensi untuk menghitung metrik performa terdapat beberapa contoh dari performance matrics  accuracy  error rate  recall  sensitivity tpr   precision  f measure  fpr  false positive rate  false alarm  fnr false negative rate   specificity', 'offsets_in_document': [{'start': 564, 'end': 743}], 'offsets_in_context': [{'start': 564, 'end': 743}], 'document_ids': ['69068109e51f1d3e2c678865632736d2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'apa saja yang termasuk performance matrics?', 'filters': None, 'id': 'd0540583a34c14a2423013c6f6642d0e', 'no_answer': False, 'answers': ['contoh dari performance matrics  accuracy  error rate  recall  sensitivity tpr   precision  f measure  fpr  false positive rate  false alarm  fnr false negative rate   specificity'], 'offsets_in_documents': [{'start': 564, 'end': 743}], 'offsets_in_contexts': [{'start': 564, 'end': 743}], 'document_ids': ['69068109e51f1d3e2c678865632736d2-0'], 'contexts': ['teknik ini tidak menjamin distribusi sample data yang sama untuk masing masing kelas selanjutnya ada confusion matrix  matriks yang digunakan untuk merangkum hasil klasifikasi yang benar atau salah yang dihasilkan oleh sebuah metode model klasifikasi pada suatu dataset terdiri dari baris dan kolom yang berkaitan dengan informasi kelas sebenarnya  true classes gold standard actual class  dan kelas yang diprediksi  predicted classes confusion matrix bukanlah metrik performa  tetapi digunakan sebagai referensi untuk menghitung metrik performa terdapat beberapa contoh dari performance matrics  accuracy  error rate  recall  sensitivity tpr   precision  f measure  fpr  false positive rate  false alarm  fnr false negative rate   specificity']}>,\n",
              " <MultiLabel: {'labels': [{'id': '71f96f26-e8ec-40fb-a944-152d22294a36', 'query': 'dimanakan produk diuji ketika test phase ', 'document': {'id': '9ac5510d20728ef1003debe63715f2de-0', 'content': 'setelah produk diuji dan siap untuk digunakan  produk ini dirilis secara resmi di pasar yang sesuai terkadang penyebaran produk terjadi secara bertahap sesuai strategi bisnis organisasi itu produk ini pertama kali dapat dirilis dalam segmen terbatas dan diuji di lingkungan bisnis nyata  uat  user acceptance testing kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1006, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'di lingkungan bisnis nyata ', 'type': 'extractive', 'score': 0.0, 'context': 'setelah produk diuji dan siap untuk digunakan  produk ini dirilis secara resmi di pasar yang sesuai terkadang penyebaran produk terjadi secara bertahap sesuai strategi bisnis organisasi itu produk ini pertama kali dapat dirilis dalam segmen terbatas dan diuji di lingkungan bisnis nyata  uat  user acceptance testing kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada', 'offsets_in_document': [{'start': 260, 'end': 287}], 'offsets_in_context': [{'start': 260, 'end': 287}], 'document_ids': ['9ac5510d20728ef1003debe63715f2de-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:38', 'updated_at': '2024-05-10 08:45:38', 'meta': {}, 'filters': None}], 'query': 'dimanakan produk diuji ketika test phase ', 'filters': None, 'id': '9eead0b5d970bc18345549d1b380b9a2', 'no_answer': False, 'answers': ['di lingkungan bisnis nyata '], 'offsets_in_documents': [{'start': 260, 'end': 287}], 'offsets_in_contexts': [{'start': 260, 'end': 287}], 'document_ids': ['9ac5510d20728ef1003debe63715f2de-0'], 'contexts': ['setelah produk diuji dan siap untuk digunakan  produk ini dirilis secara resmi di pasar yang sesuai terkadang penyebaran produk terjadi secara bertahap sesuai strategi bisnis organisasi itu produk ini pertama kali dapat dirilis dalam segmen terbatas dan diuji di lingkungan bisnis nyata  uat  user acceptance testing kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'ce7b5aa0-f0b7-4087-a6b7-bfa315f54206', 'query': 'Bagaimana informasi dapat diimplementasikan?', 'document': {'id': 'dcc0d4b706b0cf60c9b2058efdf43b38-0', 'content': 'selanjutnya  informasi tersebut dapat diimplementasikan dalam bentuk produk atau digunakan dalam proses pengambilan keputusan dengan menggunakan pendekatan ini  data dapat memberikan wawasan yang berharga dan mendukung pengambilan keputusan yang lebih baik setelah data dievaluasi dan ditafsirkan dengan demikian  teradapat lima tahapan penggunaan data untuk menghasilkan informasi berharga bagi pengambilan keputusan  yaitu analisis eksplorasi data  pembangunan model pengetahuan  pengembangan model prediktif  evaluasi data  dan interpretasi data seorang saintis data memiliki keahlian dalam beberapa latar belakang yang penting mereka harus memahami konsep pembelajaran mesin untuk menghasilkan model prediktif yang akurat', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1003, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'informasi tersebut dapat diimplementasikan dalam bentuk produk atau digunakan dalam proses pengambilan keputusan ', 'type': 'extractive', 'score': 0.0, 'context': 'selanjutnya  informasi tersebut dapat diimplementasikan dalam bentuk produk atau digunakan dalam proses pengambilan keputusan dengan menggunakan pendekatan ini  data dapat memberikan wawasan yang berharga dan mendukung pengambilan keputusan yang lebih baik setelah data dievaluasi dan ditafsirkan dengan demikian  teradapat lima tahapan penggunaan data untuk menghasilkan informasi berharga bagi pengambilan keputusan  yaitu analisis eksplorasi data  pembangunan model pengetahuan  pengembangan model prediktif  evaluasi data  dan interpretasi data seorang saintis data memiliki keahlian dalam beberapa latar belakang yang penting mereka harus memahami konsep pembelajaran mesin untuk menghasilkan model prediktif yang akurat', 'offsets_in_document': [{'start': 13, 'end': 126}], 'offsets_in_context': [{'start': 13, 'end': 126}], 'document_ids': ['dcc0d4b706b0cf60c9b2058efdf43b38-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'Bagaimana informasi dapat diimplementasikan?', 'filters': None, 'id': '91db4459c48fd7381b97045cd648a40e', 'no_answer': False, 'answers': ['informasi tersebut dapat diimplementasikan dalam bentuk produk atau digunakan dalam proses pengambilan keputusan '], 'offsets_in_documents': [{'start': 13, 'end': 126}], 'offsets_in_contexts': [{'start': 13, 'end': 126}], 'document_ids': ['dcc0d4b706b0cf60c9b2058efdf43b38-0'], 'contexts': ['selanjutnya  informasi tersebut dapat diimplementasikan dalam bentuk produk atau digunakan dalam proses pengambilan keputusan dengan menggunakan pendekatan ini  data dapat memberikan wawasan yang berharga dan mendukung pengambilan keputusan yang lebih baik setelah data dievaluasi dan ditafsirkan dengan demikian  teradapat lima tahapan penggunaan data untuk menghasilkan informasi berharga bagi pengambilan keputusan  yaitu analisis eksplorasi data  pembangunan model pengetahuan  pengembangan model prediktif  evaluasi data  dan interpretasi data seorang saintis data memiliki keahlian dalam beberapa latar belakang yang penting mereka harus memahami konsep pembelajaran mesin untuk menghasilkan model prediktif yang akurat']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b1323d7c-d4d5-478c-8300-ee8a17be4365', 'query': 'kapan intuisi diperlukan?', 'document': {'id': 'f6cf229bf4cc0915160855e4423f7e0a-0', 'content': 'intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan keingintahuan juga menjadi kunci dalam sains data  karena bidang ini terus berkembang dengan metode dan teknologi baru yang terus muncul sebagai ilmuwan data  memiliki rasa ingin tahu yang kuat untuk mempelajari dan mengikuti perkembangan teknologi menjadi sangat penting ada berbagai alat yang tersedia bagi ilmuwan data dalam melaksanakan tugas tugas mereka untuk analisis data  alat yang umum digunakan meliputi percikan  python  dan sas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1008, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan ', 'type': 'extractive', 'score': 0.0, 'context': 'intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan keingintahuan juga menjadi kunci dalam sains data  karena bidang ini terus berkembang dengan metode dan teknologi baru yang terus muncul sebagai ilmuwan data  memiliki rasa ingin tahu yang kuat untuk mempelajari dan mengikuti perkembangan teknologi menjadi sangat penting ada berbagai alat yang tersedia bagi ilmuwan data dalam melaksanakan tugas tugas mereka untuk analisis data  alat yang umum digunakan meliputi percikan  python  dan sas', 'offsets_in_document': [{'start': 0, 'end': 135}], 'offsets_in_context': [{'start': 0, 'end': 135}], 'document_ids': ['f6cf229bf4cc0915160855e4423f7e0a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kapan intuisi diperlukan?', 'filters': None, 'id': '5b80c2b640c447de7b57fc316367afc3', 'no_answer': False, 'answers': ['intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan '], 'offsets_in_documents': [{'start': 0, 'end': 135}], 'offsets_in_contexts': [{'start': 0, 'end': 135}], 'document_ids': ['f6cf229bf4cc0915160855e4423f7e0a-0'], 'contexts': ['intuisi diperlukan dalam memilih model yang tepat dengan akurasi yang sesuai  serta merasakan kapan model siap untuk diimplementasikan keingintahuan juga menjadi kunci dalam sains data  karena bidang ini terus berkembang dengan metode dan teknologi baru yang terus muncul sebagai ilmuwan data  memiliki rasa ingin tahu yang kuat untuk mempelajari dan mengikuti perkembangan teknologi menjadi sangat penting ada berbagai alat yang tersedia bagi ilmuwan data dalam melaksanakan tugas tugas mereka untuk analisis data  alat yang umum digunakan meliputi percikan  python  dan sas']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'de46aa73-83ff-4b67-b1ec-2523110a734f', 'query': 'k folds cross validation  stratified k folds cross validation  dan eave one out cross validation merupakan ', 'document': {'id': '23d91ec3888f5dfc08ef49d75984b63d-0', 'content': 'karnel non linear digunakan ketika data hanya dapat dipisahkan dengan garis lengkung atau sebuah bidang pada ruang dimensi tinggi selanjutnya terdapat model assement merupakan proses mengestimasi dan mengevaluasi kinerja berbagai model untuk memilih yang terbaik dan model selection merupakan proses memilih model terbaik setelah melalui evaluasi performa dan mempertimbangkan kesalahan prediksi pada data baru terdapat 3 teknik validasi untuk menguji kinerja model  yaitu 1  k folds cross validation  ini adalah metode validasi yang membagi data latih menjadi k subset atau  fold dalam setiap iterasi  salah satu subset digunakan sebagai data validasi  sementara k 1 subset lainnya digunakan sebagai data pelatihan proses ini diulang k kali  sehingga setiap subset digunakan sebagai data validasi tepat satu kali', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1030, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': '3 teknik validasi untuk menguji kinerja model', 'type': 'extractive', 'score': 0.0, 'context': 'karnel non linear digunakan ketika data hanya dapat dipisahkan dengan garis lengkung atau sebuah bidang pada ruang dimensi tinggi selanjutnya terdapat model assement merupakan proses mengestimasi dan mengevaluasi kinerja berbagai model untuk memilih yang terbaik dan model selection merupakan proses memilih model terbaik setelah melalui evaluasi performa dan mempertimbangkan kesalahan prediksi pada data baru terdapat 3 teknik validasi untuk menguji kinerja model  yaitu 1  k folds cross validation  ini adalah metode validasi yang membagi data latih menjadi k subset atau  fold dalam setiap iterasi  salah satu subset digunakan sebagai data validasi  sementara k 1 subset lainnya digunakan sebagai data pelatihan proses ini diulang k kali  sehingga setiap subset digunakan sebagai data validasi tepat satu kali', 'offsets_in_document': [{'start': 420, 'end': 465}], 'offsets_in_context': [{'start': 420, 'end': 465}], 'document_ids': ['23d91ec3888f5dfc08ef49d75984b63d-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'k folds cross validation  stratified k folds cross validation  dan eave one out cross validation merupakan ', 'filters': None, 'id': 'a4c2faea431ac7621637dc9c15813b46', 'no_answer': False, 'answers': ['3 teknik validasi untuk menguji kinerja model'], 'offsets_in_documents': [{'start': 420, 'end': 465}], 'offsets_in_contexts': [{'start': 420, 'end': 465}], 'document_ids': ['23d91ec3888f5dfc08ef49d75984b63d-0'], 'contexts': ['karnel non linear digunakan ketika data hanya dapat dipisahkan dengan garis lengkung atau sebuah bidang pada ruang dimensi tinggi selanjutnya terdapat model assement merupakan proses mengestimasi dan mengevaluasi kinerja berbagai model untuk memilih yang terbaik dan model selection merupakan proses memilih model terbaik setelah melalui evaluasi performa dan mempertimbangkan kesalahan prediksi pada data baru terdapat 3 teknik validasi untuk menguji kinerja model  yaitu 1  k folds cross validation  ini adalah metode validasi yang membagi data latih menjadi k subset atau  fold dalam setiap iterasi  salah satu subset digunakan sebagai data validasi  sementara k 1 subset lainnya digunakan sebagai data pelatihan proses ini diulang k kali  sehingga setiap subset digunakan sebagai data validasi tepat satu kali']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c3c15521-5d30-4b88-b447-b6295de85ff4', 'query': 'apa itu fungsi aktivasi ', 'document': {'id': '5a4296c4c02d198d014d962eaee6a42b-0', 'content': 'jaringan recurrent adalah jaringan yang terdapat neuron output yang memberi sinyal pada unit input sering disebut feedback loop fungsi aktivasi adalah  gerbang  matematis di antara input yang memberi makan neuron saat ini dan outputnya menuju lapisan berikutnya pada perkembangannya  nn menggunakan fungsi aktivasi non linear yang dapat digunakan untuk membantu jaringan mempelajari data kompleks  dan memberikan prediksi yang akurat tipe fungsi aktivasi terdiri dari binary step function  linear activation function  dan non linear activation function fungsi langkah biner adalah aktivasi berbasis ambang batas fungsi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1036, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'fungsi aktivasi adalah  gerbang  matematis di antara input yang memberi makan neuron saat ini dan outputnya menuju lapisan berikutnya ', 'type': 'extractive', 'score': 0.0, 'context': 'jaringan recurrent adalah jaringan yang terdapat neuron output yang memberi sinyal pada unit input sering disebut feedback loop fungsi aktivasi adalah  gerbang  matematis di antara input yang memberi makan neuron saat ini dan outputnya menuju lapisan berikutnya pada perkembangannya  nn menggunakan fungsi aktivasi non linear yang dapat digunakan untuk membantu jaringan mempelajari data kompleks  dan memberikan prediksi yang akurat tipe fungsi aktivasi terdiri dari binary step function  linear activation function  dan non linear activation function fungsi langkah biner adalah aktivasi berbasis ambang batas fungsi', 'offsets_in_document': [{'start': 128, 'end': 262}], 'offsets_in_context': [{'start': 128, 'end': 262}], 'document_ids': ['5a4296c4c02d198d014d962eaee6a42b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu fungsi aktivasi ', 'filters': None, 'id': 'ff35f3ed3211133e6b75e45922d78741', 'no_answer': False, 'answers': ['fungsi aktivasi adalah  gerbang  matematis di antara input yang memberi makan neuron saat ini dan outputnya menuju lapisan berikutnya '], 'offsets_in_documents': [{'start': 128, 'end': 262}], 'offsets_in_contexts': [{'start': 128, 'end': 262}], 'document_ids': ['5a4296c4c02d198d014d962eaee6a42b-0'], 'contexts': ['jaringan recurrent adalah jaringan yang terdapat neuron output yang memberi sinyal pada unit input sering disebut feedback loop fungsi aktivasi adalah  gerbang  matematis di antara input yang memberi makan neuron saat ini dan outputnya menuju lapisan berikutnya pada perkembangannya  nn menggunakan fungsi aktivasi non linear yang dapat digunakan untuk membantu jaringan mempelajari data kompleks  dan memberikan prediksi yang akurat tipe fungsi aktivasi terdiri dari binary step function  linear activation function  dan non linear activation function fungsi langkah biner adalah aktivasi berbasis ambang batas fungsi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'fb460e9e-3e96-42f2-8a99-b38dbdb8621e', 'query': 'apa yang dimaksud artificial intelligence?', 'document': {'id': '8f7767b4efd44235e094c4c981548bf6-0', 'content': ' artificial intelligence ai  adalah sebuah teknik yang memungkinkan komputer untuk meniru manusia  yang membedakan manusia dan komputer adalah cara mereka dalam memahami suatu permasalahan manusia memahami suatu permasalahan melalui pengalaman yang dimiliki  sementara komputer mengandalkan data yang ada di dalam sistemnya machine learning merupakan subbidang dari kecerdasan buatan  machine learning dapat dianggap sebagai proses yang memungkinkan komputer atau mesin untuk meningkatkan kemampuannya berdasarkan pengalama yang bersumber dari data yang ada  sehingga akhirnya memiliki kecerdasan yang mirip dengan manusia jenis metode pembelajaran dibagi menjadi tiga yaitu supervised learning pembelajaran terbimbing   unsupervised learning pembelajaran tidak terbimbing   reinforcement learning kali ini kita akan membahas lebih detail tentang metode pembelajaran supervised learning', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1019, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' artificial intelligence ai  adalah sebuah teknik yang memungkinkan komputer untuk meniru manusia', 'type': 'extractive', 'score': 0.0, 'context': ' artificial intelligence ai  adalah sebuah teknik yang memungkinkan komputer untuk meniru manusia  yang membedakan manusia dan komputer adalah cara mereka dalam memahami suatu permasalahan manusia memahami suatu permasalahan melalui pengalaman yang dimiliki  sementara komputer mengandalkan data yang ada di dalam sistemnya machine learning merupakan subbidang dari kecerdasan buatan  machine learning dapat dianggap sebagai proses yang memungkinkan komputer atau mesin untuk meningkatkan kemampuannya berdasarkan pengalama yang bersumber dari data yang ada  sehingga akhirnya memiliki kecerdasan yang mirip dengan manusia jenis metode pembelajaran dibagi menjadi tiga yaitu supervised learning pembelajaran terbimbing   unsupervised learning pembelajaran tidak terbimbing   reinforcement learning kali ini kita akan membahas lebih detail tentang metode pembelajaran supervised learning', 'offsets_in_document': [{'start': 0, 'end': 97}], 'offsets_in_context': [{'start': 0, 'end': 97}], 'document_ids': ['8f7767b4efd44235e094c4c981548bf6-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud artificial intelligence?', 'filters': None, 'id': '585d95707431e6c0d22f5ed507fc398a', 'no_answer': False, 'answers': [' artificial intelligence ai  adalah sebuah teknik yang memungkinkan komputer untuk meniru manusia'], 'offsets_in_documents': [{'start': 0, 'end': 97}], 'offsets_in_contexts': [{'start': 0, 'end': 97}], 'document_ids': ['8f7767b4efd44235e094c4c981548bf6-0'], 'contexts': [' artificial intelligence ai  adalah sebuah teknik yang memungkinkan komputer untuk meniru manusia  yang membedakan manusia dan komputer adalah cara mereka dalam memahami suatu permasalahan manusia memahami suatu permasalahan melalui pengalaman yang dimiliki  sementara komputer mengandalkan data yang ada di dalam sistemnya machine learning merupakan subbidang dari kecerdasan buatan  machine learning dapat dianggap sebagai proses yang memungkinkan komputer atau mesin untuk meningkatkan kemampuannya berdasarkan pengalama yang bersumber dari data yang ada  sehingga akhirnya memiliki kecerdasan yang mirip dengan manusia jenis metode pembelajaran dibagi menjadi tiga yaitu supervised learning pembelajaran terbimbing   unsupervised learning pembelajaran tidak terbimbing   reinforcement learning kali ini kita akan membahas lebih detail tentang metode pembelajaran supervised learning']}>,\n",
              " <MultiLabel: {'labels': [{'id': '1bac74c1-167f-4aae-baa8-ebbb884caaba', 'query': 'bagaimana cara untuk analisis tren data?', 'document': {'id': '843b2242e0f419313289e78d3676e3a2-0', 'content': 'selain itu  informasi yang diperoleh melalui sains data juga dapat digunakan dengan cara yang merugikan  baik terhadap kelompok  individu  negara  maupun komunitas tertentu secara keseluruhan  sains data telah memberikan dampak besar pada berbagai industri utama  seperti perawatan kesehatan  e commerce  keuangan  perbankan  dan transportasi dalam upaya meningkatkan produk dan basis pelanggan  sains data telah menjadi bagian integral dari semua industri selain itu  aplikasi utama sains data juga meliputi deteksi penipuan di sektor keuangan dan perbankan  serta analisis tren data menggunakan alat business intelligence oleh karena itu  sains data menjadi kunci untuk pertumbuhan dan kemajuan industri dalam domain mereka masing masing', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1014, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'analisis tren data menggunakan alat business intelligence ', 'type': 'extractive', 'score': 0.0, 'context': 'selain itu  informasi yang diperoleh melalui sains data juga dapat digunakan dengan cara yang merugikan  baik terhadap kelompok  individu  negara  maupun komunitas tertentu secara keseluruhan  sains data telah memberikan dampak besar pada berbagai industri utama  seperti perawatan kesehatan  e commerce  keuangan  perbankan  dan transportasi dalam upaya meningkatkan produk dan basis pelanggan  sains data telah menjadi bagian integral dari semua industri selain itu  aplikasi utama sains data juga meliputi deteksi penipuan di sektor keuangan dan perbankan  serta analisis tren data menggunakan alat business intelligence oleh karena itu  sains data menjadi kunci untuk pertumbuhan dan kemajuan industri dalam domain mereka masing masing', 'offsets_in_document': [{'start': 566, 'end': 624}], 'offsets_in_context': [{'start': 566, 'end': 624}], 'document_ids': ['843b2242e0f419313289e78d3676e3a2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara untuk analisis tren data?', 'filters': None, 'id': '231de2e16eb8376faca3b02a6ccc30f3', 'no_answer': False, 'answers': ['analisis tren data menggunakan alat business intelligence '], 'offsets_in_documents': [{'start': 566, 'end': 624}], 'offsets_in_contexts': [{'start': 566, 'end': 624}], 'document_ids': ['843b2242e0f419313289e78d3676e3a2-0'], 'contexts': ['selain itu  informasi yang diperoleh melalui sains data juga dapat digunakan dengan cara yang merugikan  baik terhadap kelompok  individu  negara  maupun komunitas tertentu secara keseluruhan  sains data telah memberikan dampak besar pada berbagai industri utama  seperti perawatan kesehatan  e commerce  keuangan  perbankan  dan transportasi dalam upaya meningkatkan produk dan basis pelanggan  sains data telah menjadi bagian integral dari semua industri selain itu  aplikasi utama sains data juga meliputi deteksi penipuan di sektor keuangan dan perbankan  serta analisis tren data menggunakan alat business intelligence oleh karena itu  sains data menjadi kunci untuk pertumbuhan dan kemajuan industri dalam domain mereka masing masing']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f9be5fa1-c106-40ac-b7f4-29fa748ec856', 'query': 'apa yang dimaksud sebagai clustering ', 'document': {'id': '1ca58b70fe3387ab73e38a15129c8af0-0', 'content': ' clustering adalah pengelompokan data points untuk menghasilkan cluster kelompok berdasarkan kesamaan data clustering masuk dalam kelas unsupervised learning karena proses pelatihan dilakukan tanpa menggunakan label cluster analysis mengelompokkan objek data hanya berdasarkan informasi yang ditemukan dalam data yang menggambarkan objek dan hubungannya tujuannya adalah agar objek objek dalam suatu kelompok serupa  terkait  satu sama lain  dan berbeda  atau tidak terkait  dengan objek objek dalam kelompok lain semakin besar kesamaan  atau homogenitas  dalam suatu kelompok dan semakin besar perbedaan antar kelompok  semakin baik atau lebih jelas pengelompokannya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1075, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' clustering adalah pengelompokan data points untuk menghasilkan cluster kelompok berdasarkan kesamaan data ', 'type': 'extractive', 'score': 0.0, 'context': ' clustering adalah pengelompokan data points untuk menghasilkan cluster kelompok berdasarkan kesamaan data clustering masuk dalam kelas unsupervised learning karena proses pelatihan dilakukan tanpa menggunakan label cluster analysis mengelompokkan objek data hanya berdasarkan informasi yang ditemukan dalam data yang menggambarkan objek dan hubungannya tujuannya adalah agar objek objek dalam suatu kelompok serupa  terkait  satu sama lain  dan berbeda  atau tidak terkait  dengan objek objek dalam kelompok lain semakin besar kesamaan  atau homogenitas  dalam suatu kelompok dan semakin besar perbedaan antar kelompok  semakin baik atau lebih jelas pengelompokannya', 'offsets_in_document': [{'start': 0, 'end': 107}], 'offsets_in_context': [{'start': 0, 'end': 107}], 'document_ids': ['1ca58b70fe3387ab73e38a15129c8af0-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud sebagai clustering ', 'filters': None, 'id': '03ec0233d5c64438cb9fcdec2ed373e1', 'no_answer': False, 'answers': [' clustering adalah pengelompokan data points untuk menghasilkan cluster kelompok berdasarkan kesamaan data '], 'offsets_in_documents': [{'start': 0, 'end': 107}], 'offsets_in_contexts': [{'start': 0, 'end': 107}], 'document_ids': ['1ca58b70fe3387ab73e38a15129c8af0-0'], 'contexts': [' clustering adalah pengelompokan data points untuk menghasilkan cluster kelompok berdasarkan kesamaan data clustering masuk dalam kelas unsupervised learning karena proses pelatihan dilakukan tanpa menggunakan label cluster analysis mengelompokkan objek data hanya berdasarkan informasi yang ditemukan dalam data yang menggambarkan objek dan hubungannya tujuannya adalah agar objek objek dalam suatu kelompok serupa  terkait  satu sama lain  dan berbeda  atau tidak terkait  dengan objek objek dalam kelompok lain semakin besar kesamaan  atau homogenitas  dalam suatu kelompok dan semakin besar perbedaan antar kelompok  semakin baik atau lebih jelas pengelompokannya']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e8ed2cdd-3db5-47ca-bf50-0b92ff8a5d5a', 'query': 'bagaimana cara kerja fungsi langkah biner ', 'document': {'id': '81cffc8363720f3a3cc039dd39a88d7-0', 'content': 'tipe fungsi aktivasi terdiri dari binary step function  linear activation function  dan non linear activation function fungsi langkah biner adalah aktivasi berbasis ambang batas fungsi jika nilai input di atas atau di bawah ambang batas tertentu  maka neuron diaktifkan dan mengirimkan sinyal yang persis sama ke lapisan berikutnya macam fungsi langkah biner  fungsi undak biner  fungsi undak biner fungsi bipolar  dan fungsi bipolar dengan threshold fungsi aktivasi linier berbentuk 𝐴   𝑐', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1039, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'jika nilai input di atas atau di bawah ambang batas tertentu  maka neuron diaktifkan dan mengirimkan sinyal yang persis sama ke lapisan berikutnya ', 'type': 'extractive', 'score': 0.0, 'context': 'tipe fungsi aktivasi terdiri dari binary step function  linear activation function  dan non linear activation function fungsi langkah biner adalah aktivasi berbasis ambang batas fungsi jika nilai input di atas atau di bawah ambang batas tertentu  maka neuron diaktifkan dan mengirimkan sinyal yang persis sama ke lapisan berikutnya macam fungsi langkah biner  fungsi undak biner  fungsi undak biner fungsi bipolar  dan fungsi bipolar dengan threshold fungsi aktivasi linier berbentuk 𝐴   𝑐', 'offsets_in_document': [{'start': 185, 'end': 332}], 'offsets_in_context': [{'start': 185, 'end': 332}], 'document_ids': ['81cffc8363720f3a3cc039dd39a88d7-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara kerja fungsi langkah biner ', 'filters': None, 'id': 'cc872c6a4ba5cf75c54a1cab67fea6b4', 'no_answer': False, 'answers': ['jika nilai input di atas atau di bawah ambang batas tertentu  maka neuron diaktifkan dan mengirimkan sinyal yang persis sama ke lapisan berikutnya '], 'offsets_in_documents': [{'start': 185, 'end': 332}], 'offsets_in_contexts': [{'start': 185, 'end': 332}], 'document_ids': ['81cffc8363720f3a3cc039dd39a88d7-0'], 'contexts': ['tipe fungsi aktivasi terdiri dari binary step function  linear activation function  dan non linear activation function fungsi langkah biner adalah aktivasi berbasis ambang batas fungsi jika nilai input di atas atau di bawah ambang batas tertentu  maka neuron diaktifkan dan mengirimkan sinyal yang persis sama ke lapisan berikutnya macam fungsi langkah biner  fungsi undak biner  fungsi undak biner fungsi bipolar  dan fungsi bipolar dengan threshold fungsi aktivasi linier berbentuk 𝐴   𝑐']}>,\n",
              " <MultiLabel: {'labels': [{'id': '91be9fef-d7f8-4e20-84ce-75604e06c7b6', 'query': 'apa yang diharapkan setelah proses pelatihan jst?', 'document': {'id': '23f5f67dc9bf5622a0d257adbe8011b6-0', 'content': 'proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat setelah proses pelatihan jst diharapkan mampu memetakan input data ke output data yang tepat menggunakan bobot bobot yang didapat dari proses pelatihan layer konvlusi adalah layer untuk ekstrasi fitur map secara spasialm dan sebuah cnn dapat memiliki beberapa layer konvlusi layer konvolusi pada bagian awal akan mempelajari representasi fitur atau pola spasial yang sederhana  sedangkan layer konvolusi yang lebih dalam akan mempelajari representasi fitur yang lebih kompleks', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1050, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'setelah proses pelatihan jst diharapkan mampu memetakan input data ke output data yang tepat menggunakan bobot bobot yang didapat dari proses pelatihan', 'type': 'extractive', 'score': 0.0, 'context': 'proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat setelah proses pelatihan jst diharapkan mampu memetakan input data ke output data yang tepat menggunakan bobot bobot yang didapat dari proses pelatihan layer konvlusi adalah layer untuk ekstrasi fitur map secara spasialm dan sebuah cnn dapat memiliki beberapa layer konvlusi layer konvolusi pada bagian awal akan mempelajari representasi fitur atau pola spasial yang sederhana  sedangkan layer konvolusi yang lebih dalam akan mempelajari representasi fitur yang lebih kompleks', 'offsets_in_document': [{'start': 224, 'end': 375}], 'offsets_in_context': [{'start': 224, 'end': 375}], 'document_ids': ['23f5f67dc9bf5622a0d257adbe8011b6-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang diharapkan setelah proses pelatihan jst?', 'filters': None, 'id': '1b349b3163e913f447406829d1bd65dd', 'no_answer': False, 'answers': ['setelah proses pelatihan jst diharapkan mampu memetakan input data ke output data yang tepat menggunakan bobot bobot yang didapat dari proses pelatihan'], 'offsets_in_documents': [{'start': 224, 'end': 375}], 'offsets_in_contexts': [{'start': 224, 'end': 375}], 'document_ids': ['23f5f67dc9bf5622a0d257adbe8011b6-0'], 'contexts': ['proses training memerlukan data latih yang terdiri atas pasangan input dan output tujuan pelatihan menentukan bobot bobot penghubung antar neuron dalam jst yang tepat  sehingga jst mampu memetakan input ke output yang tepat setelah proses pelatihan jst diharapkan mampu memetakan input data ke output data yang tepat menggunakan bobot bobot yang didapat dari proses pelatihan layer konvlusi adalah layer untuk ekstrasi fitur map secara spasialm dan sebuah cnn dapat memiliki beberapa layer konvlusi layer konvolusi pada bagian awal akan mempelajari representasi fitur atau pola spasial yang sederhana  sedangkan layer konvolusi yang lebih dalam akan mempelajari representasi fitur yang lebih kompleks']}>,\n",
              " <MultiLabel: {'labels': [{'id': '227855fb-54ab-4d38-b1bf-65008db4471e', 'query': 'algoritma optimasi mengestimasi parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dengan menggunakan ', 'document': {'id': 'da91d038b5b0bc4700047fd80b7b39bc-0', 'content': 'bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent least squares memerlukan waktu lama pada dataset besar  lebih dari 10000 baris   sementara algoritma optimasi cocok digunakan pada dataset besar metrik mse  mean squared error  digunakan pada estimasi parameter menggunakan optimasi gradient descent untuk menunjukkan nilai error kelebihan regresi linear adalah ringan  mudah dipahami  mudah diimplementasikan dan tidak memerlukan tuning parameter', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1070, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent', 'type': 'extractive', 'score': 0.0, 'context': 'bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent least squares memerlukan waktu lama pada dataset besar  lebih dari 10000 baris   sementara algoritma optimasi cocok digunakan pada dataset besar metrik mse  mean squared error  digunakan pada estimasi parameter menggunakan optimasi gradient descent untuk menunjukkan nilai error kelebihan regresi linear adalah ringan  mudah dipahami  mudah diimplementasikan dan tidak memerlukan tuning parameter', 'offsets_in_document': [{'start': 113, 'end': 247}], 'offsets_in_context': [{'start': 113, 'end': 247}], 'document_ids': ['da91d038b5b0bc4700047fd80b7b39bc-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'algoritma optimasi mengestimasi parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dengan menggunakan ', 'filters': None, 'id': '087c0d6519c116737b8be2423a9cea0d', 'no_answer': False, 'answers': ['dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent'], 'offsets_in_documents': [{'start': 113, 'end': 247}], 'offsets_in_contexts': [{'start': 113, 'end': 247}], 'document_ids': ['da91d038b5b0bc4700047fd80b7b39bc-0'], 'contexts': ['bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent least squares memerlukan waktu lama pada dataset besar  lebih dari 10000 baris   sementara algoritma optimasi cocok digunakan pada dataset besar metrik mse  mean squared error  digunakan pada estimasi parameter menggunakan optimasi gradient descent untuk menunjukkan nilai error kelebihan regresi linear adalah ringan  mudah dipahami  mudah diimplementasikan dan tidak memerlukan tuning parameter']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'daa6a433-d972-4572-a464-2fe49085334f', 'query': 'bagaimana algoritma dtr bekerja?', 'document': {'id': 'd2df7de761f2060abe09365e46fa6249-0', 'content': 'sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi decision tree regression  dtr  merupakan algoritma decision tree yang digunakan untuk tugas regresi  sehingga dapat digunakan untuk memprediksi keluaran bernilai kontinu algoritme dtr bekerja dengan membagi data menurut atribut yang berbeda di setiap node sambil mencoba mengurangi ukuran seleksi  gini index  yang kemudian dapat digunakan untuk memprediksi hasil variabel target struktur dari decision tree terdiri dari simpul  node  yang mewakili fitur  cabang  branch  yang mewakili keputusan  dan daun  leaf  mewakili hasil  nilai numerik untuk regresi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1055, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'algoritme dtr bekerja dengan membagi data menurut atribut yang berbeda di setiap node sambil mencoba mengurangi ukuran seleksi  gini index  yang kemudian dapat digunakan untuk memprediksi hasil variabel target', 'type': 'extractive', 'score': 0.0, 'context': 'sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi decision tree regression  dtr  merupakan algoritma decision tree yang digunakan untuk tugas regresi  sehingga dapat digunakan untuk memprediksi keluaran bernilai kontinu algoritme dtr bekerja dengan membagi data menurut atribut yang berbeda di setiap node sambil mencoba mengurangi ukuran seleksi  gini index  yang kemudian dapat digunakan untuk memprediksi hasil variabel target struktur dari decision tree terdiri dari simpul  node  yang mewakili fitur  cabang  branch  yang mewakili keputusan  dan daun  leaf  mewakili hasil  nilai numerik untuk regresi', 'offsets_in_document': [{'start': 400, 'end': 609}], 'offsets_in_context': [{'start': 400, 'end': 609}], 'document_ids': ['d2df7de761f2060abe09365e46fa6249-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana algoritma dtr bekerja?', 'filters': None, 'id': 'b3b4ea42f465fa1a015a7122cc6d6a2f', 'no_answer': False, 'answers': ['algoritme dtr bekerja dengan membagi data menurut atribut yang berbeda di setiap node sambil mencoba mengurangi ukuran seleksi  gini index  yang kemudian dapat digunakan untuk memprediksi hasil variabel target'], 'offsets_in_documents': [{'start': 400, 'end': 609}], 'offsets_in_contexts': [{'start': 400, 'end': 609}], 'document_ids': ['d2df7de761f2060abe09365e46fa6249-0'], 'contexts': ['sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi decision tree regression  dtr  merupakan algoritma decision tree yang digunakan untuk tugas regresi  sehingga dapat digunakan untuk memprediksi keluaran bernilai kontinu algoritme dtr bekerja dengan membagi data menurut atribut yang berbeda di setiap node sambil mencoba mengurangi ukuran seleksi  gini index  yang kemudian dapat digunakan untuk memprediksi hasil variabel target struktur dari decision tree terdiri dari simpul  node  yang mewakili fitur  cabang  branch  yang mewakili keputusan  dan daun  leaf  mewakili hasil  nilai numerik untuk regresi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2a55fe13-8d1c-4ea9-be7e-d5108495b41c', 'query': 'apa saja 7 fungsi aktivasi non linear ', 'document': {'id': '49c7e9a18003dd3df7df685c4229a376-0', 'content': '7 fungsi aktivasi nonlinear umum yaitu  sigmoid  tanh  relu  leacky relu  paramtric relu  softmax  swish salah satu keuntungan fungsi sigmoid yaitu prediksi yang jelas  untuk x di atas 2 atau di bawah  2  cenderung membawa nilai y  the prediksi  ke tepi kurva  sangat dekat dengan 1 atau 0. hal ini memungkinkan jelas prediksi tetapi kekurangannya yaitu vanishing gradient  untuk nilai x yang sangat tinggi atau sangat rendah  hampir tidak ada perubahan pada prediksi  menyebabkan masalah gradien menghilang hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1040, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': '7 fungsi aktivasi nonlinear umum yaitu  sigmoid  tanh  relu  leacky relu  paramtric relu  softmax  swish ', 'type': 'extractive', 'score': 0.0, 'context': '7 fungsi aktivasi nonlinear umum yaitu  sigmoid  tanh  relu  leacky relu  paramtric relu  softmax  swish salah satu keuntungan fungsi sigmoid yaitu prediksi yang jelas  untuk x di atas 2 atau di bawah  2  cenderung membawa nilai y  the prediksi  ke tepi kurva  sangat dekat dengan 1 atau 0. hal ini memungkinkan jelas prediksi tetapi kekurangannya yaitu vanishing gradient  untuk nilai x yang sangat tinggi atau sangat rendah  hampir tidak ada perubahan pada prediksi  menyebabkan masalah gradien menghilang hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai', 'offsets_in_document': [{'start': 0, 'end': 105}], 'offsets_in_context': [{'start': 0, 'end': 105}], 'document_ids': ['49c7e9a18003dd3df7df685c4229a376-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja 7 fungsi aktivasi non linear ', 'filters': None, 'id': '537d32fde9850c9ec3a5e977f0a2c477', 'no_answer': False, 'answers': ['7 fungsi aktivasi nonlinear umum yaitu  sigmoid  tanh  relu  leacky relu  paramtric relu  softmax  swish '], 'offsets_in_documents': [{'start': 0, 'end': 105}], 'offsets_in_contexts': [{'start': 0, 'end': 105}], 'document_ids': ['49c7e9a18003dd3df7df685c4229a376-0'], 'contexts': ['7 fungsi aktivasi nonlinear umum yaitu  sigmoid  tanh  relu  leacky relu  paramtric relu  softmax  swish salah satu keuntungan fungsi sigmoid yaitu prediksi yang jelas  untuk x di atas 2 atau di bawah  2  cenderung membawa nilai y  the prediksi  ke tepi kurva  sangat dekat dengan 1 atau 0. hal ini memungkinkan jelas prediksi tetapi kekurangannya yaitu vanishing gradient  untuk nilai x yang sangat tinggi atau sangat rendah  hampir tidak ada perubahan pada prediksi  menyebabkan masalah gradien menghilang hal ini dapat mengakibatkan jaringan menolak untuk belajar lebih jauh atau terlalu lambat untuk mencapai prediksi yang akurat fungsi tanh memiliki keuntungan berpusat pada nol mempermudah memodelkan input itu memiliki sangat negatif  netral  dan sangat positif nilai nilai']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b4035a87-7181-4e13-b29c-5eaf3278f8d3', 'query': 'apa keuntungunan fungsi sigmoid?', 'document': {'id': '7e0c96f9f5470a1b71937ab5ab4c2b90-0', 'content': 'mereka mengizinkan model untuk membuat pemetaan kompleks antara input dan output jaringan  yang penting untuk belajar dan memodelkan data yang kompleks  seperti gambar  kumpulan video  audio  dan data yang tidak linier atau berdimensi tinggi mereka mengizinkan backpropagation karena mereka punya fungsi turunan yang terkait dengan input dan juga memungkinkan  penumpukan  beberapa lapisan neuron untuk membuat jaringan saraf yang dalam beberapa tersembunyi lapisan neuron diperlukan untuk mempelajari data yang kompleks set dengan tingkat akurasi yang tinggi 7 fungsi aktivasi nonlinear umum yaitu  sigmoid  tanh  relu  leacky relu  paramtric relu  softmax  swish salah satu keuntungan fungsi sigmoid yaitu prediksi yang jelas  untuk x di atas 2 atau di bawah  2  cenderung membawa nilai y  the prediksi  ke tepi kurva  sangat dekat dengan 1 atau 0. hal ini memungkinkan jelas prediksi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1051, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' salah satu keuntungan fungsi sigmoid yaitu prediksi yang jelas  untuk x di atas 2 atau di bawah  2  cenderung membawa nilai y  the prediksi  ke tepi kurva  sangat dekat dengan 1 atau 0', 'type': 'extractive', 'score': 0.0, 'context': 'mereka mengizinkan model untuk membuat pemetaan kompleks antara input dan output jaringan  yang penting untuk belajar dan memodelkan data yang kompleks  seperti gambar  kumpulan video  audio  dan data yang tidak linier atau berdimensi tinggi mereka mengizinkan backpropagation karena mereka punya fungsi turunan yang terkait dengan input dan juga memungkinkan  penumpukan  beberapa lapisan neuron untuk membuat jaringan saraf yang dalam beberapa tersembunyi lapisan neuron diperlukan untuk mempelajari data yang kompleks set dengan tingkat akurasi yang tinggi 7 fungsi aktivasi nonlinear umum yaitu  sigmoid  tanh  relu  leacky relu  paramtric relu  softmax  swish salah satu keuntungan fungsi sigmoid yaitu prediksi yang jelas  untuk x di atas 2 atau di bawah  2  cenderung membawa nilai y  the prediksi  ke tepi kurva  sangat dekat dengan 1 atau 0. hal ini memungkinkan jelas prediksi', 'offsets_in_document': [{'start': 664, 'end': 849}], 'offsets_in_context': [{'start': 664, 'end': 849}], 'document_ids': ['7e0c96f9f5470a1b71937ab5ab4c2b90-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa keuntungunan fungsi sigmoid?', 'filters': None, 'id': 'e9960eee2af80400f00afb26bc5e9d1a', 'no_answer': False, 'answers': [' salah satu keuntungan fungsi sigmoid yaitu prediksi yang jelas  untuk x di atas 2 atau di bawah  2  cenderung membawa nilai y  the prediksi  ke tepi kurva  sangat dekat dengan 1 atau 0'], 'offsets_in_documents': [{'start': 664, 'end': 849}], 'offsets_in_contexts': [{'start': 664, 'end': 849}], 'document_ids': ['7e0c96f9f5470a1b71937ab5ab4c2b90-0'], 'contexts': ['mereka mengizinkan model untuk membuat pemetaan kompleks antara input dan output jaringan  yang penting untuk belajar dan memodelkan data yang kompleks  seperti gambar  kumpulan video  audio  dan data yang tidak linier atau berdimensi tinggi mereka mengizinkan backpropagation karena mereka punya fungsi turunan yang terkait dengan input dan juga memungkinkan  penumpukan  beberapa lapisan neuron untuk membuat jaringan saraf yang dalam beberapa tersembunyi lapisan neuron diperlukan untuk mempelajari data yang kompleks set dengan tingkat akurasi yang tinggi 7 fungsi aktivasi nonlinear umum yaitu  sigmoid  tanh  relu  leacky relu  paramtric relu  softmax  swish salah satu keuntungan fungsi sigmoid yaitu prediksi yang jelas  untuk x di atas 2 atau di bawah  2  cenderung membawa nilai y  the prediksi  ke tepi kurva  sangat dekat dengan 1 atau 0. hal ini memungkinkan jelas prediksi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd3b3d4fc-2284-4262-9f2a-efb1e5ffe69a', 'query': 'bagaimana evaluasi clustering dengan menggunakan clustering ', 'document': {'id': '4f0f576566298295e7e73d5faf516249-0', 'content': 'cluster yang dihasilkan dinilai valid atau tidak yaitu untuk melihat apakah benar benar ada struktur dalam data yang telah dikelompokkan hasil clustering dibandingkan  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda  untuk membandingkan dua atau lebih analisa clustering evaluasi pada clustering dapat dilakukan dengan berbagai cara yang dapat dikelompokkan sebagai evaluasi tanpa label dan evaluasi dengan label evaluasi tanpa label dilakukan dengan mengukur baik atau tidaknya hasil clustering tanpa menggunakan data luar atau label evaluasi ini disebut juga sebagai unsupervised measure evaluation', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1074, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' evaluasi pada clustering dapat dilakukan dengan berbagai cara yang dapat dikelompokkan sebagai evaluasi tanpa label dan evaluasi dengan label', 'type': 'extractive', 'score': 0.0, 'context': 'cluster yang dihasilkan dinilai valid atau tidak yaitu untuk melihat apakah benar benar ada struktur dalam data yang telah dikelompokkan hasil clustering dibandingkan  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda  untuk membandingkan dua atau lebih analisa clustering evaluasi pada clustering dapat dilakukan dengan berbagai cara yang dapat dikelompokkan sebagai evaluasi tanpa label dan evaluasi dengan label evaluasi tanpa label dilakukan dengan mengukur baik atau tidaknya hasil clustering tanpa menggunakan data luar atau label evaluasi ini disebut juga sebagai unsupervised measure evaluation', 'offsets_in_document': [{'start': 292, 'end': 434}], 'offsets_in_context': [{'start': 292, 'end': 434}], 'document_ids': ['4f0f576566298295e7e73d5faf516249-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana evaluasi clustering dengan menggunakan clustering ', 'filters': None, 'id': '552e538794c28214d4dda49eb7ecebef', 'no_answer': False, 'answers': [' evaluasi pada clustering dapat dilakukan dengan berbagai cara yang dapat dikelompokkan sebagai evaluasi tanpa label dan evaluasi dengan label'], 'offsets_in_documents': [{'start': 292, 'end': 434}], 'offsets_in_contexts': [{'start': 292, 'end': 434}], 'document_ids': ['4f0f576566298295e7e73d5faf516249-0'], 'contexts': ['cluster yang dihasilkan dinilai valid atau tidak yaitu untuk melihat apakah benar benar ada struktur dalam data yang telah dikelompokkan hasil clustering dibandingkan  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda  untuk membandingkan dua atau lebih analisa clustering evaluasi pada clustering dapat dilakukan dengan berbagai cara yang dapat dikelompokkan sebagai evaluasi tanpa label dan evaluasi dengan label evaluasi tanpa label dilakukan dengan mengukur baik atau tidaknya hasil clustering tanpa menggunakan data luar atau label evaluasi ini disebut juga sebagai unsupervised measure evaluation']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f37d9071-1703-4791-8f19-691140ddd0c7', 'query': 'apa yang dimaksud sebagai parameter minpts pada dbscan clustering ', 'document': {'id': '922d8c8e8b5644871acab205f575d10f-0', 'content': 'parameter eps adalah jarak yang menentukan ketetanggaan  neighborhood dua titik dianggap sebagai tetangga jika jarak di antara mereka kurang dari atau sama dengan eps parameter minpts adalah jumlah minimum titik data untuk menentukan cluster berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1084, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'parameter minpts adalah jumlah minimum titik data untuk menentukan cluster berdasarkan dua parameter ini  ', 'type': 'extractive', 'score': 0.0, 'context': 'parameter eps adalah jarak yang menentukan ketetanggaan  neighborhood dua titik dianggap sebagai tetangga jika jarak di antara mereka kurang dari atau sama dengan eps parameter minpts adalah jumlah minimum titik data untuk menentukan cluster berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps', 'offsets_in_document': [{'start': 167, 'end': 273}], 'offsets_in_context': [{'start': 167, 'end': 273}], 'document_ids': ['922d8c8e8b5644871acab205f575d10f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud sebagai parameter minpts pada dbscan clustering ', 'filters': None, 'id': 'ae0699dc852e57411826f286c9470093', 'no_answer': False, 'answers': ['parameter minpts adalah jumlah minimum titik data untuk menentukan cluster berdasarkan dua parameter ini  '], 'offsets_in_documents': [{'start': 167, 'end': 273}], 'offsets_in_contexts': [{'start': 167, 'end': 273}], 'document_ids': ['922d8c8e8b5644871acab205f575d10f-0'], 'contexts': ['parameter eps adalah jarak yang menentukan ketetanggaan  neighborhood dua titik dianggap sebagai tetangga jika jarak di antara mereka kurang dari atau sama dengan eps parameter minpts adalah jumlah minimum titik data untuk menentukan cluster berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps']}>,\n",
              " <MultiLabel: {'labels': [{'id': '067e0a01-d54c-49c3-a70e-01117c5c91b9', 'query': 'apa itu layer konvolusi?', 'document': {'id': 'd50ece99b3b6a7580e59efb2d6418e2-0', 'content': 'layer konvlusi adalah layer untuk ekstrasi fitur map secara spasialm dan sebuah cnn dapat memiliki beberapa layer konvlusi layer konvolusi pada bagian awal akan mempelajari representasi fitur atau pola spasial yang sederhana  sedangkan layer konvolusi yang lebih dalam akan mempelajari representasi fitur yang lebih kompleks pooling layer  layer ini berfungsi untuk mereduksi ukuran output dari activation map agar jumlah jumlah parameter jaringan menjadi lebih kecil sehingga dapat mempercepat komputasi dan menghindari overfitting setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1048, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'layer konvlusi adalah layer untuk ekstrasi fitur map secara spasialm dan sebuah cnn dapat memiliki beberapa layer konvlusi layer konvolusi pada bagian awal akan mempelajari representasi fitur atau pola spasial yang sederhana  sedangkan layer konvolusi yang lebih dalam akan mempelajari representasi fitur yang lebih kompleks', 'type': 'extractive', 'score': 0.0, 'context': 'layer konvlusi adalah layer untuk ekstrasi fitur map secara spasialm dan sebuah cnn dapat memiliki beberapa layer konvlusi layer konvolusi pada bagian awal akan mempelajari representasi fitur atau pola spasial yang sederhana  sedangkan layer konvolusi yang lebih dalam akan mempelajari representasi fitur yang lebih kompleks pooling layer  layer ini berfungsi untuk mereduksi ukuran output dari activation map agar jumlah jumlah parameter jaringan menjadi lebih kecil sehingga dapat mempercepat komputasi dan menghindari overfitting setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling', 'offsets_in_document': [{'start': 0, 'end': 324}], 'offsets_in_context': [{'start': 0, 'end': 324}], 'document_ids': ['d50ece99b3b6a7580e59efb2d6418e2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu layer konvolusi?', 'filters': None, 'id': '67132b139623a10f06d0b60afa1919f3', 'no_answer': False, 'answers': ['layer konvlusi adalah layer untuk ekstrasi fitur map secara spasialm dan sebuah cnn dapat memiliki beberapa layer konvlusi layer konvolusi pada bagian awal akan mempelajari representasi fitur atau pola spasial yang sederhana  sedangkan layer konvolusi yang lebih dalam akan mempelajari representasi fitur yang lebih kompleks'], 'offsets_in_documents': [{'start': 0, 'end': 324}], 'offsets_in_contexts': [{'start': 0, 'end': 324}], 'document_ids': ['d50ece99b3b6a7580e59efb2d6418e2-0'], 'contexts': ['layer konvlusi adalah layer untuk ekstrasi fitur map secara spasialm dan sebuah cnn dapat memiliki beberapa layer konvlusi layer konvolusi pada bagian awal akan mempelajari representasi fitur atau pola spasial yang sederhana  sedangkan layer konvolusi yang lebih dalam akan mempelajari representasi fitur yang lebih kompleks pooling layer  layer ini berfungsi untuk mereduksi ukuran output dari activation map agar jumlah jumlah parameter jaringan menjadi lebih kecil sehingga dapat mempercepat komputasi dan menghindari overfitting setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd8a67b8e-0eb9-4fdd-9ac7-3f5e8c81fe2a', 'query': 'kenapa model regresi liner lebih cocok digunakan untuk mendapatkan akurasi prediksi yang tinggi?', 'document': {'id': 'd7c245dbfdee18a4ad78ce8735453685-0', 'content': 'metrik mse  mean squared error  digunakan pada estimasi parameter menggunakan optimasi gradient descent untuk menunjukkan nilai error kelebihan regresi linear adalah ringan  mudah dipahami  mudah diimplementasikan dan tidak memerlukan tuning parameter kelemahan regresi linear adalah sangat sensitif terhadap outliers dan terlalu sederhana untuk real world problems  dengan mengasumsikan hubungan linear antara variabel pada regresi linier  data yang diplot memiliki hubungan linier atau berupa garis lurus antara dependent dan independent variables sehingga model regresi linier lebih cocok digunakan untuk mendapatkan akurasi prediksi yang tinggi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1052, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' data yang diplot memiliki hubungan linier atau berupa garis lurus antara dependent dan independent variables sehingga model regresi linier lebih cocok digunakan untuk mendapatkan akurasi prediksi yang tinggi', 'type': 'extractive', 'score': 0.0, 'context': 'metrik mse  mean squared error  digunakan pada estimasi parameter menggunakan optimasi gradient descent untuk menunjukkan nilai error kelebihan regresi linear adalah ringan  mudah dipahami  mudah diimplementasikan dan tidak memerlukan tuning parameter kelemahan regresi linear adalah sangat sensitif terhadap outliers dan terlalu sederhana untuk real world problems  dengan mengasumsikan hubungan linear antara variabel pada regresi linier  data yang diplot memiliki hubungan linier atau berupa garis lurus antara dependent dan independent variables sehingga model regresi linier lebih cocok digunakan untuk mendapatkan akurasi prediksi yang tinggi', 'offsets_in_document': [{'start': 440, 'end': 648}], 'offsets_in_context': [{'start': 440, 'end': 648}], 'document_ids': ['d7c245dbfdee18a4ad78ce8735453685-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kenapa model regresi liner lebih cocok digunakan untuk mendapatkan akurasi prediksi yang tinggi?', 'filters': None, 'id': 'cf3787da4475427bfb56a02389efb28e', 'no_answer': False, 'answers': [' data yang diplot memiliki hubungan linier atau berupa garis lurus antara dependent dan independent variables sehingga model regresi linier lebih cocok digunakan untuk mendapatkan akurasi prediksi yang tinggi'], 'offsets_in_documents': [{'start': 440, 'end': 648}], 'offsets_in_contexts': [{'start': 440, 'end': 648}], 'document_ids': ['d7c245dbfdee18a4ad78ce8735453685-0'], 'contexts': ['metrik mse  mean squared error  digunakan pada estimasi parameter menggunakan optimasi gradient descent untuk menunjukkan nilai error kelebihan regresi linear adalah ringan  mudah dipahami  mudah diimplementasikan dan tidak memerlukan tuning parameter kelemahan regresi linear adalah sangat sensitif terhadap outliers dan terlalu sederhana untuk real world problems  dengan mengasumsikan hubungan linear antara variabel pada regresi linier  data yang diplot memiliki hubungan linier atau berupa garis lurus antara dependent dan independent variables sehingga model regresi linier lebih cocok digunakan untuk mendapatkan akurasi prediksi yang tinggi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0caa1d35-e8ed-4b29-a4f7-a4e097aa29aa', 'query': 'apa yang dimaksud dengan dbscan ', 'document': {'id': '8822ad0766c97f45127804abde4b4c30-0', 'content': 'strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai dbscan  density based spatial clustering of application with noise  clustering adalah algoritma clustering berdasar kepadatan data algoritma ini mampu menemukan cluster dengan sembarang bentuk  tidak harus bentuk sphere  dan mampu menghandle noise dengan baik ide utama dbscan adalah bahwa suatu titik adalah milik suatu cluster jika terletak dekat dengan banyak titik dari cluster tersebut', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1078, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'dbscan  density based spatial clustering of application with noise  clustering adalah algoritma clustering berdasar kepadatan data', 'type': 'extractive', 'score': 0.0, 'context': 'strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai dbscan  density based spatial clustering of application with noise  clustering adalah algoritma clustering berdasar kepadatan data algoritma ini mampu menemukan cluster dengan sembarang bentuk  tidak harus bentuk sphere  dan mampu menghandle noise dengan baik ide utama dbscan adalah bahwa suatu titik adalah milik suatu cluster jika terletak dekat dengan banyak titik dari cluster tersebut', 'offsets_in_document': [{'start': 486, 'end': 616}], 'offsets_in_context': [{'start': 486, 'end': 616}], 'document_ids': ['8822ad0766c97f45127804abde4b4c30-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan dbscan ', 'filters': None, 'id': '6655cdbf957a9e5aca395c746c3ee41b', 'no_answer': False, 'answers': ['dbscan  density based spatial clustering of application with noise  clustering adalah algoritma clustering berdasar kepadatan data'], 'offsets_in_documents': [{'start': 486, 'end': 616}], 'offsets_in_contexts': [{'start': 486, 'end': 616}], 'document_ids': ['8822ad0766c97f45127804abde4b4c30-0'], 'contexts': ['strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai dbscan  density based spatial clustering of application with noise  clustering adalah algoritma clustering berdasar kepadatan data algoritma ini mampu menemukan cluster dengan sembarang bentuk  tidak harus bentuk sphere  dan mampu menghandle noise dengan baik ide utama dbscan adalah bahwa suatu titik adalah milik suatu cluster jika terletak dekat dengan banyak titik dari cluster tersebut']}>,\n",
              " <MultiLabel: {'labels': [{'id': '8fb56725-681f-447d-ba5e-d2092ce511d9', 'query': 'apa yang dimaksud dengan root mean squared error?', 'document': {'id': 'daea51d2708ee1c1122356935c1d81cc-0', 'content': 'yang ketiga  mean squared error  mse  adalah rata rata perbedaan kuadrat dari nilai aktual dalam kumpulan data dan nilai yang diprediksi oleh model mse berfokus pada kesalahan yang lebih besar  seperti saat kita mengkuadratkan efek kesalahan kesalahan besar menjadi lebih menonjol yang keempat  relative squared error  rse  yang mengambil kesalahan kuadrat total dan menormalkannya dengan membaginya dengan kuadrat total kesalahan prediktor sederhana yang kelima  root mean squared error  rmse  adalah perbedaan rata rata akar kuadrat antara nilai sebenarnya dan nilai prediksi dengan mengambil sebuah kuadrat dari mse  kita mendapatkan root mean square error', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1058, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'root mean squared error  rmse  adalah perbedaan rata rata akar kuadrat antara nilai sebenarnya dan nilai prediksi dengan mengambil sebuah kuadrat dari mse  kita mendapatkan root mean square error', 'type': 'extractive', 'score': 0.0, 'context': 'yang ketiga  mean squared error  mse  adalah rata rata perbedaan kuadrat dari nilai aktual dalam kumpulan data dan nilai yang diprediksi oleh model mse berfokus pada kesalahan yang lebih besar  seperti saat kita mengkuadratkan efek kesalahan kesalahan besar menjadi lebih menonjol yang keempat  relative squared error  rse  yang mengambil kesalahan kuadrat total dan menormalkannya dengan membaginya dengan kuadrat total kesalahan prediktor sederhana yang kelima  root mean squared error  rmse  adalah perbedaan rata rata akar kuadrat antara nilai sebenarnya dan nilai prediksi dengan mengambil sebuah kuadrat dari mse  kita mendapatkan root mean square error', 'offsets_in_document': [{'start': 464, 'end': 659}], 'offsets_in_context': [{'start': 464, 'end': 659}], 'document_ids': ['daea51d2708ee1c1122356935c1d81cc-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan root mean squared error?', 'filters': None, 'id': 'c260d33685b4e5500903818e00cc992c', 'no_answer': False, 'answers': ['root mean squared error  rmse  adalah perbedaan rata rata akar kuadrat antara nilai sebenarnya dan nilai prediksi dengan mengambil sebuah kuadrat dari mse  kita mendapatkan root mean square error'], 'offsets_in_documents': [{'start': 464, 'end': 659}], 'offsets_in_contexts': [{'start': 464, 'end': 659}], 'document_ids': ['daea51d2708ee1c1122356935c1d81cc-0'], 'contexts': ['yang ketiga  mean squared error  mse  adalah rata rata perbedaan kuadrat dari nilai aktual dalam kumpulan data dan nilai yang diprediksi oleh model mse berfokus pada kesalahan yang lebih besar  seperti saat kita mengkuadratkan efek kesalahan kesalahan besar menjadi lebih menonjol yang keempat  relative squared error  rse  yang mengambil kesalahan kuadrat total dan menormalkannya dengan membaginya dengan kuadrat total kesalahan prediktor sederhana yang kelima  root mean squared error  rmse  adalah perbedaan rata rata akar kuadrat antara nilai sebenarnya dan nilai prediksi dengan mengambil sebuah kuadrat dari mse  kita mendapatkan root mean square error']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0876c5dd-6dd1-42cc-9949-550daf3db11a', 'query': 'apa yang dimaksud dengan atribut binary ', 'document': {'id': '14915989008fa812ef15cdfbba18a69c-0', 'content': 'misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1049, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1', 'type': 'extractive', 'score': 0.0, 'context': 'misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'offsets_in_document': [{'start': 324, 'end': 401}], 'offsets_in_context': [{'start': 324, 'end': 401}], 'document_ids': ['14915989008fa812ef15cdfbba18a69c-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan atribut binary ', 'filters': None, 'id': 'c77a4768a315b593074d017e3632246d', 'no_answer': False, 'answers': [' atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1'], 'offsets_in_documents': [{'start': 324, 'end': 401}], 'offsets_in_contexts': [{'start': 324, 'end': 401}], 'document_ids': ['14915989008fa812ef15cdfbba18a69c-0'], 'contexts': ['misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd1b2f71d-9894-46e4-85b7-26bf17cc747a', 'query': 'apa yang dimaksud dengan atribut ', 'document': {'id': '14915989008fa812ef15cdfbba18a69c-0', 'content': 'misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1049, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'atribut juga dikenal dengan sebutan variabel atau fitur ', 'type': 'extractive', 'score': 0.0, 'context': 'misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'offsets_in_document': [{'start': 96, 'end': 152}], 'offsets_in_context': [{'start': 96, 'end': 152}], 'document_ids': ['14915989008fa812ef15cdfbba18a69c-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan atribut ', 'filters': None, 'id': 'b5d41e59b8756662d3b95bf311f0221a', 'no_answer': False, 'answers': ['atribut juga dikenal dengan sebutan variabel atau fitur '], 'offsets_in_documents': [{'start': 96, 'end': 152}], 'offsets_in_contexts': [{'start': 96, 'end': 152}], 'document_ids': ['14915989008fa812ef15cdfbba18a69c-0'], 'contexts': ['misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah']}>,\n",
              " <MultiLabel: {'labels': [{'id': '116106b4-a96d-4c1e-8e6e-345bbd8469ff', 'query': 'atribut terbagi menjadi tipe apa saja ', 'document': {'id': '14915989008fa812ef15cdfbba18a69c-0', 'content': 'misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1049, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' nominal  binary  ordinal  dan numeric', 'type': 'extractive', 'score': 0.0, 'context': 'misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'offsets_in_document': [{'start': 202, 'end': 240}], 'offsets_in_context': [{'start': 202, 'end': 240}], 'document_ids': ['14915989008fa812ef15cdfbba18a69c-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'atribut terbagi menjadi tipe apa saja ', 'filters': None, 'id': 'c0955275d8843fa40b2c19c12a272422', 'no_answer': False, 'answers': [' nominal  binary  ordinal  dan numeric'], 'offsets_in_documents': [{'start': 202, 'end': 240}], 'offsets_in_contexts': [{'start': 202, 'end': 240}], 'document_ids': ['14915989008fa812ef15cdfbba18a69c-0'], 'contexts': ['misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'afdf1b18-7921-428b-b3a3-fabee9af23a9', 'query': 'apa yang dimaksud dengan atribut ordinal ', 'document': {'id': '14915989008fa812ef15cdfbba18a69c-0', 'content': 'misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1049, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'type': 'extractive', 'score': 0.0, 'context': 'misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah', 'offsets_in_document': [{'start': 402, 'end': 492}], 'offsets_in_context': [{'start': 402, 'end': 492}], 'document_ids': ['14915989008fa812ef15cdfbba18a69c-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan atribut ordinal ', 'filters': None, 'id': '21b179a561f8ea6fbc148484a9b322a3', 'no_answer': False, 'answers': [' atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah'], 'offsets_in_documents': [{'start': 402, 'end': 492}], 'offsets_in_contexts': [{'start': 402, 'end': 492}], 'document_ids': ['14915989008fa812ef15cdfbba18a69c-0'], 'contexts': ['misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur atribut terbagi menjadi beberapa tipe  antara lain nominal  binary  ordinal  dan numeric atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'a1d3a253-939b-4312-bb94-7490b1181fe8', 'query': 'kenapa model non linear lebih rumit?', 'document': {'id': 'd928e12bc22108714032f652521aef98-0', 'content': 'model non linier lebih rumit daripada model linier untuk dikembangkan karena fungsi dibuat melalui serangkaian perkiraan  iterasi  itu mungkin berasal dari trial and error support vector machines  svm  merupakan algoritma yang banyak digunakan pada tugas klasifikasi support vector regression  svr  merupakan algoritma regresi yang didasarkan pada svm  dan bertujuan untuk memprediksi real values persamaan pada support vector regression sama seperti pada linear regression  yaitu y   wx b pada svr  garis lurus disebut dengan hyperplane', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1054, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'model non linier lebih rumit daripada model linier untuk dikembangkan karena fungsi dibuat melalui serangkaian perkiraan  iterasi  itu mungkin berasal dari trial and error', 'type': 'extractive', 'score': 0.0, 'context': 'model non linier lebih rumit daripada model linier untuk dikembangkan karena fungsi dibuat melalui serangkaian perkiraan  iterasi  itu mungkin berasal dari trial and error support vector machines  svm  merupakan algoritma yang banyak digunakan pada tugas klasifikasi support vector regression  svr  merupakan algoritma regresi yang didasarkan pada svm  dan bertujuan untuk memprediksi real values persamaan pada support vector regression sama seperti pada linear regression  yaitu y   wx b pada svr  garis lurus disebut dengan hyperplane', 'offsets_in_document': [{'start': 0, 'end': 171}], 'offsets_in_context': [{'start': 0, 'end': 171}], 'document_ids': ['d928e12bc22108714032f652521aef98-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kenapa model non linear lebih rumit?', 'filters': None, 'id': 'f68baff0b22d7255864c4e7b8503cca3', 'no_answer': False, 'answers': ['model non linier lebih rumit daripada model linier untuk dikembangkan karena fungsi dibuat melalui serangkaian perkiraan  iterasi  itu mungkin berasal dari trial and error'], 'offsets_in_documents': [{'start': 0, 'end': 171}], 'offsets_in_contexts': [{'start': 0, 'end': 171}], 'document_ids': ['d928e12bc22108714032f652521aef98-0'], 'contexts': ['model non linier lebih rumit daripada model linier untuk dikembangkan karena fungsi dibuat melalui serangkaian perkiraan  iterasi  itu mungkin berasal dari trial and error support vector machines  svm  merupakan algoritma yang banyak digunakan pada tugas klasifikasi support vector regression  svr  merupakan algoritma regresi yang didasarkan pada svm  dan bertujuan untuk memprediksi real values persamaan pada support vector regression sama seperti pada linear regression  yaitu y   wx b pada svr  garis lurus disebut dengan hyperplane']}>,\n",
              " <MultiLabel: {'labels': [{'id': '416669b2-9d22-4aef-8d17-4b8fbdc5f650', 'query': 'apa yang dimaksud dengan regresi linear ', 'document': {'id': 'b89981b664fd0bd0fcb5679144b3c48f-0', 'content': ' regresi linier adalah model pembelajaran mesin yang diawasi dimana model menemukan hubungan linier antara variabel bebas dan variabel tak bebas variabel tak bebas  y  adalah variabel yang ingin kita prediksi atau jelaskan  sementara variabel bebas  x  adalah variabel yang digunakan untuk menjelaskan variabel tak bebas regresi linier terdiri dari dua jenis yaitu regresi linier sederhana yang hanya memiliki satu variabel bebas dengan persamaan y   a   bx dan regresi variabel jamak yang memiliki lebih dari satu variabel bebas dengan persamaan y   b0   b1x1  b2x2 b3x3. bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1059, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'regresi linier adalah model pembelajaran mesin yang diawasi dimana model menemukan hubungan linier antara variabel bebas dan variabel tak bebas', 'type': 'extractive', 'score': 0.0, 'context': ' regresi linier adalah model pembelajaran mesin yang diawasi dimana model menemukan hubungan linier antara variabel bebas dan variabel tak bebas variabel tak bebas  y  adalah variabel yang ingin kita prediksi atau jelaskan  sementara variabel bebas  x  adalah variabel yang digunakan untuk menjelaskan variabel tak bebas regresi linier terdiri dari dua jenis yaitu regresi linier sederhana yang hanya memiliki satu variabel bebas dengan persamaan y   a   bx dan regresi variabel jamak yang memiliki lebih dari satu variabel bebas dengan persamaan y   b0   b1x1  b2x2 b3x3. bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent', 'offsets_in_document': [{'start': 1, 'end': 144}], 'offsets_in_context': [{'start': 1, 'end': 144}], 'document_ids': ['b89981b664fd0bd0fcb5679144b3c48f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan regresi linear ', 'filters': None, 'id': '5910be41eaea31fd3db3d79bedfa8d8c', 'no_answer': False, 'answers': ['regresi linier adalah model pembelajaran mesin yang diawasi dimana model menemukan hubungan linier antara variabel bebas dan variabel tak bebas'], 'offsets_in_documents': [{'start': 1, 'end': 144}], 'offsets_in_contexts': [{'start': 1, 'end': 144}], 'document_ids': ['b89981b664fd0bd0fcb5679144b3c48f-0'], 'contexts': [' regresi linier adalah model pembelajaran mesin yang diawasi dimana model menemukan hubungan linier antara variabel bebas dan variabel tak bebas variabel tak bebas  y  adalah variabel yang ingin kita prediksi atau jelaskan  sementara variabel bebas  x  adalah variabel yang digunakan untuk menjelaskan variabel tak bebas regresi linier terdiri dari dua jenis yaitu regresi linier sederhana yang hanya memiliki satu variabel bebas dengan persamaan y   a   bx dan regresi variabel jamak yang memiliki lebih dari satu variabel bebas dengan persamaan y   b0   b1x1  b2x2 b3x3. bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b75c1e71-9350-4e6f-a23e-c28fb37a3570', 'query': 'apa saja dua jenis regresi linear ', 'document': {'id': 'b89981b664fd0bd0fcb5679144b3c48f-0', 'content': ' regresi linier adalah model pembelajaran mesin yang diawasi dimana model menemukan hubungan linier antara variabel bebas dan variabel tak bebas variabel tak bebas  y  adalah variabel yang ingin kita prediksi atau jelaskan  sementara variabel bebas  x  adalah variabel yang digunakan untuk menjelaskan variabel tak bebas regresi linier terdiri dari dua jenis yaitu regresi linier sederhana yang hanya memiliki satu variabel bebas dengan persamaan y   a   bx dan regresi variabel jamak yang memiliki lebih dari satu variabel bebas dengan persamaan y   b0   b1x1  b2x2 b3x3. bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1059, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' regresi linier terdiri dari dua jenis yaitu regresi linier sederhana yang hanya memiliki satu variabel bebas dengan persamaan y   a   bx dan regresi variabel jamak yang memiliki lebih dari satu variabel bebas dengan persamaan y   b0   b1x1  b2x2 b3x3.', 'type': 'extractive', 'score': 0.0, 'context': ' regresi linier adalah model pembelajaran mesin yang diawasi dimana model menemukan hubungan linier antara variabel bebas dan variabel tak bebas variabel tak bebas  y  adalah variabel yang ingin kita prediksi atau jelaskan  sementara variabel bebas  x  adalah variabel yang digunakan untuk menjelaskan variabel tak bebas regresi linier terdiri dari dua jenis yaitu regresi linier sederhana yang hanya memiliki satu variabel bebas dengan persamaan y   a   bx dan regresi variabel jamak yang memiliki lebih dari satu variabel bebas dengan persamaan y   b0   b1x1  b2x2 b3x3. bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent', 'offsets_in_document': [{'start': 320, 'end': 572}], 'offsets_in_context': [{'start': 320, 'end': 572}], 'document_ids': ['b89981b664fd0bd0fcb5679144b3c48f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja dua jenis regresi linear ', 'filters': None, 'id': '79ebcf17bed652117b10231405860474', 'no_answer': False, 'answers': [' regresi linier terdiri dari dua jenis yaitu regresi linier sederhana yang hanya memiliki satu variabel bebas dengan persamaan y   a   bx dan regresi variabel jamak yang memiliki lebih dari satu variabel bebas dengan persamaan y   b0   b1x1  b2x2 b3x3.'], 'offsets_in_documents': [{'start': 320, 'end': 572}], 'offsets_in_contexts': [{'start': 320, 'end': 572}], 'document_ids': ['b89981b664fd0bd0fcb5679144b3c48f-0'], 'contexts': [' regresi linier adalah model pembelajaran mesin yang diawasi dimana model menemukan hubungan linier antara variabel bebas dan variabel tak bebas variabel tak bebas  y  adalah variabel yang ingin kita prediksi atau jelaskan  sementara variabel bebas  x  adalah variabel yang digunakan untuk menjelaskan variabel tak bebas regresi linier terdiri dari dua jenis yaitu regresi linier sederhana yang hanya memiliki satu variabel bebas dengan persamaan y   a   bx dan regresi variabel jamak yang memiliki lebih dari satu variabel bebas dengan persamaan y   b0   b1x1  b2x2 b3x3. bnxn parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dapat di estimasi menggunakan dua cara yaitu least squares dengan menggunakan operasi aljabar linier dan cara kedua yaitu algoritma optimasi dengan gradient descent']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e072f72b-cb40-4852-8e88-46d0fd757e45', 'query': 'apa saja kelebihan support vector regression ', 'document': {'id': '4869ad62b741eca84728ab4f1f885641-0', 'content': 'konsep dari slack variable yaitu untuk setiap nilai yang berada di luar ϵ  dapat dinyatakan deviasinya dari margin sebagai ξ deviasi berusaha untuk diminimalkan  sehingga dapat ditambahkan pada objective function  minimize   yaitu min ½   w   2   c _ i 1  n   ξi    dengan batasan  yi   wixi   ɛ  ξi kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1067, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi ', 'type': 'extractive', 'score': 0.0, 'context': 'konsep dari slack variable yaitu untuk setiap nilai yang berada di luar ϵ  dapat dinyatakan deviasinya dari margin sebagai ξ deviasi berusaha untuk diminimalkan  sehingga dapat ditambahkan pada objective function  minimize   yaitu min ½   w   2   c _ i 1  n   ξi    dengan batasan  yi   wixi   ɛ  ξi kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi', 'offsets_in_document': [{'start': 300, 'end': 418}], 'offsets_in_context': [{'start': 300, 'end': 418}], 'document_ids': ['4869ad62b741eca84728ab4f1f885641-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja kelebihan support vector regression ', 'filters': None, 'id': '47207bb2378fb44001d3ca68295f187e', 'no_answer': False, 'answers': ['kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi '], 'offsets_in_documents': [{'start': 300, 'end': 418}], 'offsets_in_contexts': [{'start': 300, 'end': 418}], 'document_ids': ['4869ad62b741eca84728ab4f1f885641-0'], 'contexts': ['konsep dari slack variable yaitu untuk setiap nilai yang berada di luar ϵ  dapat dinyatakan deviasinya dari margin sebagai ξ deviasi berusaha untuk diminimalkan  sehingga dapat ditambahkan pada objective function  minimize   yaitu min ½   w   2   c _ i 1  n   ξi    dengan batasan  yi   wixi   ɛ  ξi kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '109c2c79-d4e7-4a68-9ffb-dcb03aa1e589', 'query': 'apa saja kelemahan support vector regression ', 'document': {'id': '4869ad62b741eca84728ab4f1f885641-0', 'content': 'konsep dari slack variable yaitu untuk setiap nilai yang berada di luar ϵ  dapat dinyatakan deviasinya dari margin sebagai ξ deviasi berusaha untuk diminimalkan  sehingga dapat ditambahkan pada objective function  minimize   yaitu min ½   w   2   c _ i 1  n   ξi    dengan batasan  yi   wixi   ɛ  ξi kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1067, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise ', 'type': 'extractive', 'score': 0.0, 'context': 'konsep dari slack variable yaitu untuk setiap nilai yang berada di luar ϵ  dapat dinyatakan deviasinya dari margin sebagai ξ deviasi berusaha untuk diminimalkan  sehingga dapat ditambahkan pada objective function  minimize   yaitu min ½   w   2   c _ i 1  n   ξi    dengan batasan  yi   wixi   ɛ  ξi kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi', 'offsets_in_document': [{'start': 427, 'end': 535}], 'offsets_in_context': [{'start': 427, 'end': 535}], 'document_ids': ['4869ad62b741eca84728ab4f1f885641-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja kelemahan support vector regression ', 'filters': None, 'id': 'a86d117f3ba0f2d4cba81d76a177c9ef', 'no_answer': False, 'answers': [' kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise '], 'offsets_in_documents': [{'start': 427, 'end': 535}], 'offsets_in_contexts': [{'start': 427, 'end': 535}], 'document_ids': ['4869ad62b741eca84728ab4f1f885641-0'], 'contexts': ['konsep dari slack variable yaitu untuk setiap nilai yang berada di luar ϵ  dapat dinyatakan deviasinya dari margin sebagai ξ deviasi berusaha untuk diminimalkan  sehingga dapat ditambahkan pada objective function  minimize   yaitu min ½   w   2   c _ i 1  n   ξi    dengan batasan  yi   wixi   ɛ  ξi kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '1526e18f-9a62-4d8a-9a48-bde3f0a6c91f', 'query': 'apa yang dimaksud dengan metode bottom up dalam hierarchical clustering ', 'document': {'id': 'd55b992461e577442e7acac18843a21-0', 'content': 'karena k adalah jumlah cluster  maka k adalah berupa bilangan integer positif berikut adalah algoritma k means clustering  1  pilih sejumlah k titik sebagai centroid awal  2  ulangi langkah langkah 3 dan 4 hingga centroid tidak berubah  3  bentuk sejumlah k cluster dengan mengelompokkan masing masing data point ke centroid terdekat  4  hitung ulang centroid masing masing kelompok  cluster hierarchical clustering adalah metode analisis kelompok yang berusaha untuk membangun sebuah hirarki kelompok data strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1077, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'agglomerative  bottom up', 'type': 'extractive', 'score': 0.0, 'context': 'karena k adalah jumlah cluster  maka k adalah berupa bilangan integer positif berikut adalah algoritma k means clustering  1  pilih sejumlah k titik sebagai centroid awal  2  ulangi langkah langkah 3 dan 4 hingga centroid tidak berubah  3  bentuk sejumlah k cluster dengan mengelompokkan masing masing data point ke centroid terdekat  4  hitung ulang centroid masing masing kelompok  cluster hierarchical clustering adalah metode analisis kelompok yang berusaha untuk membangun sebuah hirarki kelompok data strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai', 'offsets_in_document': [{'start': 559, 'end': 583}], 'offsets_in_context': [{'start': 559, 'end': 583}], 'document_ids': ['d55b992461e577442e7acac18843a21-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan metode bottom up dalam hierarchical clustering ', 'filters': None, 'id': '38faf24bf1b394f0ea255f762db332c2', 'no_answer': False, 'answers': ['agglomerative  bottom up'], 'offsets_in_documents': [{'start': 559, 'end': 583}], 'offsets_in_contexts': [{'start': 559, 'end': 583}], 'document_ids': ['d55b992461e577442e7acac18843a21-0'], 'contexts': ['karena k adalah jumlah cluster  maka k adalah berupa bilangan integer positif berikut adalah algoritma k means clustering  1  pilih sejumlah k titik sebagai centroid awal  2  ulangi langkah langkah 3 dan 4 hingga centroid tidak berubah  3  bentuk sejumlah k cluster dengan mengelompokkan masing masing data point ke centroid terdekat  4  hitung ulang centroid masing masing kelompok  cluster hierarchical clustering adalah metode analisis kelompok yang berusaha untuk membangun sebuah hirarki kelompok data strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e4cda63f-4a99-4c50-8365-298f98747a66', 'query': 'apa yang dimaksud dengan metode top down dalam hierarchical clustering ', 'document': {'id': 'd55b992461e577442e7acac18843a21-0', 'content': 'karena k adalah jumlah cluster  maka k adalah berupa bilangan integer positif berikut adalah algoritma k means clustering  1  pilih sejumlah k titik sebagai centroid awal  2  ulangi langkah langkah 3 dan 4 hingga centroid tidak berubah  3  bentuk sejumlah k cluster dengan mengelompokkan masing masing data point ke centroid terdekat  4  hitung ulang centroid masing masing kelompok  cluster hierarchical clustering adalah metode analisis kelompok yang berusaha untuk membangun sebuah hirarki kelompok data strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1077, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'divisive  top down ', 'type': 'extractive', 'score': 0.0, 'context': 'karena k adalah jumlah cluster  maka k adalah berupa bilangan integer positif berikut adalah algoritma k means clustering  1  pilih sejumlah k titik sebagai centroid awal  2  ulangi langkah langkah 3 dan 4 hingga centroid tidak berubah  3  bentuk sejumlah k cluster dengan mengelompokkan masing masing data point ke centroid terdekat  4  hitung ulang centroid masing masing kelompok  cluster hierarchical clustering adalah metode analisis kelompok yang berusaha untuk membangun sebuah hirarki kelompok data strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai', 'offsets_in_document': [{'start': 589, 'end': 608}], 'offsets_in_context': [{'start': 589, 'end': 608}], 'document_ids': ['d55b992461e577442e7acac18843a21-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan metode top down dalam hierarchical clustering ', 'filters': None, 'id': 'c5fba04afee62d2e2454f25a290ce831', 'no_answer': False, 'answers': ['divisive  top down '], 'offsets_in_documents': [{'start': 589, 'end': 608}], 'offsets_in_contexts': [{'start': 589, 'end': 608}], 'document_ids': ['d55b992461e577442e7acac18843a21-0'], 'contexts': ['karena k adalah jumlah cluster  maka k adalah berupa bilangan integer positif berikut adalah algoritma k means clustering  1  pilih sejumlah k titik sebagai centroid awal  2  ulangi langkah langkah 3 dan 4 hingga centroid tidak berubah  3  bentuk sejumlah k cluster dengan mengelompokkan masing masing data point ke centroid terdekat  4  hitung ulang centroid masing masing kelompok  cluster hierarchical clustering adalah metode analisis kelompok yang berusaha untuk membangun sebuah hirarki kelompok data strategi pengelompokannya umumnya ada 2 jenis yaitu agglomerative  bottom up  dan divisive  top down agglomerative hierarchical cluster dilakukan dengan  1  menghitung matriks jarak antar data  2  ulangi langkah 3 dan 4 sehingga hanya satu kelompok yang tersisa  3  gabungkan dua kelompok terdekat berdasarkan parameter kedekatan yang ditentukan  4  perbarui matriks jarak antar data untuk merepresentasikan kedekatan di antara kelompok baru dan kelompok yang masih tersisa  5  selesai']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3343fac1-5fdd-44ee-9676-e17195f7a7c5', 'query': 'apa yang dilakukan pertama kali dalam proses pembelajaran pada cnn ', 'document': {'id': '7156fcc2669cd96e39d70d3136431804-0', 'content': 'setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1047, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'inisialisasi nilai bobot bobot jaringan secara random', 'type': 'extractive', 'score': 0.0, 'context': 'setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi', 'offsets_in_document': [{'start': 639, 'end': 692}], 'offsets_in_context': [{'start': 639, 'end': 692}], 'document_ids': ['7156fcc2669cd96e39d70d3136431804-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dilakukan pertama kali dalam proses pembelajaran pada cnn ', 'filters': None, 'id': '633b674bcde910893aa83b2f2615e4ae', 'no_answer': False, 'answers': ['inisialisasi nilai bobot bobot jaringan secara random'], 'offsets_in_documents': [{'start': 639, 'end': 692}], 'offsets_in_contexts': [{'start': 639, 'end': 692}], 'document_ids': ['7156fcc2669cd96e39d70d3136431804-0'], 'contexts': ['setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '60207b11-2b20-49a1-b075-9730efd92cd9', 'query': 'kapan proses pembelajaran cnn akan berakhir ', 'document': {'id': '7156fcc2669cd96e39d70d3136431804-0', 'content': 'setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1047, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'hingga kondisi berhenti terpenuhi', 'type': 'extractive', 'score': 0.0, 'context': 'setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi', 'offsets_in_document': [{'start': 1030, 'end': 1063}], 'offsets_in_context': [{'start': 1030, 'end': 1063}], 'document_ids': ['7156fcc2669cd96e39d70d3136431804-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kapan proses pembelajaran cnn akan berakhir ', 'filters': None, 'id': '71a4a0921b758a519a6cedfe55922533', 'no_answer': False, 'answers': ['hingga kondisi berhenti terpenuhi'], 'offsets_in_documents': [{'start': 1030, 'end': 1063}], 'offsets_in_contexts': [{'start': 1030, 'end': 1063}], 'document_ids': ['7156fcc2669cd96e39d70d3136431804-0'], 'contexts': ['setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '8df0332b-9e83-4fa8-bffc-7de2d73991c7', 'query': 'bagaimana contoh operasi pooling layer ', 'document': {'id': '7156fcc2669cd96e39d70d3136431804-0', 'content': 'setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1047, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2', 'type': 'extractive', 'score': 0.0, 'context': 'setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi', 'offsets_in_document': [{'start': 368, 'end': 515}], 'offsets_in_context': [{'start': 368, 'end': 515}], 'document_ids': ['7156fcc2669cd96e39d70d3136431804-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana contoh operasi pooling layer ', 'filters': None, 'id': 'aebbf980814665e6afdfac65d6b46a30', 'no_answer': False, 'answers': ['contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2'], 'offsets_in_documents': [{'start': 368, 'end': 515}], 'offsets_in_contexts': [{'start': 368, 'end': 515}], 'document_ids': ['7156fcc2669cd96e39d70d3136431804-0'], 'contexts': ['setiap pooling layer umumnya memiliki 2 parameter  yaitu ukuran filter dan panjang stride  pergeseran terdapat 2 jenis operasi pooling yakni max pooling dan average pooling max pooling   mengambil nilai tertinggi dari nilai masukan yang ditempati oleh filter  sedangkan average pooling   mengambil nilai rata rata dari nilai masukan yang ditempati oleh filter sebagai contoh  operasi max pooling pada sebuah masukan berukuran 4x4 dengan ukuran filter 2x2 dan panjang stride 1 akan menghasilkan matriks berukuran 2x2.pembelajaran pada convulutional neural network adalah sebagai berikut  proses pembelajaran pada cnn umumnya diawali dengan inisialisasi nilai bobot bobot jaringan secara random  proses forward yaitu memasukkan data latih ke jaringan dan menghitung output  nya  lalu menghitung loss antara output jaringan dengan target output seharusnya yang telah diketahui pada data latih  proses backward untuk memperbaiki nilai bobot bobot jaringan yang dapat meminimalkan nilai loss nya dan yang terakhir kembali ke langkah 2 hingga kondisi berhenti terpenuhi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '32af38cd-2eed-4dbc-9049-0b37b6c8c12f', 'query': 'apa itu regresi non linear?', 'document': {'id': 'f88d9f8c6ad06154187744e2589a40d6-0', 'content': 'regresi non linear adalah salah satu bentuk regresi analisis di mana data cocok dengan model dan kemudian dinyatakan sebagai fungsi matematika regresi linier sederhana menghubungkan dua variabel  x dan y  dengan garis lurus  y   mx   b   sedangkan nonlinier regresi menghubungkan kedua variabel secara nonlinier hubungan  melengkung pemodelan regresi nonlinier mirip dengan pemodelan regresi linear di mana keduanya berusaha untuk melacak respon tertentu dari sekumpulan variabel secara grafis model non linier lebih rumit daripada model linier untuk dikembangkan karena fungsi dibuat melalui serangkaian perkiraan  iterasi  itu mungkin berasal dari trial and error support vector machines  svm  merupakan algoritma yang banyak digunakan pada tugas klasifikasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1053, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'regresi non linear adalah salah satu bentuk regresi analisis di mana data cocok dengan model dan kemudian dinyatakan sebagai fungsi matematika ', 'type': 'extractive', 'score': 0.0, 'context': 'regresi non linear adalah salah satu bentuk regresi analisis di mana data cocok dengan model dan kemudian dinyatakan sebagai fungsi matematika regresi linier sederhana menghubungkan dua variabel  x dan y  dengan garis lurus  y   mx   b   sedangkan nonlinier regresi menghubungkan kedua variabel secara nonlinier hubungan  melengkung pemodelan regresi nonlinier mirip dengan pemodelan regresi linear di mana keduanya berusaha untuk melacak respon tertentu dari sekumpulan variabel secara grafis model non linier lebih rumit daripada model linier untuk dikembangkan karena fungsi dibuat melalui serangkaian perkiraan  iterasi  itu mungkin berasal dari trial and error support vector machines  svm  merupakan algoritma yang banyak digunakan pada tugas klasifikasi', 'offsets_in_document': [{'start': 0, 'end': 143}], 'offsets_in_context': [{'start': 0, 'end': 143}], 'document_ids': ['f88d9f8c6ad06154187744e2589a40d6-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu regresi non linear?', 'filters': None, 'id': '17396a5e26b80d25ad62765a24e3fc8c', 'no_answer': False, 'answers': ['regresi non linear adalah salah satu bentuk regresi analisis di mana data cocok dengan model dan kemudian dinyatakan sebagai fungsi matematika '], 'offsets_in_documents': [{'start': 0, 'end': 143}], 'offsets_in_contexts': [{'start': 0, 'end': 143}], 'document_ids': ['f88d9f8c6ad06154187744e2589a40d6-0'], 'contexts': ['regresi non linear adalah salah satu bentuk regresi analisis di mana data cocok dengan model dan kemudian dinyatakan sebagai fungsi matematika regresi linier sederhana menghubungkan dua variabel  x dan y  dengan garis lurus  y   mx   b   sedangkan nonlinier regresi menghubungkan kedua variabel secara nonlinier hubungan  melengkung pemodelan regresi nonlinier mirip dengan pemodelan regresi linear di mana keduanya berusaha untuk melacak respon tertentu dari sekumpulan variabel secara grafis model non linier lebih rumit daripada model linier untuk dikembangkan karena fungsi dibuat melalui serangkaian perkiraan  iterasi  itu mungkin berasal dari trial and error support vector machines  svm  merupakan algoritma yang banyak digunakan pada tugas klasifikasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'df141d9b-a148-4b11-b411-71fbacaf614e', 'query': 'apa saja yang termasuk metrik evaluasi regresi ', 'document': {'id': '40e2a0787959e585bd94760f6503a039-0', 'content': 'metrik evaluasi regresi diantaranya pertama adalah mean absolute error  mae  adalah rata rata perbedaan absolut antara nilai aktual dalam kumpulan data dan nilai yang diprediksi semakin kecil mae  semakin akurat modelnya jika mae nol  itu menunjukkan modelnya sempurna jika mae besar maka modelnya tidak bagus yang kedua  relative absolute error  rae  dinyatakan sebagai rasio  membandingkan mean error  residual  dengan kesalahan yang dihasilkan oleh sebuah trivial atau naive model', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1057, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'metrik evaluasi regresi diantaranya pertama adalah mean absolute error  mae ', 'type': 'extractive', 'score': 0.0, 'context': 'metrik evaluasi regresi diantaranya pertama adalah mean absolute error  mae  adalah rata rata perbedaan absolut antara nilai aktual dalam kumpulan data dan nilai yang diprediksi semakin kecil mae  semakin akurat modelnya jika mae nol  itu menunjukkan modelnya sempurna jika mae besar maka modelnya tidak bagus yang kedua  relative absolute error  rae  dinyatakan sebagai rasio  membandingkan mean error  residual  dengan kesalahan yang dihasilkan oleh sebuah trivial atau naive model', 'offsets_in_document': [{'start': 0, 'end': 76}], 'offsets_in_context': [{'start': 0, 'end': 76}], 'document_ids': ['40e2a0787959e585bd94760f6503a039-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja yang termasuk metrik evaluasi regresi ', 'filters': None, 'id': 'ce2b0507fd1f1b704a22580cd0112f67', 'no_answer': False, 'answers': ['metrik evaluasi regresi diantaranya pertama adalah mean absolute error  mae '], 'offsets_in_documents': [{'start': 0, 'end': 76}], 'offsets_in_contexts': [{'start': 0, 'end': 76}], 'document_ids': ['40e2a0787959e585bd94760f6503a039-0'], 'contexts': ['metrik evaluasi regresi diantaranya pertama adalah mean absolute error  mae  adalah rata rata perbedaan absolut antara nilai aktual dalam kumpulan data dan nilai yang diprediksi semakin kecil mae  semakin akurat modelnya jika mae nol  itu menunjukkan modelnya sempurna jika mae besar maka modelnya tidak bagus yang kedua  relative absolute error  rae  dinyatakan sebagai rasio  membandingkan mean error  residual  dengan kesalahan yang dihasilkan oleh sebuah trivial atau naive model']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b7aea830-a2ad-4b24-a78a-8e6e2d73f4b8', 'query': 'apa itu prediksi akhir ', 'document': {'id': '5db46bdcb184b95a02cb8e47b07d4fac-0', 'content': 'prediksi akhir adalah rata rata dari nilai variabel dependen dalam leaf node tertentu melalui beberapa iterasi  pohon mampu memprediksi nilai yang tepat untuk titik data kelebihan decision tree regression adalah mudah dipahami dan diinterpretasikan  dapat bekerja pada fitur numerical maupun fitur categorical  dan tidka memerlukan banyak pemrosesan data  seperti one hot encoding  dummy variables  etc sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1064, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'prediksi akhir adalah rata rata dari nilai variabel dependen dalam leaf node tertentu melalui beberapa iterasi', 'type': 'extractive', 'score': 0.0, 'context': 'prediksi akhir adalah rata rata dari nilai variabel dependen dalam leaf node tertentu melalui beberapa iterasi  pohon mampu memprediksi nilai yang tepat untuk titik data kelebihan decision tree regression adalah mudah dipahami dan diinterpretasikan  dapat bekerja pada fitur numerical maupun fitur categorical  dan tidka memerlukan banyak pemrosesan data  seperti one hot encoding  dummy variables  etc sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan', 'offsets_in_document': [{'start': 0, 'end': 110}], 'offsets_in_context': [{'start': 0, 'end': 110}], 'document_ids': ['5db46bdcb184b95a02cb8e47b07d4fac-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu prediksi akhir ', 'filters': None, 'id': 'cc1a4ebf570ff2ef66acadff472f1e03', 'no_answer': False, 'answers': ['prediksi akhir adalah rata rata dari nilai variabel dependen dalam leaf node tertentu melalui beberapa iterasi'], 'offsets_in_documents': [{'start': 0, 'end': 110}], 'offsets_in_contexts': [{'start': 0, 'end': 110}], 'document_ids': ['5db46bdcb184b95a02cb8e47b07d4fac-0'], 'contexts': ['prediksi akhir adalah rata rata dari nilai variabel dependen dalam leaf node tertentu melalui beberapa iterasi  pohon mampu memprediksi nilai yang tepat untuk titik data kelebihan decision tree regression adalah mudah dipahami dan diinterpretasikan  dapat bekerja pada fitur numerical maupun fitur categorical  dan tidka memerlukan banyak pemrosesan data  seperti one hot encoding  dummy variables  etc sementara kelemahananya adalah cenderung overfit dan perubahan kecil pada data cenderung menyebabkan perbedaan besar pada struktur decision tree  sehingga menyebabkan ketidakstabilan random forest merupakan algoritma berbasis  tree  yang menggunakan fitur kualitas dari beberapa decision tree untuk membuat keputusan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b23260ab-955b-4259-aea8-1ec770a7a1f3', 'query': 'bagaimana sebuah titik ditentukan sebagai titik yang bertetangga dalam dbscan clustering ', 'document': {'id': '30f0a8b695a098b0d9740a5d4a3b97f3-0', 'content': 'algoritma ini mampu menemukan cluster dengan sembarang bentuk  tidak harus bentuk sphere  dan mampu menghandle noise dengan baik ide utama dbscan adalah bahwa suatu titik adalah milik suatu cluster jika terletak dekat dengan banyak titik dari cluster tersebut terdapat dua parameter utama dalam dbscan  yaitu parameter eps dan parameter minpts parameter eps adalah jarak yang menentukan ketetanggaan  neighborhood dua titik dianggap sebagai tetangga jika jarak di antara mereka kurang dari atau sama dengan eps', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1079, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'dua titik dianggap sebagai tetangga jika jarak di antara mereka kurang dari atau sama dengan eps', 'type': 'extractive', 'score': 0.0, 'context': 'algoritma ini mampu menemukan cluster dengan sembarang bentuk  tidak harus bentuk sphere  dan mampu menghandle noise dengan baik ide utama dbscan adalah bahwa suatu titik adalah milik suatu cluster jika terletak dekat dengan banyak titik dari cluster tersebut terdapat dua parameter utama dalam dbscan  yaitu parameter eps dan parameter minpts parameter eps adalah jarak yang menentukan ketetanggaan  neighborhood dua titik dianggap sebagai tetangga jika jarak di antara mereka kurang dari atau sama dengan eps', 'offsets_in_document': [{'start': 414, 'end': 510}], 'offsets_in_context': [{'start': 414, 'end': 510}], 'document_ids': ['30f0a8b695a098b0d9740a5d4a3b97f3-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana sebuah titik ditentukan sebagai titik yang bertetangga dalam dbscan clustering ', 'filters': None, 'id': '3115e82e1613457d5929095f4135fd4e', 'no_answer': False, 'answers': ['dua titik dianggap sebagai tetangga jika jarak di antara mereka kurang dari atau sama dengan eps'], 'offsets_in_documents': [{'start': 414, 'end': 510}], 'offsets_in_contexts': [{'start': 414, 'end': 510}], 'document_ids': ['30f0a8b695a098b0d9740a5d4a3b97f3-0'], 'contexts': ['algoritma ini mampu menemukan cluster dengan sembarang bentuk  tidak harus bentuk sphere  dan mampu menghandle noise dengan baik ide utama dbscan adalah bahwa suatu titik adalah milik suatu cluster jika terletak dekat dengan banyak titik dari cluster tersebut terdapat dua parameter utama dalam dbscan  yaitu parameter eps dan parameter minpts parameter eps adalah jarak yang menentukan ketetanggaan  neighborhood dua titik dianggap sebagai tetangga jika jarak di antara mereka kurang dari atau sama dengan eps']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b4305681-902e-401b-95ad-781eea0b80f7', 'query': 'apa yang menentukan sebuah titik menjadi titik boundary ', 'document': {'id': 'e8be337f14bf7c41d9c9c4a729d9a099-0', 'content': 'berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps titik diklasifikasikan sebagai boundary jika titik dapat dicapai dari core pints dan terdapat kurang dari minpts jumlah titik dalam daerah sekitarnya sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1083, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'titik diklasifikasikan sebagai boundary jika titik dapat dicapai dari core pints dan terdapat kurang dari minpts jumlah titik dalam daerah sekitarnya ', 'type': 'extractive', 'score': 0.0, 'context': 'berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps titik diklasifikasikan sebagai boundary jika titik dapat dicapai dari core pints dan terdapat kurang dari minpts jumlah titik dalam daerah sekitarnya sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda', 'offsets_in_document': [{'start': 224, 'end': 374}], 'offsets_in_context': [{'start': 224, 'end': 374}], 'document_ids': ['e8be337f14bf7c41d9c9c4a729d9a099-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang menentukan sebuah titik menjadi titik boundary ', 'filters': None, 'id': '88558fd73870221a51983a313ea0b90a', 'no_answer': False, 'answers': ['titik diklasifikasikan sebagai boundary jika titik dapat dicapai dari core pints dan terdapat kurang dari minpts jumlah titik dalam daerah sekitarnya '], 'offsets_in_documents': [{'start': 224, 'end': 374}], 'offsets_in_contexts': [{'start': 224, 'end': 374}], 'document_ids': ['e8be337f14bf7c41d9c9c4a729d9a099-0'], 'contexts': ['berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps titik diklasifikasikan sebagai boundary jika titik dapat dicapai dari core pints dan terdapat kurang dari minpts jumlah titik dalam daerah sekitarnya sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2eef2eff-db5a-4c18-864d-ff1eacd9fc50', 'query': 'mengapa suatu titik dianggap sebagai core points ', 'document': {'id': 'e8be337f14bf7c41d9c9c4a729d9a099-0', 'content': 'berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps titik diklasifikasikan sebagai boundary jika titik dapat dicapai dari core pints dan terdapat kurang dari minpts jumlah titik dalam daerah sekitarnya sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1083, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps', 'type': 'extractive', 'score': 0.0, 'context': 'berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps titik diklasifikasikan sebagai boundary jika titik dapat dicapai dari core pints dan terdapat kurang dari minpts jumlah titik dalam daerah sekitarnya sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda', 'offsets_in_document': [{'start': 91, 'end': 223}], 'offsets_in_context': [{'start': 91, 'end': 223}], 'document_ids': ['e8be337f14bf7c41d9c9c4a729d9a099-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa suatu titik dianggap sebagai core points ', 'filters': None, 'id': '5da537dd7f165db389ca0a2ee6b07b43', 'no_answer': False, 'answers': ['titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps'], 'offsets_in_documents': [{'start': 91, 'end': 223}], 'offsets_in_contexts': [{'start': 91, 'end': 223}], 'document_ids': ['e8be337f14bf7c41d9c9c4a729d9a099-0'], 'contexts': ['berdasarkan dua parameter ini  titik diklasifikasikan sebagai core  boundary  atau outlier titik adalah core point jika setidaknya ada sejumlah minpts titik  termasuk titik itu sendiri  di daerah sekitarnya dalam radius eps titik diklasifikasikan sebagai boundary jika titik dapat dicapai dari core pints dan terdapat kurang dari minpts jumlah titik dalam daerah sekitarnya sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'cc94ff23-4a51-4c4a-97c6-ff43f6c672a1', 'query': 'apa saja yang dijelaskan dalam beberapa fase dari sdlc ', 'document': {'id': '753cdb69dc4fd75f4b0b3bbcc9d653fe-0', 'content': 'sdlc menjelaskan tentang mengembangkan perangkat lunak yang digerakkan untuk solusi masalah bisnis sdlc disebut juga siklus hidup pengembangan solusi bisnis sdlc terdiri dari beberapa fase yang menjelaskan cara mengembangkan  memelihara  mengganti  dan meningkatkan perangkat lunak fase fase sdlc yaitu planning  analysis  design  build  test  dan deployment planning phase analisis kebutuhannya adalah fase yang paling penting dan mendasar dalam sdlc', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1091, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'sdlc menjelaskan tentang mengembangkan perangkat lunak yang digerakkan untuk solusi masalah bisnis ', 'type': 'extractive', 'score': 0.0, 'context': 'sdlc menjelaskan tentang mengembangkan perangkat lunak yang digerakkan untuk solusi masalah bisnis sdlc disebut juga siklus hidup pengembangan solusi bisnis sdlc terdiri dari beberapa fase yang menjelaskan cara mengembangkan  memelihara  mengganti  dan meningkatkan perangkat lunak fase fase sdlc yaitu planning  analysis  design  build  test  dan deployment planning phase analisis kebutuhannya adalah fase yang paling penting dan mendasar dalam sdlc', 'offsets_in_document': [{'start': 0, 'end': 99}], 'offsets_in_context': [{'start': 0, 'end': 99}], 'document_ids': ['753cdb69dc4fd75f4b0b3bbcc9d653fe-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja yang dijelaskan dalam beberapa fase dari sdlc ', 'filters': None, 'id': '6aaaaa12757b71a5ef7bd67115f12fe5', 'no_answer': False, 'answers': ['sdlc menjelaskan tentang mengembangkan perangkat lunak yang digerakkan untuk solusi masalah bisnis '], 'offsets_in_documents': [{'start': 0, 'end': 99}], 'offsets_in_contexts': [{'start': 0, 'end': 99}], 'document_ids': ['753cdb69dc4fd75f4b0b3bbcc9d653fe-0'], 'contexts': ['sdlc menjelaskan tentang mengembangkan perangkat lunak yang digerakkan untuk solusi masalah bisnis sdlc disebut juga siklus hidup pengembangan solusi bisnis sdlc terdiri dari beberapa fase yang menjelaskan cara mengembangkan  memelihara  mengganti  dan meningkatkan perangkat lunak fase fase sdlc yaitu planning  analysis  design  build  test  dan deployment planning phase analisis kebutuhannya adalah fase yang paling penting dan mendasar dalam sdlc']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c3b8a9c3-bdda-45d1-82d0-8b4842160a91', 'query': 'bagaimana cara kerja model v ', 'document': {'id': 'b56723c2e215ce845be2bb9ca70e9e8e-0', 'content': 'model berbentuk v merupakan perpanjangan model air terjun  metodologi sdlc ini menguji pada setiap tahap pengembangan seperti halnya air terjun  proses ini dapat mengalami hambatan model big bang berisiko tinggi membuang sebagian besar sumber dayanya dalam pengembangan dan bekerja paling baik untuk proyek proyek kecil ini tidak memiliki tahap definisi persyaratan menyeluruh dari metode lain model spiral yang paling fleksibel dari model sdlc', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1100, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'menguji pada setiap tahap pengembangan seperti halnya air terjun', 'type': 'extractive', 'score': 0.0, 'context': 'model berbentuk v merupakan perpanjangan model air terjun  metodologi sdlc ini menguji pada setiap tahap pengembangan seperti halnya air terjun  proses ini dapat mengalami hambatan model big bang berisiko tinggi membuang sebagian besar sumber dayanya dalam pengembangan dan bekerja paling baik untuk proyek proyek kecil ini tidak memiliki tahap definisi persyaratan menyeluruh dari metode lain model spiral yang paling fleksibel dari model sdlc', 'offsets_in_document': [{'start': 79, 'end': 143}], 'offsets_in_context': [{'start': 79, 'end': 143}], 'document_ids': ['b56723c2e215ce845be2bb9ca70e9e8e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara kerja model v ', 'filters': None, 'id': '1d2b39c63d5ccb790ccee38a73ec0688', 'no_answer': False, 'answers': ['menguji pada setiap tahap pengembangan seperti halnya air terjun'], 'offsets_in_documents': [{'start': 79, 'end': 143}], 'offsets_in_contexts': [{'start': 79, 'end': 143}], 'document_ids': ['b56723c2e215ce845be2bb9ca70e9e8e-0'], 'contexts': ['model berbentuk v merupakan perpanjangan model air terjun  metodologi sdlc ini menguji pada setiap tahap pengembangan seperti halnya air terjun  proses ini dapat mengalami hambatan model big bang berisiko tinggi membuang sebagian besar sumber dayanya dalam pengembangan dan bekerja paling baik untuk proyek proyek kecil ini tidak memiliki tahap definisi persyaratan menyeluruh dari metode lain model spiral yang paling fleksibel dari model sdlc']}>,\n",
              " <MultiLabel: {'labels': [{'id': '72ad021b-1739-4cc7-9f00-0c8e9124669c', 'query': 'bagaimana metode agile dalam sdlc membedakan diri dari model pengembangan perangkat lunak lainnya ', 'document': {'id': 'f4e8d37c4e11d6e4c5c07886c2a43da9-0', 'content': 'setiap fase memiliki mini plan sendiri dan setiap fase  air terjun  ke fase berikutnya kelemahan terbesar dari model ini adalah detail kecil yang tersisa tidak lengkap dapat menahan seluruh proses model agile sdlc memisahkan produk menjadi siklus dan memberikan produk yang berfungsi dengan sangat cepat metodologi ini menghasilkan suksesi rilis pengujian setiap rilis feed kembali info yang dimasukkan ke dalam versi berikutnya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1110, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'metodologi ini menghasilkan suksesi rilis pengujian setiap rilis feed kembali info yang dimasukkan ke dalam versi berikutnya', 'type': 'extractive', 'score': 0.0, 'context': 'setiap fase memiliki mini plan sendiri dan setiap fase  air terjun  ke fase berikutnya kelemahan terbesar dari model ini adalah detail kecil yang tersisa tidak lengkap dapat menahan seluruh proses model agile sdlc memisahkan produk menjadi siklus dan memberikan produk yang berfungsi dengan sangat cepat metodologi ini menghasilkan suksesi rilis pengujian setiap rilis feed kembali info yang dimasukkan ke dalam versi berikutnya', 'offsets_in_document': [{'start': 304, 'end': 428}], 'offsets_in_context': [{'start': 304, 'end': 428}], 'document_ids': ['f4e8d37c4e11d6e4c5c07886c2a43da9-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana metode agile dalam sdlc membedakan diri dari model pengembangan perangkat lunak lainnya ', 'filters': None, 'id': '6074fe528ef398b30d228376c269c5da', 'no_answer': False, 'answers': ['metodologi ini menghasilkan suksesi rilis pengujian setiap rilis feed kembali info yang dimasukkan ke dalam versi berikutnya'], 'offsets_in_documents': [{'start': 304, 'end': 428}], 'offsets_in_contexts': [{'start': 304, 'end': 428}], 'document_ids': ['f4e8d37c4e11d6e4c5c07886c2a43da9-0'], 'contexts': ['setiap fase memiliki mini plan sendiri dan setiap fase  air terjun  ke fase berikutnya kelemahan terbesar dari model ini adalah detail kecil yang tersisa tidak lengkap dapat menahan seluruh proses model agile sdlc memisahkan produk menjadi siklus dan memberikan produk yang berfungsi dengan sangat cepat metodologi ini menghasilkan suksesi rilis pengujian setiap rilis feed kembali info yang dimasukkan ke dalam versi berikutnya']}>,\n",
              " <MultiLabel: {'labels': [{'id': '808f370a-af22-43eb-8dac-b8552c35d869', 'query': 'apa operator boolean yang digunakan untuk menggambarkan sesuatu yang lebih kecil sama dengan ', 'document': {'id': '80c4652375d949dc55d661d6bc434b45-0', 'content': 'setiap operator boolean dapat digunakan untuk subset data    untuk lebih besar    lebih kecil     sama dengan     lebih besar sama dengan     lebih kecil sama dengan     tidak sama dengan kemudian untuk pustaka pada scikit learn menyediakan algoritma pembelajaran mesin seperti klasifikasi  regresi  pengelompokan  validasi model  dan lainnya serta pustaka scikit learn ini dibangun di atas numpy  scipy  dan matplotlib selanjutnya untuk pustaka python pada seaborn berdasarkan matplotlib  menyediakan antarmuka tingkat tinggi untuk menggambar grafik statistik yang menarik  serta mirip  dalam gaya  dengan perpustakaan ggplot2 yang populer dalam bahasa r', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1125, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'setiap operator boolean dapat digunakan untuk subset data    untuk lebih besar    lebih kecil     sama dengan     lebih besar sama dengan     lebih kecil sama dengan     tidak sama dengan', 'type': 'extractive', 'score': 0.0, 'context': 'setiap operator boolean dapat digunakan untuk subset data    untuk lebih besar    lebih kecil     sama dengan     lebih besar sama dengan     lebih kecil sama dengan     tidak sama dengan kemudian untuk pustaka pada scikit learn menyediakan algoritma pembelajaran mesin seperti klasifikasi  regresi  pengelompokan  validasi model  dan lainnya serta pustaka scikit learn ini dibangun di atas numpy  scipy  dan matplotlib selanjutnya untuk pustaka python pada seaborn berdasarkan matplotlib  menyediakan antarmuka tingkat tinggi untuk menggambar grafik statistik yang menarik  serta mirip  dalam gaya  dengan perpustakaan ggplot2 yang populer dalam bahasa r', 'offsets_in_document': [{'start': 0, 'end': 187}], 'offsets_in_context': [{'start': 0, 'end': 187}], 'document_ids': ['80c4652375d949dc55d661d6bc434b45-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa operator boolean yang digunakan untuk menggambarkan sesuatu yang lebih kecil sama dengan ', 'filters': None, 'id': 'cebeda7e37e1ae56609c801fadbd09bd', 'no_answer': False, 'answers': ['setiap operator boolean dapat digunakan untuk subset data    untuk lebih besar    lebih kecil     sama dengan     lebih besar sama dengan     lebih kecil sama dengan     tidak sama dengan'], 'offsets_in_documents': [{'start': 0, 'end': 187}], 'offsets_in_contexts': [{'start': 0, 'end': 187}], 'document_ids': ['80c4652375d949dc55d661d6bc434b45-0'], 'contexts': ['setiap operator boolean dapat digunakan untuk subset data    untuk lebih besar    lebih kecil     sama dengan     lebih besar sama dengan     lebih kecil sama dengan     tidak sama dengan kemudian untuk pustaka pada scikit learn menyediakan algoritma pembelajaran mesin seperti klasifikasi  regresi  pengelompokan  validasi model  dan lainnya serta pustaka scikit learn ini dibangun di atas numpy  scipy  dan matplotlib selanjutnya untuk pustaka python pada seaborn berdasarkan matplotlib  menyediakan antarmuka tingkat tinggi untuk menggambar grafik statistik yang menarik  serta mirip  dalam gaya  dengan perpustakaan ggplot2 yang populer dalam bahasa r']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3e76ab3a-3610-4465-8331-dad0a2de52ed', 'query': 'apa pustaka yang dibangun di atas numpy  scipy  dan matplotlib ', 'document': {'id': '80c4652375d949dc55d661d6bc434b45-0', 'content': 'setiap operator boolean dapat digunakan untuk subset data    untuk lebih besar    lebih kecil     sama dengan     lebih besar sama dengan     lebih kecil sama dengan     tidak sama dengan kemudian untuk pustaka pada scikit learn menyediakan algoritma pembelajaran mesin seperti klasifikasi  regresi  pengelompokan  validasi model  dan lainnya serta pustaka scikit learn ini dibangun di atas numpy  scipy  dan matplotlib selanjutnya untuk pustaka python pada seaborn berdasarkan matplotlib  menyediakan antarmuka tingkat tinggi untuk menggambar grafik statistik yang menarik  serta mirip  dalam gaya  dengan perpustakaan ggplot2 yang populer dalam bahasa r', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1125, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'pustaka scikit learn', 'type': 'extractive', 'score': 0.0, 'context': 'setiap operator boolean dapat digunakan untuk subset data    untuk lebih besar    lebih kecil     sama dengan     lebih besar sama dengan     lebih kecil sama dengan     tidak sama dengan kemudian untuk pustaka pada scikit learn menyediakan algoritma pembelajaran mesin seperti klasifikasi  regresi  pengelompokan  validasi model  dan lainnya serta pustaka scikit learn ini dibangun di atas numpy  scipy  dan matplotlib selanjutnya untuk pustaka python pada seaborn berdasarkan matplotlib  menyediakan antarmuka tingkat tinggi untuk menggambar grafik statistik yang menarik  serta mirip  dalam gaya  dengan perpustakaan ggplot2 yang populer dalam bahasa r', 'offsets_in_document': [{'start': 349, 'end': 369}], 'offsets_in_context': [{'start': 349, 'end': 369}], 'document_ids': ['80c4652375d949dc55d661d6bc434b45-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa pustaka yang dibangun di atas numpy  scipy  dan matplotlib ', 'filters': None, 'id': 'ccf135e5e99ede514bbad23a2a37d4e6', 'no_answer': False, 'answers': ['pustaka scikit learn'], 'offsets_in_documents': [{'start': 349, 'end': 369}], 'offsets_in_contexts': [{'start': 349, 'end': 369}], 'document_ids': ['80c4652375d949dc55d661d6bc434b45-0'], 'contexts': ['setiap operator boolean dapat digunakan untuk subset data    untuk lebih besar    lebih kecil     sama dengan     lebih besar sama dengan     lebih kecil sama dengan     tidak sama dengan kemudian untuk pustaka pada scikit learn menyediakan algoritma pembelajaran mesin seperti klasifikasi  regresi  pengelompokan  validasi model  dan lainnya serta pustaka scikit learn ini dibangun di atas numpy  scipy  dan matplotlib selanjutnya untuk pustaka python pada seaborn berdasarkan matplotlib  menyediakan antarmuka tingkat tinggi untuk menggambar grafik statistik yang menarik  serta mirip  dalam gaya  dengan perpustakaan ggplot2 yang populer dalam bahasa r']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3586fb1e-2158-4560-a102-a5c4680b15f7', 'query': 'apa nama method yang digunakan untuk menghapus kolom yang hanya berisi single value ', 'document': {'id': '2465fb2a1cf7434f31b2b8455d62af79-0', 'content': 'drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df nunique jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1130, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'df nunique', 'type': 'extractive', 'score': 0.0, 'context': 'drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df nunique jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik', 'offsets_in_document': [{'start': 230, 'end': 240}], 'offsets_in_context': [{'start': 230, 'end': 240}], 'document_ids': ['2465fb2a1cf7434f31b2b8455d62af79-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa nama method yang digunakan untuk menghapus kolom yang hanya berisi single value ', 'filters': None, 'id': '3cf2d19454317f80d919ae8ced80feb7', 'no_answer': False, 'answers': ['df nunique'], 'offsets_in_documents': [{'start': 230, 'end': 240}], 'offsets_in_contexts': [{'start': 230, 'end': 240}], 'document_ids': ['2465fb2a1cf7434f31b2b8455d62af79-0'], 'contexts': ['drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df nunique jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'ea0efd01-641c-445d-af81-c4d24e2cb8a4', 'query': 'apa fungsi dari penggunaan method df.nunique   ', 'document': {'id': '2465fb2a1cf7434f31b2b8455d62af79-0', 'content': 'drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df nunique jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1130, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  ', 'type': 'extractive', 'score': 0.0, 'context': 'drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df nunique jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik', 'offsets_in_document': [{'start': 131, 'end': 211}], 'offsets_in_context': [{'start': 131, 'end': 211}], 'document_ids': ['2465fb2a1cf7434f31b2b8455d62af79-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa fungsi dari penggunaan method df.nunique   ', 'filters': None, 'id': 'e15e3e6cde2a759c8c1ba81445ae724d', 'no_answer': False, 'answers': ['mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  '], 'offsets_in_documents': [{'start': 131, 'end': 211}], 'offsets_in_contexts': [{'start': 131, 'end': 211}], 'document_ids': ['2465fb2a1cf7434f31b2b8455d62af79-0'], 'contexts': ['drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df nunique jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f6828aff-2042-4de0-a78f-c9894442cf22', 'query': 'apa yang dilakukan pada model spiral?', 'document': {'id': '78b495e077c162aa0ab4648f8275f62b-0', 'content': 'model spiral melewati fase perencanaan  desain  pembangunan dan pengujian berulang ulang  dengan perbaikan bertahap pada setiap lintasan solusi berbasis perangkat lunak terdiri dari dua bagian yaitu model dan sistem model memiliki terdiri dari prototipe dan diagram dan dokumen pendukung sistem terdiri dari perangkat keras dan perangkat lunak arsitektur data kecil dan data besar yang khas untuk sains data adalah suatu struktur atau rancangan yang digunakan untuk mengatur dan mengelola data dalam proyek sains data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1114, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'model spiral melewati fase perencanaan  desain  pembangunan dan pengujian berulang ulang  dengan perbaikan bertahap pada setiap lintasan', 'type': 'extractive', 'score': 0.0, 'context': 'model spiral melewati fase perencanaan  desain  pembangunan dan pengujian berulang ulang  dengan perbaikan bertahap pada setiap lintasan solusi berbasis perangkat lunak terdiri dari dua bagian yaitu model dan sistem model memiliki terdiri dari prototipe dan diagram dan dokumen pendukung sistem terdiri dari perangkat keras dan perangkat lunak arsitektur data kecil dan data besar yang khas untuk sains data adalah suatu struktur atau rancangan yang digunakan untuk mengatur dan mengelola data dalam proyek sains data', 'offsets_in_document': [{'start': 0, 'end': 136}], 'offsets_in_context': [{'start': 0, 'end': 136}], 'document_ids': ['78b495e077c162aa0ab4648f8275f62b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dilakukan pada model spiral?', 'filters': None, 'id': 'e010aa8c80d884b416a5b68f6a8710fa', 'no_answer': False, 'answers': ['model spiral melewati fase perencanaan  desain  pembangunan dan pengujian berulang ulang  dengan perbaikan bertahap pada setiap lintasan'], 'offsets_in_documents': [{'start': 0, 'end': 136}], 'offsets_in_contexts': [{'start': 0, 'end': 136}], 'document_ids': ['78b495e077c162aa0ab4648f8275f62b-0'], 'contexts': ['model spiral melewati fase perencanaan  desain  pembangunan dan pengujian berulang ulang  dengan perbaikan bertahap pada setiap lintasan solusi berbasis perangkat lunak terdiri dari dua bagian yaitu model dan sistem model memiliki terdiri dari prototipe dan diagram dan dokumen pendukung sistem terdiri dari perangkat keras dan perangkat lunak arsitektur data kecil dan data besar yang khas untuk sains data adalah suatu struktur atau rancangan yang digunakan untuk mengatur dan mengelola data dalam proyek sains data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '598c4bbf-71a6-41b3-9f05-0c4157b734a2', 'query': 'mengapa suatu titik dianggap sebagai outlier ', 'document': {'id': '8d37e96e15a66bc72585441cdb2f6f2e-0', 'content': 'sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda interpretasi hasil clustering dibutuhkan jika semisal cluster memiliki nilai evaluasi sejumlah 10  apakah cluster yang dihasilkan sudah bagus atau belum cluster yang dihasilkan dinilai valid atau tidak yaitu untuk melihat apakah benar benar ada struktur dalam data yang telah dikelompokkan hasil clustering dibandingkan  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda  untuk membandingkan dua atau lebih analisa clustering', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1086, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun', 'type': 'extractive', 'score': 0.0, 'context': 'sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda interpretasi hasil clustering dibutuhkan jika semisal cluster memiliki nilai evaluasi sejumlah 10  apakah cluster yang dihasilkan sudah bagus atau belum cluster yang dihasilkan dinilai valid atau tidak yaitu untuk melihat apakah benar benar ada struktur dalam data yang telah dikelompokkan hasil clustering dibandingkan  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda  untuk membandingkan dua atau lebih analisa clustering', 'offsets_in_document': [{'start': 0, 'end': 109}], 'offsets_in_context': [{'start': 0, 'end': 109}], 'document_ids': ['8d37e96e15a66bc72585441cdb2f6f2e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa suatu titik dianggap sebagai outlier ', 'filters': None, 'id': 'a872bbbc43cd220bccc854a8db187aa8', 'no_answer': False, 'answers': ['sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun'], 'offsets_in_documents': [{'start': 0, 'end': 109}], 'offsets_in_contexts': [{'start': 0, 'end': 109}], 'document_ids': ['8d37e96e15a66bc72585441cdb2f6f2e-0'], 'contexts': ['sebuah titik adalah outlier jika titik tersebut bukan titik inti dan tidak dapat dijangkau dari titik manapun evaluasi pada clustering dibutuhkan untuk beberapa alasan seperti interpretasi hasil clustering  menilai validitas cluster yang dihasilkan  serta membandingkan hasil clustering  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda interpretasi hasil clustering dibutuhkan jika semisal cluster memiliki nilai evaluasi sejumlah 10  apakah cluster yang dihasilkan sudah bagus atau belum cluster yang dihasilkan dinilai valid atau tidak yaitu untuk melihat apakah benar benar ada struktur dalam data yang telah dikelompokkan hasil clustering dibandingkan  baik dengan algoritma yang sama ataupun dengan algoritma yang berbeda  untuk membandingkan dua atau lebih analisa clustering']}>,\n",
              " <MultiLabel: {'labels': [{'id': '7df6c26f-5ea6-4041-85fb-35440f5978f7', 'query': 'mengapa planning phase merupakan fase yang penting dalam sdlc ', 'document': {'id': '1b21114e8a1d444cbb268b9a06e528a0-0', 'content': 'fase fase sdlc yaitu planning  analysis  design  build  test  dan deployment planning phase analisis kebutuhannya adalah fase yang paling penting dan mendasar dalam sdlc ini dilakukan oleh anggota senior tim dengan masukan dari pelanggan  departemen penjualan  survei pasar dan ahli domain di industri informasi ini kemudian digunakan untuk merencanakan pendekatan proyek dasar dan untuk melakukan studi kelayakan produk di bidang ekonomi  operasional dan teknis perencanaan untuk persyaratan jaminan kualitas dan identifikasi risiko yang terkait dengan proyek juga dilakukan dalam tahap perencanaan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1092, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'merencanakan pendekatan proyek dasar dan untuk melakukan studi kelayakan produk ', 'type': 'extractive', 'score': 0.0, 'context': 'fase fase sdlc yaitu planning  analysis  design  build  test  dan deployment planning phase analisis kebutuhannya adalah fase yang paling penting dan mendasar dalam sdlc ini dilakukan oleh anggota senior tim dengan masukan dari pelanggan  departemen penjualan  survei pasar dan ahli domain di industri informasi ini kemudian digunakan untuk merencanakan pendekatan proyek dasar dan untuk melakukan studi kelayakan produk di bidang ekonomi  operasional dan teknis perencanaan untuk persyaratan jaminan kualitas dan identifikasi risiko yang terkait dengan proyek juga dilakukan dalam tahap perencanaan', 'offsets_in_document': [{'start': 341, 'end': 421}], 'offsets_in_context': [{'start': 341, 'end': 421}], 'document_ids': ['1b21114e8a1d444cbb268b9a06e528a0-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa planning phase merupakan fase yang penting dalam sdlc ', 'filters': None, 'id': '58bc031e148ca6eaa6f2ee9c8c610f15', 'no_answer': False, 'answers': ['merencanakan pendekatan proyek dasar dan untuk melakukan studi kelayakan produk '], 'offsets_in_documents': [{'start': 341, 'end': 421}], 'offsets_in_contexts': [{'start': 341, 'end': 421}], 'document_ids': ['1b21114e8a1d444cbb268b9a06e528a0-0'], 'contexts': ['fase fase sdlc yaitu planning  analysis  design  build  test  dan deployment planning phase analisis kebutuhannya adalah fase yang paling penting dan mendasar dalam sdlc ini dilakukan oleh anggota senior tim dengan masukan dari pelanggan  departemen penjualan  survei pasar dan ahli domain di industri informasi ini kemudian digunakan untuk merencanakan pendekatan proyek dasar dan untuk melakukan studi kelayakan produk di bidang ekonomi  operasional dan teknis perencanaan untuk persyaratan jaminan kualitas dan identifikasi risiko yang terkait dengan proyek juga dilakukan dalam tahap perencanaan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0e1c8c46-a7b1-40de-986e-14be8752af3d', 'query': 'apa input dari test phase ', 'document': {'id': '30e8dd820c55771273004984d13e9ebc-0', 'content': 'jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1097, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'program   perangkat lunak yang belum diuji', 'type': 'extractive', 'score': 0.0, 'context': 'jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji', 'offsets_in_document': [{'start': 344, 'end': 386}], 'offsets_in_context': [{'start': 344, 'end': 386}], 'document_ids': ['30e8dd820c55771273004984d13e9ebc-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa input dari test phase ', 'filters': None, 'id': '427c715ae5a14aea5c92c4d7bc04b342', 'no_answer': False, 'answers': ['program   perangkat lunak yang belum diuji'], 'offsets_in_documents': [{'start': 344, 'end': 386}], 'offsets_in_contexts': [{'start': 344, 'end': 386}], 'document_ids': ['30e8dd820c55771273004984d13e9ebc-0'], 'contexts': ['jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji']}>,\n",
              " <MultiLabel: {'labels': [{'id': '4d7c036f-9be9-44d5-8210-b4f32b2f3ccd', 'query': 'apa yang harus diikuti oleh pengembang dalam build phase ', 'document': {'id': '30e8dd820c55771273004984d13e9ebc-0', 'content': 'jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1097, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'pedoman pengkodean yang ditentukan oleh organisasi mereka ', 'type': 'extractive', 'score': 0.0, 'context': 'jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji', 'offsets_in_document': [{'start': 132, 'end': 190}], 'offsets_in_context': [{'start': 132, 'end': 190}], 'document_ids': ['30e8dd820c55771273004984d13e9ebc-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang harus diikuti oleh pengembang dalam build phase ', 'filters': None, 'id': 'de5a81937d7811c5672d40b57bf49c11', 'no_answer': False, 'answers': ['pedoman pengkodean yang ditentukan oleh organisasi mereka '], 'offsets_in_documents': [{'start': 132, 'end': 190}], 'offsets_in_contexts': [{'start': 132, 'end': 190}], 'document_ids': ['30e8dd820c55771273004984d13e9ebc-0'], 'contexts': ['jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e66ceb4d-48aa-4bff-8e2e-0172dac87a9a', 'query': 'apa tujuan menggunakan alat pemograman ', 'document': {'id': '30e8dd820c55771273004984d13e9ebc-0', 'content': 'jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1097, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'untuk menghasilkan kode ', 'type': 'extractive', 'score': 0.0, 'context': 'jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji', 'offsets_in_document': [{'start': 266, 'end': 290}], 'offsets_in_context': [{'start': 266, 'end': 290}], 'document_ids': ['30e8dd820c55771273004984d13e9ebc-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa tujuan menggunakan alat pemograman ', 'filters': None, 'id': '576413b3cf28438a35d6db1d95267e7b', 'no_answer': False, 'answers': ['untuk menghasilkan kode '], 'offsets_in_documents': [{'start': 266, 'end': 290}], 'offsets_in_contexts': [{'start': 266, 'end': 290}], 'document_ids': ['30e8dd820c55771273004984d13e9ebc-0'], 'contexts': ['jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode outputnya berupa program software test phase inputnya program   perangkat lunak yang belum diuji  uji dokumen rencana fase ini mengacu pada tahap pengujian hanya produk di mana cacat produk dilaporkan  dilacak  diperbaiki dan diuji ulang  sampai produk mencapai standar kualitas yang ditentukan dalam srs outputnya berupa program  perangkat lunak yang diuji']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2fdd17cc-5360-40cb-a88a-c010c216788f', 'query': 'apa pustaka yang menyediakan alat untuk manipulasi data seperti menyortir dan agregasi ', 'document': {'id': 'f7c70790c938ad6704868f44086133b6-0', 'content': 'untuk melihat fungsi pustaka scipy dengan import scipy dan help scipy scipy memiliki jenis scipy linear algebra   scipy linalg  scipy curve fitting   scipy curve_fit  dan scipy integral   scipy integrate kemudian untuk pustaka panda dapat menambahkan struktur dan alat data yang dirancang untuk bekerja dengan data seperti tabel  mirip dengan series dan data frame dalam r   menyediakan alat untuk manipulasi data seperti membentuk kembali  menggabungkan  menyortir  mengiris  agregasi  dan lainnya  pandas juga memungkinkan penanganan data yang hilang dalam pandas juga dapat membaca data dan mendeskripsikan data untuk membaca data dengan pandas terdapat sejumlah perintah panda untuk membaca format data lainnya seperti pd', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1122, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'pustaka panda', 'type': 'extractive', 'score': 0.0, 'context': 'untuk melihat fungsi pustaka scipy dengan import scipy dan help scipy scipy memiliki jenis scipy linear algebra   scipy linalg  scipy curve fitting   scipy curve_fit  dan scipy integral   scipy integrate kemudian untuk pustaka panda dapat menambahkan struktur dan alat data yang dirancang untuk bekerja dengan data seperti tabel  mirip dengan series dan data frame dalam r   menyediakan alat untuk manipulasi data seperti membentuk kembali  menggabungkan  menyortir  mengiris  agregasi  dan lainnya  pandas juga memungkinkan penanganan data yang hilang dalam pandas juga dapat membaca data dan mendeskripsikan data untuk membaca data dengan pandas terdapat sejumlah perintah panda untuk membaca format data lainnya seperti pd', 'offsets_in_document': [{'start': 219, 'end': 232}], 'offsets_in_context': [{'start': 219, 'end': 232}], 'document_ids': ['f7c70790c938ad6704868f44086133b6-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa pustaka yang menyediakan alat untuk manipulasi data seperti menyortir dan agregasi ', 'filters': None, 'id': 'e62e4f9a90c0b9e1690503c318b7ed34', 'no_answer': False, 'answers': ['pustaka panda'], 'offsets_in_documents': [{'start': 219, 'end': 232}], 'offsets_in_contexts': [{'start': 219, 'end': 232}], 'document_ids': ['f7c70790c938ad6704868f44086133b6-0'], 'contexts': ['untuk melihat fungsi pustaka scipy dengan import scipy dan help scipy scipy memiliki jenis scipy linear algebra   scipy linalg  scipy curve fitting   scipy curve_fit  dan scipy integral   scipy integrate kemudian untuk pustaka panda dapat menambahkan struktur dan alat data yang dirancang untuk bekerja dengan data seperti tabel  mirip dengan series dan data frame dalam r   menyediakan alat untuk manipulasi data seperti membentuk kembali  menggabungkan  menyortir  mengiris  agregasi  dan lainnya  pandas juga memungkinkan penanganan data yang hilang dalam pandas juga dapat membaca data dan mendeskripsikan data untuk membaca data dengan pandas terdapat sejumlah perintah panda untuk membaca format data lainnya seperti pd']}>,\n",
              " <MultiLabel: {'labels': [{'id': '9ff6ebaf-6476-438c-934c-bdd6770c0031', 'query': 'bagaimana peran metodologi crisp dm dalam proyek data mining ', 'document': {'id': '794bf66c4de77f567a7ee3302ec1020a-0', 'content': ' metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1090, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data', 'type': 'extractive', 'score': 0.0, 'context': ' metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data', 'offsets_in_document': [{'start': 571, 'end': 661}], 'offsets_in_context': [{'start': 571, 'end': 661}], 'document_ids': ['794bf66c4de77f567a7ee3302ec1020a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana peran metodologi crisp dm dalam proyek data mining ', 'filters': None, 'id': '6136e5e03bdaf5fbdf903bc5ebdb5ac5', 'no_answer': False, 'answers': ['dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data'], 'offsets_in_documents': [{'start': 571, 'end': 661}], 'offsets_in_contexts': [{'start': 571, 'end': 661}], 'document_ids': ['794bf66c4de77f567a7ee3302ec1020a-0'], 'contexts': [' metodologi dasar sains data adalah langkah langkah yang digunakan dalam proyek data science agar dapat menghasilkan hasil yang optimal dapat menjawab pertanyaan dari suatu masalah yang ingin diselesaikan metodologi ini tidak tergantung pada teknologi atau tools tertentu dalam sains data  terdapat dua metodologi dasar yang sering digunakan  yaitu cross industry standard process for data mining  crisp dm  dan software development life cycle  sdlc crisp dm secara teratur berada di tempat nomor satu dalam berbagai survei industri keuntungan utama dari crisp dm adalah dirancang untuk menjadi independen dari perangkat lunak  vendor  atau teknik analisis data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '9f8e9ba7-0c05-4caa-8868-48f87046cfb5', 'query': 'apa output dari deployment phase ', 'document': {'id': 'd6686b06e1cb7f288626baed940accb2-0', 'content': 'kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada outputnya berupa program   perangkat lunak yang diuji bermigrasi ke produksi dan merayakannya dalam \\nsdlc terdapat 9 model yang dapat digunakan  seperti model waterfall  agile  iterative  v shaped  big bang  spiral  rad  rapid application development dan model prototyping model waterfall adalah model tertua dan paling mudah', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1096, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'program   perangkat lunak yang diuji bermigrasi ke produksi dan merayakannya', 'type': 'extractive', 'score': 0.0, 'context': 'kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada outputnya berupa program   perangkat lunak yang diuji bermigrasi ke produksi dan merayakannya dalam \\nsdlc terdapat 9 model yang dapat digunakan  seperti model waterfall  agile  iterative  v shaped  big bang  spiral  rad  rapid application development dan model prototyping model waterfall adalah model tertua dan paling mudah', 'offsets_in_document': [{'start': 237, 'end': 313}], 'offsets_in_context': [{'start': 237, 'end': 313}], 'document_ids': ['d6686b06e1cb7f288626baed940accb2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa output dari deployment phase ', 'filters': None, 'id': '4226e24577e79374aca82db02ce1d1a6', 'no_answer': False, 'answers': ['program   perangkat lunak yang diuji bermigrasi ke produksi dan merayakannya'], 'offsets_in_documents': [{'start': 237, 'end': 313}], 'offsets_in_contexts': [{'start': 237, 'end': 313}], 'document_ids': ['d6686b06e1cb7f288626baed940accb2-0'], 'contexts': ['kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada outputnya berupa program   perangkat lunak yang diuji bermigrasi ke produksi dan merayakannya dalam \\nsdlc terdapat 9 model yang dapat digunakan  seperti model waterfall  agile  iterative  v shaped  big bang  spiral  rad  rapid application development dan model prototyping model waterfall adalah model tertua dan paling mudah']}>,\n",
              " <MultiLabel: {'labels': [{'id': '73368205-d7d0-4bdc-94a8-c2ede58328d2', 'query': 'berapa model yang terdapat pada sdlc', 'document': {'id': 'd6686b06e1cb7f288626baed940accb2-0', 'content': 'kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada outputnya berupa program   perangkat lunak yang diuji bermigrasi ke produksi dan merayakannya dalam \\nsdlc terdapat 9 model yang dapat digunakan  seperti model waterfall  agile  iterative  v shaped  big bang  spiral  rad  rapid application development dan model prototyping model waterfall adalah model tertua dan paling mudah', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1096, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' 9 model ', 'type': 'extractive', 'score': 0.0, 'context': 'kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada outputnya berupa program   perangkat lunak yang diuji bermigrasi ke produksi dan merayakannya dalam \\nsdlc terdapat 9 model yang dapat digunakan  seperti model waterfall  agile  iterative  v shaped  big bang  spiral  rad  rapid application development dan model prototyping model waterfall adalah model tertua dan paling mudah', 'offsets_in_document': [{'start': 334, 'end': 343}], 'offsets_in_context': [{'start': 334, 'end': 343}], 'document_ids': ['d6686b06e1cb7f288626baed940accb2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'berapa model yang terdapat pada sdlc', 'filters': None, 'id': '8ec2564328dba4b595ef463688da773c', 'no_answer': False, 'answers': [' 9 model '], 'offsets_in_documents': [{'start': 334, 'end': 343}], 'offsets_in_contexts': [{'start': 334, 'end': 343}], 'document_ids': ['d6686b06e1cb7f288626baed940accb2-0'], 'contexts': ['kemudian berdasarkan umpan balik  produk dapat dirilis apa itu atau dengan peningkatan yang disarankan di segmen pasar penargetan setelah produk dirilis di pasar  pemeliharaannya dilakukan untuk basis pelanggan yang ada outputnya berupa program   perangkat lunak yang diuji bermigrasi ke produksi dan merayakannya dalam \\nsdlc terdapat 9 model yang dapat digunakan  seperti model waterfall  agile  iterative  v shaped  big bang  spiral  rad  rapid application development dan model prototyping model waterfall adalah model tertua dan paling mudah']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3baf265a-00b0-49f0-a92a-3ca279849932', 'query': 'apa itu fungsi range dalam python ', 'document': {'id': '840a424cacb26fbb4b62be09e188a30a-0', 'content': 'kemudian untuk fungsi range adalah objek yang dapat dilinyalir  tetapi tidak identik di mana fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan ketika diperlukan sehingga disebut lazy iterator terdapat pula files input yang terdiri dari inflobj   open  data    r   untuk membuka file  data  untuk input  kemudian s   inflobj read   untuk membaca keseluruhan file pada satu string  lalu s   inflobj read n  untuk membaca n bytes  n    1   kemudian l   inflobj readline   untuk membaca satu line  dan l   inflobj', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1116, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' fungsi range adalah objek yang dapat dilinyalir  tetapi tidak identik di mana fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan ketika diperlukan ', 'type': 'extractive', 'score': 0.0, 'context': 'kemudian untuk fungsi range adalah objek yang dapat dilinyalir  tetapi tidak identik di mana fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan ketika diperlukan sehingga disebut lazy iterator terdapat pula files input yang terdiri dari inflobj   open  data    r   untuk membuka file  data  untuk input  kemudian s   inflobj read   untuk membaca keseluruhan file pada satu string  lalu s   inflobj read n  untuk membaca n bytes  n    1   kemudian l   inflobj readline   untuk membaca satu line  dan l   inflobj', 'offsets_in_document': [{'start': 14, 'end': 209}], 'offsets_in_context': [{'start': 14, 'end': 209}], 'document_ids': ['840a424cacb26fbb4b62be09e188a30a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu fungsi range dalam python ', 'filters': None, 'id': '70829b06c4c0a42f73477731ba752176', 'no_answer': False, 'answers': [' fungsi range adalah objek yang dapat dilinyalir  tetapi tidak identik di mana fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan ketika diperlukan '], 'offsets_in_documents': [{'start': 14, 'end': 209}], 'offsets_in_contexts': [{'start': 14, 'end': 209}], 'document_ids': ['840a424cacb26fbb4b62be09e188a30a-0'], 'contexts': ['kemudian untuk fungsi range adalah objek yang dapat dilinyalir  tetapi tidak identik di mana fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan ketika diperlukan sehingga disebut lazy iterator terdapat pula files input yang terdiri dari inflobj   open  data    r   untuk membuka file  data  untuk input  kemudian s   inflobj read   untuk membaca keseluruhan file pada satu string  lalu s   inflobj read n  untuk membaca n bytes  n    1   kemudian l   inflobj readline   untuk membaca satu line  dan l   inflobj']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'fa793120-4d23-4e8d-aaf4-e1c55d5a057d', 'query': 'apa yang dihasilkan dan tidak dihasilkan dari fungsi range ', 'document': {'id': '840a424cacb26fbb4b62be09e188a30a-0', 'content': 'kemudian untuk fungsi range adalah objek yang dapat dilinyalir  tetapi tidak identik di mana fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan ketika diperlukan sehingga disebut lazy iterator terdapat pula files input yang terdiri dari inflobj   open  data    r   untuk membuka file  data  untuk input  kemudian s   inflobj read   untuk membaca keseluruhan file pada satu string  lalu s   inflobj read n  untuk membaca n bytes  n    1   kemudian l   inflobj readline   untuk membaca satu line  dan l   inflobj', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1116, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan', 'type': 'extractive', 'score': 0.0, 'context': 'kemudian untuk fungsi range adalah objek yang dapat dilinyalir  tetapi tidak identik di mana fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan ketika diperlukan sehingga disebut lazy iterator terdapat pula files input yang terdiri dari inflobj   open  data    r   untuk membuka file  data  untuk input  kemudian s   inflobj read   untuk membaca keseluruhan file pada satu string  lalu s   inflobj read n  untuk membaca n bytes  n    1   kemudian l   inflobj readline   untuk membaca satu line  dan l   inflobj', 'offsets_in_document': [{'start': 92, 'end': 190}], 'offsets_in_context': [{'start': 92, 'end': 190}], 'document_ids': ['840a424cacb26fbb4b62be09e188a30a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dihasilkan dan tidak dihasilkan dari fungsi range ', 'filters': None, 'id': 'c39512f5466ea7498cc2d69e45833e02', 'no_answer': False, 'answers': [' fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan'], 'offsets_in_documents': [{'start': 92, 'end': 190}], 'offsets_in_contexts': [{'start': 92, 'end': 190}], 'document_ids': ['840a424cacb26fbb4b62be09e188a30a-0'], 'contexts': ['kemudian untuk fungsi range adalah objek yang dapat dilinyalir  tetapi tidak identik di mana fungsi range tidak menghasilkan semua nilai sekaligus  tetapi menghasilkan nilai secara berurutan ketika diperlukan sehingga disebut lazy iterator terdapat pula files input yang terdiri dari inflobj   open  data    r   untuk membuka file  data  untuk input  kemudian s   inflobj read   untuk membaca keseluruhan file pada satu string  lalu s   inflobj read n  untuk membaca n bytes  n    1   kemudian l   inflobj readline   untuk membaca satu line  dan l   inflobj']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'a7cd0509-bb2f-4eda-93f8-44a21527249f', 'query': 'apakah mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya merupakan hal yang penting dalam business understanding ', 'document': {'id': '48f5412cf30205b9e0b6cdd8cc8c8840-0', 'content': 'tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1131, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya ', 'type': 'extractive', 'score': 0.0, 'context': 'tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator', 'offsets_in_document': [{'start': 180, 'end': 284}], 'offsets_in_context': [{'start': 180, 'end': 284}], 'document_ids': ['48f5412cf30205b9e0b6cdd8cc8c8840-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apakah mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya merupakan hal yang penting dalam business understanding ', 'filters': None, 'id': '33865fff2651ed705345db3ab8f9ae65', 'no_answer': False, 'answers': ['penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya '], 'offsets_in_documents': [{'start': 180, 'end': 284}], 'offsets_in_contexts': [{'start': 180, 'end': 284}], 'document_ids': ['48f5412cf30205b9e0b6cdd8cc8c8840-0'], 'contexts': ['tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator']}>,\n",
              " <MultiLabel: {'labels': [{'id': '14a2f26f-d2ac-4e21-ae75-1626ef2d9371', 'query': 'apakah masalah dan solusi termasuk dalam tahapan desain alur sistem ', 'document': {'id': '48f5412cf30205b9e0b6cdd8cc8c8840-0', 'content': 'tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1131, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi', 'type': 'extractive', 'score': 0.0, 'context': 'tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator', 'offsets_in_document': [{'start': 284, 'end': 397}], 'offsets_in_context': [{'start': 284, 'end': 397}], 'document_ids': ['48f5412cf30205b9e0b6cdd8cc8c8840-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apakah masalah dan solusi termasuk dalam tahapan desain alur sistem ', 'filters': None, 'id': '69ee41bdd180ac249dba71207e2fc55c', 'no_answer': False, 'answers': ['tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi'], 'offsets_in_documents': [{'start': 284, 'end': 397}], 'offsets_in_contexts': [{'start': 284, 'end': 397}], 'document_ids': ['48f5412cf30205b9e0b6cdd8cc8c8840-0'], 'contexts': ['tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator']}>,\n",
              " <MultiLabel: {'labels': [{'id': '5b335749-da53-4030-8a2e-c263022f4914', 'query': 'bagaimana proses tradisional dolakukan dalam dasar sains data ', 'document': {'id': 'c16fb0e340a9ba52cc01c19b9af9b039-0', 'content': 'arsitektur data dirancang untuk memastikan data dapat diakses  disimpan  dan dianalisis dengan efisien dan efektif proses tradisional untuk membangun model prediktif dan mencetak data melibatkan beberapa langkah pertama  data harus diekstraksi dari sumber yang relevan kemudian  data tersebut harus diintegrasikan dan dipersiapkan agar siap digunakan setelah itu  model model dibangun berdasarkan data yang telah dipersiapkan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1113, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data harus diekstraksi dari sumber yang relevan kemudian  data tersebut harus diintegrasikan dan dipersiapkan agar siap digunakan setelah itu  model model dibangun berdasarkan data yang telah dipersiapkan', 'type': 'extractive', 'score': 0.0, 'context': 'arsitektur data dirancang untuk memastikan data dapat diakses  disimpan  dan dianalisis dengan efisien dan efektif proses tradisional untuk membangun model prediktif dan mencetak data melibatkan beberapa langkah pertama  data harus diekstraksi dari sumber yang relevan kemudian  data tersebut harus diintegrasikan dan dipersiapkan agar siap digunakan setelah itu  model model dibangun berdasarkan data yang telah dipersiapkan', 'offsets_in_document': [{'start': 221, 'end': 425}], 'offsets_in_context': [{'start': 221, 'end': 425}], 'document_ids': ['c16fb0e340a9ba52cc01c19b9af9b039-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana proses tradisional dolakukan dalam dasar sains data ', 'filters': None, 'id': 'd7ac23d2d170d19b7dc3a18a25b68f63', 'no_answer': False, 'answers': ['data harus diekstraksi dari sumber yang relevan kemudian  data tersebut harus diintegrasikan dan dipersiapkan agar siap digunakan setelah itu  model model dibangun berdasarkan data yang telah dipersiapkan'], 'offsets_in_documents': [{'start': 221, 'end': 425}], 'offsets_in_contexts': [{'start': 221, 'end': 425}], 'document_ids': ['c16fb0e340a9ba52cc01c19b9af9b039-0'], 'contexts': ['arsitektur data dirancang untuk memastikan data dapat diakses  disimpan  dan dianalisis dengan efisien dan efektif proses tradisional untuk membangun model prediktif dan mencetak data melibatkan beberapa langkah pertama  data harus diekstraksi dari sumber yang relevan kemudian  data tersebut harus diintegrasikan dan dipersiapkan agar siap digunakan setelah itu  model model dibangun berdasarkan data yang telah dipersiapkan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '46cac613-a602-4fbd-b8a1-3c958904d527', 'query': 'apakah prediksi termasuk dalam deskripsi tipe masalah ', 'document': {'id': '7c674fd4749206807ba6b4e3693ecc45-0', 'content': ' c mengidentifikasi resiko dan menentukan rencana antisipasi tiap resiko \\n jadwal  bagaimana jika jadwal mundur dari rencana semula  \\n keuangan  bagaimana jika sponsor project mengalami masalah anggaran  \\n data  bagaimana jika kualitas data jelek \\n hasil  bagaimana jika hasil tidak seperti yang diharapkan  \\nd membuat terminology istilah \\ntujuannya satu tim punya pemahaman istilah yang sama \\ne mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1104, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi ', 'type': 'extractive', 'score': 0.0, 'context': ' c mengidentifikasi resiko dan menentukan rencana antisipasi tiap resiko \\n jadwal  bagaimana jika jadwal mundur dari rencana semula  \\n keuangan  bagaimana jika sponsor project mengalami masalah anggaran  \\n data  bagaimana jika kualitas data jelek \\n hasil  bagaimana jika hasil tidak seperti yang diharapkan  \\nd membuat terminology istilah \\ntujuannya satu tim punya pemahaman istilah yang sama \\ne mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang', 'offsets_in_document': [{'start': 509, 'end': 584}], 'offsets_in_context': [{'start': 509, 'end': 584}], 'document_ids': ['7c674fd4749206807ba6b4e3693ecc45-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apakah prediksi termasuk dalam deskripsi tipe masalah ', 'filters': None, 'id': 'fb424e33dfad788af8b7534cd50fbd6d', 'no_answer': False, 'answers': ['mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi '], 'offsets_in_documents': [{'start': 509, 'end': 584}], 'offsets_in_contexts': [{'start': 509, 'end': 584}], 'document_ids': ['7c674fd4749206807ba6b4e3693ecc45-0'], 'contexts': [' c mengidentifikasi resiko dan menentukan rencana antisipasi tiap resiko \\n jadwal  bagaimana jika jadwal mundur dari rencana semula  \\n keuangan  bagaimana jika sponsor project mengalami masalah anggaran  \\n data  bagaimana jika kualitas data jelek \\n hasil  bagaimana jika hasil tidak seperti yang diharapkan  \\nd membuat terminology istilah \\ntujuannya satu tim punya pemahaman istilah yang sama \\ne mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang']}>,\n",
              " <MultiLabel: {'labels': [{'id': '18efd3a4-29a9-4613-b0b5-77119ecf214c', 'query': 'apa tahap yang dilakukan setelah analisis peryaratan pada analysis phase dalam sdlc ', 'document': {'id': '6665502ba0b18d0f06fdec7e7275daef-0', 'content': 'outputnya studi kelayakan teknis   inisiasi proyek analysis phase inputnya studi kelayakan teknis dan pendanaan analisis proyek dilakukan setelah analisis persyaratan dilakukan  tahap selanjutnya adalah mendefinisikan dan mendokumentasikan persyaratan produk dengan jelas dan membuatnya disetujui dari pelanggan atau analis pasar hal ini dilakukan melalui dokumen srs  software requirement specification  yang terdiri dari semua persyaratan produk yang akan dirancang dan dikembangkan selama siklus hidup proyek output berupa srs  spesifikasi persyaratan perangkat lunak', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1107, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'tahap selanjutnya adalah mendefinisikan dan mendokumentasikan persyaratan produk dengan jelas dan membuatnya disetujui dari pelanggan atau analis pasar ', 'type': 'extractive', 'score': 0.0, 'context': 'outputnya studi kelayakan teknis   inisiasi proyek analysis phase inputnya studi kelayakan teknis dan pendanaan analisis proyek dilakukan setelah analisis persyaratan dilakukan  tahap selanjutnya adalah mendefinisikan dan mendokumentasikan persyaratan produk dengan jelas dan membuatnya disetujui dari pelanggan atau analis pasar hal ini dilakukan melalui dokumen srs  software requirement specification  yang terdiri dari semua persyaratan produk yang akan dirancang dan dikembangkan selama siklus hidup proyek output berupa srs  spesifikasi persyaratan perangkat lunak', 'offsets_in_document': [{'start': 178, 'end': 330}], 'offsets_in_context': [{'start': 178, 'end': 330}], 'document_ids': ['6665502ba0b18d0f06fdec7e7275daef-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa tahap yang dilakukan setelah analisis peryaratan pada analysis phase dalam sdlc ', 'filters': None, 'id': 'b26936b854707bb8449fb5263423212c', 'no_answer': False, 'answers': ['tahap selanjutnya adalah mendefinisikan dan mendokumentasikan persyaratan produk dengan jelas dan membuatnya disetujui dari pelanggan atau analis pasar '], 'offsets_in_documents': [{'start': 178, 'end': 330}], 'offsets_in_contexts': [{'start': 178, 'end': 330}], 'document_ids': ['6665502ba0b18d0f06fdec7e7275daef-0'], 'contexts': ['outputnya studi kelayakan teknis   inisiasi proyek analysis phase inputnya studi kelayakan teknis dan pendanaan analisis proyek dilakukan setelah analisis persyaratan dilakukan  tahap selanjutnya adalah mendefinisikan dan mendokumentasikan persyaratan produk dengan jelas dan membuatnya disetujui dari pelanggan atau analis pasar hal ini dilakukan melalui dokumen srs  software requirement specification  yang terdiri dari semua persyaratan produk yang akan dirancang dan dikembangkan selama siklus hidup proyek output berupa srs  spesifikasi persyaratan perangkat lunak']}>,\n",
              " <MultiLabel: {'labels': [{'id': '30a2b311-fdc0-4a4f-84a0-15144f872442', 'query': 'apa kepanjangan dari scc ', 'document': {'id': 'd0ea940b30be355105c19e7957e0b0fd-0', 'content': 'sedangkan untuk files output terdapat outflobj   open  data    w   untuk membuka file  data  untuk memodifikasi  outflobj write s  untuk memodifikasi string s pada file  outflobj writelines l  untuk memodifikasi setiap string pada list l di file  outflobj close   untuk menutup file python memiliki pustakanya untuk data science dengan beberapa toolboxes atau libraries popular yaitu math  numpy  scipy  pandas  scikit learn  serta libraries visualisasinya yaitu mathplotlib dan seaborn di mana semua pustaka terinstall pada scc  shared computing cluster', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1118, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'shared computing cluster', 'type': 'extractive', 'score': 0.0, 'context': 'sedangkan untuk files output terdapat outflobj   open  data    w   untuk membuka file  data  untuk memodifikasi  outflobj write s  untuk memodifikasi string s pada file  outflobj writelines l  untuk memodifikasi setiap string pada list l di file  outflobj close   untuk menutup file python memiliki pustakanya untuk data science dengan beberapa toolboxes atau libraries popular yaitu math  numpy  scipy  pandas  scikit learn  serta libraries visualisasinya yaitu mathplotlib dan seaborn di mana semua pustaka terinstall pada scc  shared computing cluster', 'offsets_in_document': [{'start': 530, 'end': 554}], 'offsets_in_context': [{'start': 530, 'end': 554}], 'document_ids': ['d0ea940b30be355105c19e7957e0b0fd-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa kepanjangan dari scc ', 'filters': None, 'id': '20c7cee61ca87f45247f6162852d6b74', 'no_answer': False, 'answers': ['shared computing cluster'], 'offsets_in_documents': [{'start': 530, 'end': 554}], 'offsets_in_contexts': [{'start': 530, 'end': 554}], 'document_ids': ['d0ea940b30be355105c19e7957e0b0fd-0'], 'contexts': ['sedangkan untuk files output terdapat outflobj   open  data    w   untuk membuka file  data  untuk memodifikasi  outflobj write s  untuk memodifikasi string s pada file  outflobj writelines l  untuk memodifikasi setiap string pada list l di file  outflobj close   untuk menutup file python memiliki pustakanya untuk data science dengan beberapa toolboxes atau libraries popular yaitu math  numpy  scipy  pandas  scikit learn  serta libraries visualisasinya yaitu mathplotlib dan seaborn di mana semua pustaka terinstall pada scc  shared computing cluster']}>,\n",
              " <MultiLabel: {'labels': [{'id': '497b566c-40a0-44ab-a1be-bcc21d9dc042', 'query': 'apa fungsi files output outflobj.writelines l  ', 'document': {'id': 'd0ea940b30be355105c19e7957e0b0fd-0', 'content': 'sedangkan untuk files output terdapat outflobj   open  data    w   untuk membuka file  data  untuk memodifikasi  outflobj write s  untuk memodifikasi string s pada file  outflobj writelines l  untuk memodifikasi setiap string pada list l di file  outflobj close   untuk menutup file python memiliki pustakanya untuk data science dengan beberapa toolboxes atau libraries popular yaitu math  numpy  scipy  pandas  scikit learn  serta libraries visualisasinya yaitu mathplotlib dan seaborn di mana semua pustaka terinstall pada scc  shared computing cluster', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1118, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'untuk memodifikasi setiap string pada list l di file ', 'type': 'extractive', 'score': 0.0, 'context': 'sedangkan untuk files output terdapat outflobj   open  data    w   untuk membuka file  data  untuk memodifikasi  outflobj write s  untuk memodifikasi string s pada file  outflobj writelines l  untuk memodifikasi setiap string pada list l di file  outflobj close   untuk menutup file python memiliki pustakanya untuk data science dengan beberapa toolboxes atau libraries popular yaitu math  numpy  scipy  pandas  scikit learn  serta libraries visualisasinya yaitu mathplotlib dan seaborn di mana semua pustaka terinstall pada scc  shared computing cluster', 'offsets_in_document': [{'start': 193, 'end': 246}], 'offsets_in_context': [{'start': 193, 'end': 246}], 'document_ids': ['d0ea940b30be355105c19e7957e0b0fd-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa fungsi files output outflobj.writelines l  ', 'filters': None, 'id': '1f955849da06e00097ff61df278a9c52', 'no_answer': False, 'answers': ['untuk memodifikasi setiap string pada list l di file '], 'offsets_in_documents': [{'start': 193, 'end': 246}], 'offsets_in_contexts': [{'start': 193, 'end': 246}], 'document_ids': ['d0ea940b30be355105c19e7957e0b0fd-0'], 'contexts': ['sedangkan untuk files output terdapat outflobj   open  data    w   untuk membuka file  data  untuk memodifikasi  outflobj write s  untuk memodifikasi string s pada file  outflobj writelines l  untuk memodifikasi setiap string pada list l di file  outflobj close   untuk menutup file python memiliki pustakanya untuk data science dengan beberapa toolboxes atau libraries popular yaitu math  numpy  scipy  pandas  scikit learn  serta libraries visualisasinya yaitu mathplotlib dan seaborn di mana semua pustaka terinstall pada scc  shared computing cluster']}>,\n",
              " <MultiLabel: {'labels': [{'id': '6bbfb96f-e61f-45b4-be5a-235e739e6e66', 'query': 'apa fungsi axes dalam pustaka panda ', 'document': {'id': 'd2a11b55bb55bbcd47ce0e2f8611dc58-0', 'content': 'read_sas  myfile sas7bdat    pd read_hdf  myfile h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1129, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mendefinisikan list baris label dan nama kolom', 'type': 'extractive', 'score': 0.0, 'context': 'read_sas  myfile sas7bdat    pd read_hdf  myfile h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data', 'offsets_in_document': [{'start': 272, 'end': 318}], 'offsets_in_context': [{'start': 272, 'end': 318}], 'document_ids': ['d2a11b55bb55bbcd47ce0e2f8611dc58-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa fungsi axes dalam pustaka panda ', 'filters': None, 'id': '5d4e040f4474fd820924d3670722f133', 'no_answer': False, 'answers': ['mendefinisikan list baris label dan nama kolom'], 'offsets_in_documents': [{'start': 272, 'end': 318}], 'offsets_in_contexts': [{'start': 272, 'end': 318}], 'document_ids': ['d2a11b55bb55bbcd47ce0e2f8611dc58-0'], 'contexts': ['read_sas  myfile sas7bdat    pd read_hdf  myfile h5   df kemudian dapat pula untuk membuat data frame menggunakan list dengan atribut data frame yaitu dtypes untuk mendefinisikan list tipe dari kolom yang ada  columns untuk mendefinisikan list dari nama kolom  axes untuk mendefinisikan list baris label dan nama kolom  ndim untuk mendefinisikan number dari dimensi  size untuk mendifinisikan number dari elemen  shape untuk me return representasi tuple dari dimensi  dan values untuk mendefinisikan representasi numpy dari sebuah data']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e28b35d8-e9d7-4c71-8c89-acdcf53be5c9', 'query': 'apakah yang dituliskan pada langkah langkah tersebut ', 'document': {'id': '40455a72650f77429304f32e017e4b2a-0', 'content': 'mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang kemudian menentukan alur proses penyelesaian sub sub problem  dan solusi  serta tools dan teknik yang dibutuhkan analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1135, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': '  sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya', 'type': 'extractive', 'score': 0.0, 'context': 'mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang kemudian menentukan alur proses penyelesaian sub sub problem  dan solusi  serta tools dan teknik yang dibutuhkan analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin', 'offsets_in_document': [{'start': 479, 'end': 558}], 'offsets_in_context': [{'start': 479, 'end': 558}], 'document_ids': ['40455a72650f77429304f32e017e4b2a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apakah yang dituliskan pada langkah langkah tersebut ', 'filters': None, 'id': '81ace95530324a07eb47cd44be2dd454', 'no_answer': False, 'answers': ['  sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya'], 'offsets_in_documents': [{'start': 479, 'end': 558}], 'offsets_in_contexts': [{'start': 479, 'end': 558}], 'document_ids': ['40455a72650f77429304f32e017e4b2a-0'], 'contexts': ['mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang kemudian menentukan alur proses penyelesaian sub sub problem  dan solusi  serta tools dan teknik yang dibutuhkan analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'fbd6b8de-53f8-4fa8-a164-1f2cad368c27', 'query': 'setelah menuliskan semua langkah  kemudian tugas apa yang harus dilakukan ', 'document': {'id': '40455a72650f77429304f32e017e4b2a-0', 'content': 'mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang kemudian menentukan alur proses penyelesaian sub sub problem  dan solusi  serta tools dan teknik yang dibutuhkan analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1135, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'menentukan alur proses penyelesaian sub sub problem  dan solusi  serta tools dan teknik yang dibutuhkan', 'type': 'extractive', 'score': 0.0, 'context': 'mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang kemudian menentukan alur proses penyelesaian sub sub problem  dan solusi  serta tools dan teknik yang dibutuhkan analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin', 'offsets_in_document': [{'start': 599, 'end': 702}], 'offsets_in_context': [{'start': 599, 'end': 702}], 'document_ids': ['40455a72650f77429304f32e017e4b2a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'setelah menuliskan semua langkah  kemudian tugas apa yang harus dilakukan ', 'filters': None, 'id': '93c4c2d6a22d5824d5fcd167cb90660c', 'no_answer': False, 'answers': ['menentukan alur proses penyelesaian sub sub problem  dan solusi  serta tools dan teknik yang dibutuhkan'], 'offsets_in_documents': [{'start': 599, 'end': 702}], 'offsets_in_contexts': [{'start': 599, 'end': 702}], 'document_ids': ['40455a72650f77429304f32e017e4b2a-0'], 'contexts': ['mengestimasi cost dan menghitung keuntungan atau manfaat yang didapatkan\\n3. mendefinisikan tujuan mengolah data\\n mendeskripsikan tipe masalah seperti klustering  prediksi atau klasifikasi \\n menentukan hasil dari pengolahan data seperti model  laporan  presentasi  dan dataset yang terproses\\n menentukan kriteria keberhasilan pengolahan data\\n4. merencanakan project\\ntuliskan langkah demi langkah rencana kegiatan untuk project yang akan dikerjakan tambahkan tiap langkah tersebut   sumber daya yang dibutuhkan  input  output  ketergantungan langkah sebelumnya  perjelas langkah yang diulang kemudian menentukan alur proses penyelesaian sub sub problem  dan solusi  serta tools dan teknik yang dibutuhkan analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin']}>,\n",
              " <MultiLabel: {'labels': [{'id': '8b9733f2-8acc-4641-a645-113c80b8525d', 'query': 'dimanakah togaf banyak digunakan ', 'document': {'id': '869d7e6acc5b2bae4ecce90744a23fe5-0', 'content': 'pengguna dan aplikasi dapat dengan mudah mengakses data dari database data warehouse dan hadoop togaf adalah sebuah framework yang banyak digunakan untuk pembuatan arsitektur enterprise togaf juga merupakan metode umum yang komprehensif framework ini populer karena beberapa alasan pertama  togaf merupakan standard terbuka yang tidak tergantung pada vendor ataupun teknologi yang digunakan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1098, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' untuk pembuatan arsitektur enterprise ', 'type': 'extractive', 'score': 0.0, 'context': 'pengguna dan aplikasi dapat dengan mudah mengakses data dari database data warehouse dan hadoop togaf adalah sebuah framework yang banyak digunakan untuk pembuatan arsitektur enterprise togaf juga merupakan metode umum yang komprehensif framework ini populer karena beberapa alasan pertama  togaf merupakan standard terbuka yang tidak tergantung pada vendor ataupun teknologi yang digunakan', 'offsets_in_document': [{'start': 147, 'end': 186}], 'offsets_in_context': [{'start': 147, 'end': 186}], 'document_ids': ['869d7e6acc5b2bae4ecce90744a23fe5-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'dimanakah togaf banyak digunakan ', 'filters': None, 'id': '070f68efd9b397c64e1803016950dca2', 'no_answer': False, 'answers': [' untuk pembuatan arsitektur enterprise '], 'offsets_in_documents': [{'start': 147, 'end': 186}], 'offsets_in_contexts': [{'start': 147, 'end': 186}], 'document_ids': ['869d7e6acc5b2bae4ecce90744a23fe5-0'], 'contexts': ['pengguna dan aplikasi dapat dengan mudah mengakses data dari database data warehouse dan hadoop togaf adalah sebuah framework yang banyak digunakan untuk pembuatan arsitektur enterprise togaf juga merupakan metode umum yang komprehensif framework ini populer karena beberapa alasan pertama  togaf merupakan standard terbuka yang tidak tergantung pada vendor ataupun teknologi yang digunakan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '203709c0-1412-4b22-aa95-3b718cd02da7', 'query': 'apa sebutan lain dengan togaf ', 'document': {'id': '869d7e6acc5b2bae4ecce90744a23fe5-0', 'content': 'pengguna dan aplikasi dapat dengan mudah mengakses data dari database data warehouse dan hadoop togaf adalah sebuah framework yang banyak digunakan untuk pembuatan arsitektur enterprise togaf juga merupakan metode umum yang komprehensif framework ini populer karena beberapa alasan pertama  togaf merupakan standard terbuka yang tidak tergantung pada vendor ataupun teknologi yang digunakan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1098, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'hadoop ', 'type': 'extractive', 'score': 0.0, 'context': 'pengguna dan aplikasi dapat dengan mudah mengakses data dari database data warehouse dan hadoop togaf adalah sebuah framework yang banyak digunakan untuk pembuatan arsitektur enterprise togaf juga merupakan metode umum yang komprehensif framework ini populer karena beberapa alasan pertama  togaf merupakan standard terbuka yang tidak tergantung pada vendor ataupun teknologi yang digunakan', 'offsets_in_document': [{'start': 89, 'end': 96}], 'offsets_in_context': [{'start': 89, 'end': 96}], 'document_ids': ['869d7e6acc5b2bae4ecce90744a23fe5-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa sebutan lain dengan togaf ', 'filters': None, 'id': 'cb630453de926d50c1a3f3f39771af11', 'no_answer': False, 'answers': ['hadoop '], 'offsets_in_documents': [{'start': 89, 'end': 96}], 'offsets_in_contexts': [{'start': 89, 'end': 96}], 'document_ids': ['869d7e6acc5b2bae4ecce90744a23fe5-0'], 'contexts': ['pengguna dan aplikasi dapat dengan mudah mengakses data dari database data warehouse dan hadoop togaf adalah sebuah framework yang banyak digunakan untuk pembuatan arsitektur enterprise togaf juga merupakan metode umum yang komprehensif framework ini populer karena beberapa alasan pertama  togaf merupakan standard terbuka yang tidak tergantung pada vendor ataupun teknologi yang digunakan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3b70c467-3f6d-406c-a765-71fce8ee9dc6', 'query': 'apa hasil yang harus disesuaikan dengan ddd ', 'document': {'id': 'ff714ccd554cc7223af964717ada12cc-0', 'content': 'build phase inputnya ddd  dokumen desain detail pada design phase pengembangan aktual dimulai dan produk dibangun kode pemrograman dihasilkan sesuai ddd  detail design document  selama fase ini jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1109, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kode pemrograman', 'type': 'extractive', 'score': 0.0, 'context': 'build phase inputnya ddd  dokumen desain detail pada design phase pengembangan aktual dimulai dan produk dibangun kode pemrograman dihasilkan sesuai ddd  detail design document  selama fase ini jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode', 'offsets_in_document': [{'start': 114, 'end': 130}], 'offsets_in_context': [{'start': 114, 'end': 130}], 'document_ids': ['ff714ccd554cc7223af964717ada12cc-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa hasil yang harus disesuaikan dengan ddd ', 'filters': None, 'id': '438ae985e70e8eb49a453a69671b4264', 'no_answer': False, 'answers': ['kode pemrograman'], 'offsets_in_documents': [{'start': 114, 'end': 130}], 'offsets_in_contexts': [{'start': 114, 'end': 130}], 'document_ids': ['ff714ccd554cc7223af964717ada12cc-0'], 'contexts': ['build phase inputnya ddd  dokumen desain detail pada design phase pengembangan aktual dimulai dan produk dibangun kode pemrograman dihasilkan sesuai ddd  detail design document  selama fase ini jika desain dilakukan secara rinci dan terorganisir  pembuatan kode dapat dicapai tanpa banyak kerumitan pengembang harus mengikuti pedoman pengkodean yang ditentukan oleh organisasi mereka dan alat pemrograman seperti kompiler  interpreter  debugger  dll digunakan untuk menghasilkan kode']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd9940509-5ed2-4bb8-89db-ab38cc3fb5e3', 'query': 'apakah pengertian data collection ', 'document': {'id': '3f7bc0686cc03eef762d29a5c27db675-0', 'content': 'analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin setelah melakukan tahap analitik maka tahap selanjutnya adalah \\n data collection   pengumpulan data dari satu sumber\\n gathering data   pengumpulan data dari satu sumber', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1138, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': '  pengumpulan data dari satu sumber', 'type': 'extractive', 'score': 0.0, 'context': 'analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin setelah melakukan tahap analitik maka tahap selanjutnya adalah \\n data collection   pengumpulan data dari satu sumber\\n gathering data   pengumpulan data dari satu sumber', 'offsets_in_document': [{'start': 239, 'end': 274}], 'offsets_in_context': [{'start': 239, 'end': 274}], 'document_ids': ['3f7bc0686cc03eef762d29a5c27db675-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apakah pengertian data collection ', 'filters': None, 'id': '9f67a9664937ad023eed952289debc06', 'no_answer': False, 'answers': ['  pengumpulan data dari satu sumber'], 'offsets_in_documents': [{'start': 239, 'end': 274}], 'offsets_in_contexts': [{'start': 239, 'end': 274}], 'document_ids': ['3f7bc0686cc03eef762d29a5c27db675-0'], 'contexts': ['analytic approach dilakukan setelah masalah bisnis telah dinyatakan dengan jelas mencakup penyelesaian masalah dengan teknik statistik dan pembelajaran mesin setelah melakukan tahap analitik maka tahap selanjutnya adalah \\n data collection   pengumpulan data dari satu sumber\\n gathering data   pengumpulan data dari satu sumber']}>,\n",
              " <MultiLabel: {'labels': [{'id': '7edf0fe1-73e6-4aeb-91d8-d069516da2c4', 'query': 'mengapa data penduduk di suatu negara menggunakan penyampelan kluster ', 'document': {'id': 'e94afe64370deec5ad6d9e7af59d57a0-0', 'content': 'penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut penyampelan daerah merupakan bagian dari penyampelan kluster yang secara khusus melibatkan suatu daerah sampel sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1142, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'sumber datanya sangat luas', 'type': 'extractive', 'score': 0.0, 'context': 'penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut penyampelan daerah merupakan bagian dari penyampelan kluster yang secara khusus melibatkan suatu daerah sampel sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih', 'offsets_in_document': [{'start': 128, 'end': 154}], 'offsets_in_context': [{'start': 128, 'end': 154}], 'document_ids': ['e94afe64370deec5ad6d9e7af59d57a0-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa data penduduk di suatu negara menggunakan penyampelan kluster ', 'filters': None, 'id': '9c08c59c47949b94aed4d08d2312050d', 'no_answer': False, 'answers': ['sumber datanya sangat luas'], 'offsets_in_documents': [{'start': 128, 'end': 154}], 'offsets_in_contexts': [{'start': 128, 'end': 154}], 'document_ids': ['e94afe64370deec5ad6d9e7af59d57a0-0'], 'contexts': ['penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut penyampelan daerah merupakan bagian dari penyampelan kluster yang secara khusus melibatkan suatu daerah sampel sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c1c7b622-3be1-404c-8004-090762d437d9', 'query': 'apa metode yang digunakan jika setiap elemen populasi tidak mempunyai peluang yang sama untuk dipilih ', 'document': {'id': 'e94afe64370deec5ad6d9e7af59d57a0-0', 'content': 'penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut penyampelan daerah merupakan bagian dari penyampelan kluster yang secara khusus melibatkan suatu daerah sampel sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1142, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' metode sampling non probabilitas ', 'type': 'extractive', 'score': 0.0, 'context': 'penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut penyampelan daerah merupakan bagian dari penyampelan kluster yang secara khusus melibatkan suatu daerah sampel sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih', 'offsets_in_document': [{'start': 406, 'end': 440}], 'offsets_in_context': [{'start': 406, 'end': 440}], 'document_ids': ['e94afe64370deec5ad6d9e7af59d57a0-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa metode yang digunakan jika setiap elemen populasi tidak mempunyai peluang yang sama untuk dipilih ', 'filters': None, 'id': 'e91e02935d2536bb0160c968e6cb49b3', 'no_answer': False, 'answers': [' metode sampling non probabilitas '], 'offsets_in_documents': [{'start': 406, 'end': 440}], 'offsets_in_contexts': [{'start': 406, 'end': 440}], 'document_ids': ['e94afe64370deec5ad6d9e7af59d57a0-0'], 'contexts': ['penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut penyampelan daerah merupakan bagian dari penyampelan kluster yang secara khusus melibatkan suatu daerah sampel sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih']}>,\n",
              " <MultiLabel: {'labels': [{'id': '8a40fb7a-f3d0-451f-a045-01b545670823', 'query': 'untuk menampilkan simpangan  deviasi  data visualisasi data apa yang dipilih ', 'document': {'id': '9e5bde105616d88685cfae52195d837a-0', 'content': '7  menunjukkan proporsi bagian dari jumlah total dengan tree map  histogram  pie chart  stacked bar  peta  dan sunburst 8  menunjukkan perubahan nilai pada satu variabel dengan kolomi dan scatter plot 9  menampilkan simpangan atau deviasi data dengan kombinasi  deviation bar  bar chart  dan area 10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1176, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kombinasi  deviation bar  bar chart  dan area', 'type': 'extractive', 'score': 0.0, 'context': '7  menunjukkan proporsi bagian dari jumlah total dengan tree map  histogram  pie chart  stacked bar  peta  dan sunburst 8  menunjukkan perubahan nilai pada satu variabel dengan kolomi dan scatter plot 9  menampilkan simpangan atau deviasi data dengan kombinasi  deviation bar  bar chart  dan area 10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar', 'offsets_in_document': [{'start': 251, 'end': 296}], 'offsets_in_context': [{'start': 251, 'end': 296}], 'document_ids': ['9e5bde105616d88685cfae52195d837a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'untuk menampilkan simpangan  deviasi  data visualisasi data apa yang dipilih ', 'filters': None, 'id': 'eb485af0f3166d401989b804eadeb7f4', 'no_answer': False, 'answers': ['kombinasi  deviation bar  bar chart  dan area'], 'offsets_in_documents': [{'start': 251, 'end': 296}], 'offsets_in_contexts': [{'start': 251, 'end': 296}], 'document_ids': ['9e5bde105616d88685cfae52195d837a-0'], 'contexts': ['7  menunjukkan proporsi bagian dari jumlah total dengan tree map  histogram  pie chart  stacked bar  peta  dan sunburst 8  menunjukkan perubahan nilai pada satu variabel dengan kolomi dan scatter plot 9  menampilkan simpangan atau deviasi data dengan kombinasi  deviation bar  bar chart  dan area 10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'feb96989-bdbf-4188-8048-94514a3cf01b', 'query': 'untuk menunjukan perubahan nilai pada satu variabel. visualisasi data apa yang dipilih ', 'document': {'id': '9e5bde105616d88685cfae52195d837a-0', 'content': '7  menunjukkan proporsi bagian dari jumlah total dengan tree map  histogram  pie chart  stacked bar  peta  dan sunburst 8  menunjukkan perubahan nilai pada satu variabel dengan kolomi dan scatter plot 9  menampilkan simpangan atau deviasi data dengan kombinasi  deviation bar  bar chart  dan area 10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1176, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kolomi dan scatter plot', 'type': 'extractive', 'score': 0.0, 'context': '7  menunjukkan proporsi bagian dari jumlah total dengan tree map  histogram  pie chart  stacked bar  peta  dan sunburst 8  menunjukkan perubahan nilai pada satu variabel dengan kolomi dan scatter plot 9  menampilkan simpangan atau deviasi data dengan kombinasi  deviation bar  bar chart  dan area 10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar', 'offsets_in_document': [{'start': 177, 'end': 200}], 'offsets_in_context': [{'start': 177, 'end': 200}], 'document_ids': ['9e5bde105616d88685cfae52195d837a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'untuk menunjukan perubahan nilai pada satu variabel. visualisasi data apa yang dipilih ', 'filters': None, 'id': '636764b6e3a0b6616be518f08cb70fb1', 'no_answer': False, 'answers': ['kolomi dan scatter plot'], 'offsets_in_documents': [{'start': 177, 'end': 200}], 'offsets_in_contexts': [{'start': 177, 'end': 200}], 'document_ids': ['9e5bde105616d88685cfae52195d837a-0'], 'contexts': ['7  menunjukkan proporsi bagian dari jumlah total dengan tree map  histogram  pie chart  stacked bar  peta  dan sunburst 8  menunjukkan perubahan nilai pada satu variabel dengan kolomi dan scatter plot 9  menampilkan simpangan atau deviasi data dengan kombinasi  deviation bar  bar chart  dan area 10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar']}>,\n",
              " <MultiLabel: {'labels': [{'id': '9f09e117-2557-4c31-a108-ad3e63ffd2c0', 'query': 'data sekuensial termasuk jenis data apa ', 'document': {'id': '2cd58ee67a370ce5523c6cb476b38e56-0', 'content': 'data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data sumber data dikelompokkan menjadi dua  yakni sumber internal dan eksternal contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1143, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'ata yang ordered', 'type': 'extractive', 'score': 0.0, 'context': 'data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data sumber data dikelompokkan menjadi dua  yakni sumber internal dan eksternal contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages', 'offsets_in_document': [{'start': 1, 'end': 17}], 'offsets_in_context': [{'start': 1, 'end': 17}], 'document_ids': ['2cd58ee67a370ce5523c6cb476b38e56-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'data sekuensial termasuk jenis data apa ', 'filters': None, 'id': '969e79252177dfe58db1784bdf52f9a2', 'no_answer': False, 'answers': ['ata yang ordered'], 'offsets_in_documents': [{'start': 1, 'end': 17}], 'offsets_in_contexts': [{'start': 1, 'end': 17}], 'document_ids': ['2cd58ee67a370ce5523c6cb476b38e56-0'], 'contexts': ['data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data sumber data dikelompokkan menjadi dua  yakni sumber internal dan eksternal contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages']}>,\n",
              " <MultiLabel: {'labels': [{'id': '790f2694-2971-4a79-8760-6a6835fcf757', 'query': 'data gambar dan suara termasuk jenis data apa ', 'document': {'id': '2cd58ee67a370ce5523c6cb476b38e56-0', 'content': 'data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data sumber data dikelompokkan menjadi dua  yakni sumber internal dan eksternal contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1143, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data spatial  image  dan multimedia ', 'type': 'extractive', 'score': 0.0, 'context': 'data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data sumber data dikelompokkan menjadi dua  yakni sumber internal dan eksternal contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages', 'offsets_in_document': [{'start': 118, 'end': 154}], 'offsets_in_context': [{'start': 118, 'end': 154}], 'document_ids': ['2cd58ee67a370ce5523c6cb476b38e56-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'data gambar dan suara termasuk jenis data apa ', 'filters': None, 'id': 'dae2d3cc274fd217f95c859246dbe6f1', 'no_answer': False, 'answers': ['data spatial  image  dan multimedia '], 'offsets_in_documents': [{'start': 118, 'end': 154}], 'offsets_in_contexts': [{'start': 118, 'end': 154}], 'document_ids': ['2cd58ee67a370ce5523c6cb476b38e56-0'], 'contexts': ['data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data sumber data dikelompokkan menjadi dua  yakni sumber internal dan eksternal contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages']}>,\n",
              " <MultiLabel: {'labels': [{'id': '81c43895-c15f-47be-9f82-06ebbf3df306', 'query': 'apa saja metode sampling yang dapat digunakan ', 'document': {'id': 'ecbcdde78021cf3a330e728fc1dcc1dd-0', 'content': 'ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi setiap elemen populasi mempunyai peluang yang sama sebagai sampel ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1148, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'ada dua metode sampling  yaitu probabilitas dan non probabilitas ', 'type': 'extractive', 'score': 0.0, 'context': 'ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi setiap elemen populasi mempunyai peluang yang sama sebagai sampel ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas', 'offsets_in_document': [{'start': 0, 'end': 65}], 'offsets_in_context': [{'start': 0, 'end': 65}], 'document_ids': ['ecbcdde78021cf3a330e728fc1dcc1dd-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja metode sampling yang dapat digunakan ', 'filters': None, 'id': 'ee810ba12479b73245c4304404f66deb', 'no_answer': False, 'answers': ['ada dua metode sampling  yaitu probabilitas dan non probabilitas '], 'offsets_in_documents': [{'start': 0, 'end': 65}], 'offsets_in_contexts': [{'start': 0, 'end': 65}], 'document_ids': ['ecbcdde78021cf3a330e728fc1dcc1dd-0'], 'contexts': ['ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi setiap elemen populasi mempunyai peluang yang sama sebagai sampel ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas']}>,\n",
              " <MultiLabel: {'labels': [{'id': '1738b0f1-eca6-4672-b017-766cc0789d77', 'query': 'bagaimana cara melakukan sampling dalam data understanding ', 'document': {'id': 'ecbcdde78021cf3a330e728fc1dcc1dd-0', 'content': 'ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi setiap elemen populasi mempunyai peluang yang sama sebagai sampel ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1148, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'ada dua metode sampling  yaitu probabilitas dan non probabilitas', 'type': 'extractive', 'score': 0.0, 'context': 'ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi setiap elemen populasi mempunyai peluang yang sama sebagai sampel ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas', 'offsets_in_document': [{'start': 0, 'end': 64}], 'offsets_in_context': [{'start': 0, 'end': 64}], 'document_ids': ['ecbcdde78021cf3a330e728fc1dcc1dd-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara melakukan sampling dalam data understanding ', 'filters': None, 'id': '779a64af127986e782849c6dfa45c82d', 'no_answer': False, 'answers': ['ada dua metode sampling  yaitu probabilitas dan non probabilitas'], 'offsets_in_documents': [{'start': 0, 'end': 64}], 'offsets_in_contexts': [{'start': 0, 'end': 64}], 'document_ids': ['ecbcdde78021cf3a330e728fc1dcc1dd-0'], 'contexts': ['ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi setiap elemen populasi mempunyai peluang yang sama sebagai sampel ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas']}>,\n",
              " <MultiLabel: {'labels': [{'id': '9aa28ea2-37e3-4881-a4bb-bfbb774bf077', 'query': 'apa yang dimaksud dengan data objek ', 'document': {'id': '8f3ba82fb92bdfc8b79da5e7eeab2a2-0', 'content': 'data atau dataset merupakan kumpulan dari data objek yang merepresentasikan sebuah entitas  atribut sementara itu  data objek disebut juga sebagai record  point  sample  dan instance atribut merepresentasikan karakteristik sebuah data objek misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1153, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data objek disebut juga sebagai record  point  sample  dan instance atribut merepresentasikan karakteristik sebuah data', 'type': 'extractive', 'score': 0.0, 'context': 'data atau dataset merupakan kumpulan dari data objek yang merepresentasikan sebuah entitas  atribut sementara itu  data objek disebut juga sebagai record  point  sample  dan instance atribut merepresentasikan karakteristik sebuah data objek misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur', 'offsets_in_document': [{'start': 115, 'end': 234}], 'offsets_in_context': [{'start': 115, 'end': 234}], 'document_ids': ['8f3ba82fb92bdfc8b79da5e7eeab2a2-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan data objek ', 'filters': None, 'id': '39cea5fe68e1cecd63bd7b8bcd3d28ec', 'no_answer': False, 'answers': ['data objek disebut juga sebagai record  point  sample  dan instance atribut merepresentasikan karakteristik sebuah data'], 'offsets_in_documents': [{'start': 115, 'end': 234}], 'offsets_in_contexts': [{'start': 115, 'end': 234}], 'document_ids': ['8f3ba82fb92bdfc8b79da5e7eeab2a2-0'], 'contexts': ['data atau dataset merupakan kumpulan dari data objek yang merepresentasikan sebuah entitas  atribut sementara itu  data objek disebut juga sebagai record  point  sample  dan instance atribut merepresentasikan karakteristik sebuah data objek misalnya  untuk data mahasiswa  terdapat atribut nama  nim  usia  jenis kelamin  dan sebagainya atribut juga dikenal dengan sebutan variabel atau fitur']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e941a432-15ed-4748-a4a5-8f58d6875c5c', 'query': 'apa yang dimaksud dengan populasi ', 'document': {'id': 'f2bef93847b319ba879d3e16348597d7-0', 'content': 'contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages populasi merupakan wilayah generalisasi yang terdiri atas subjek atau objek yang mempunyai kualitas dan karakteristik tertentu yang ditetapkan oleh peneliti untuk dipelajari  kemudian ditarik kesimpulannya populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1159, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'populasi merupakan wilayah generalisasi yang terdiri atas subjek atau objek yang mempunyai kualitas dan karakteristik tertentu yang ditetapkan oleh peneliti untuk dipelajari  kemudian ditarik kesimpulannya ', 'type': 'extractive', 'score': 0.0, 'context': 'contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages populasi merupakan wilayah generalisasi yang terdiri atas subjek atau objek yang mempunyai kualitas dan karakteristik tertentu yang ditetapkan oleh peneliti untuk dipelajari  kemudian ditarik kesimpulannya populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi', 'offsets_in_document': [{'start': 220, 'end': 426}], 'offsets_in_context': [{'start': 220, 'end': 426}], 'document_ids': ['f2bef93847b319ba879d3e16348597d7-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan populasi ', 'filters': None, 'id': '583f3ed964a4195f26c1c64e65d713e5', 'no_answer': False, 'answers': ['populasi merupakan wilayah generalisasi yang terdiri atas subjek atau objek yang mempunyai kualitas dan karakteristik tertentu yang ditetapkan oleh peneliti untuk dipelajari  kemudian ditarik kesimpulannya '], 'offsets_in_documents': [{'start': 220, 'end': 426}], 'offsets_in_contexts': [{'start': 220, 'end': 426}], 'document_ids': ['f2bef93847b319ba879d3e16348597d7-0'], 'contexts': ['contoh sumber internal adalah database  teks dokumen  multimedia dokumen  audio  video   dan spreadsheet  excel  csv  json  dll sedangkan contoh sumber eksternal adalah open data repositories dan public domain web pages populasi merupakan wilayah generalisasi yang terdiri atas subjek atau objek yang mempunyai kualitas dan karakteristik tertentu yang ditetapkan oleh peneliti untuk dipelajari  kemudian ditarik kesimpulannya populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '26c1e831-71ed-4338-bafd-60b794792aca', 'query': 'apa teknik yang digunakan untuk visualisasi', 'document': {'id': 'a8589ea0072b262ee62464cc94f941fe-0', 'content': 'visualisasi data dapat menggunakan teknik visualisasi dengan bahasa python dengan python untuk membuat visualisasi dapat menggunakan toolkit seperti matplotlib pyplot dan seaborn kemudian jalankan import matplotlib pyplot tetapi harus dipastikan sudah terinstal sebelumnya terdapat beberapa macam visualisasi dengan berbagai macam yang diantaranya pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  boxplot  dan macam   macam visualisasi lainnya dengan berbagai macam visualisasi yang ada dapat diberikan penjelasan masing   masing macam visualisasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1170, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'visualisasi data dapat menggunakan teknik visualisasi dengan bahasa python dengan python untuk membuat visualisasi dapat menggunakan toolkit seperti matplotlib pyplot dan seaborn ', 'type': 'extractive', 'score': 0.0, 'context': 'visualisasi data dapat menggunakan teknik visualisasi dengan bahasa python dengan python untuk membuat visualisasi dapat menggunakan toolkit seperti matplotlib pyplot dan seaborn kemudian jalankan import matplotlib pyplot tetapi harus dipastikan sudah terinstal sebelumnya terdapat beberapa macam visualisasi dengan berbagai macam yang diantaranya pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  boxplot  dan macam   macam visualisasi lainnya dengan berbagai macam visualisasi yang ada dapat diberikan penjelasan masing   masing macam visualisasi', 'offsets_in_document': [{'start': 0, 'end': 179}], 'offsets_in_context': [{'start': 0, 'end': 179}], 'document_ids': ['a8589ea0072b262ee62464cc94f941fe-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa teknik yang digunakan untuk visualisasi', 'filters': None, 'id': '44d93ae6918b789a48748a036a1ed77d', 'no_answer': False, 'answers': ['visualisasi data dapat menggunakan teknik visualisasi dengan bahasa python dengan python untuk membuat visualisasi dapat menggunakan toolkit seperti matplotlib pyplot dan seaborn '], 'offsets_in_documents': [{'start': 0, 'end': 179}], 'offsets_in_contexts': [{'start': 0, 'end': 179}], 'document_ids': ['a8589ea0072b262ee62464cc94f941fe-0'], 'contexts': ['visualisasi data dapat menggunakan teknik visualisasi dengan bahasa python dengan python untuk membuat visualisasi dapat menggunakan toolkit seperti matplotlib pyplot dan seaborn kemudian jalankan import matplotlib pyplot tetapi harus dipastikan sudah terinstal sebelumnya terdapat beberapa macam visualisasi dengan berbagai macam yang diantaranya pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  boxplot  dan macam   macam visualisasi lainnya dengan berbagai macam visualisasi yang ada dapat diberikan penjelasan masing   masing macam visualisasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '02ea8511-53bd-4c9d-9fa7-bf16660ef483', 'query': 'kenapa wrapper method lebih beresiko dibandingkan dengan filte method?', 'document': {'id': 'a02e451bf8583a12996ae0a16505f104-0', 'content': 'pada kasus regresi  dapat berupa mse  sedangkan pada kasus klasifikasi dapat berupa akurasi  precision  recall atau yang lainnya biasanya membutuhkan biaya komputasi yang tinggi wrapper method dependen terhadap algoritma pembelajaran yang digunakan  sehingga lebih berisiko overfitting jika dibandingkan dengan filter method biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1137, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'wrapper method dependen terhadap algoritma pembelajaran yang digunakan  sehingga lebih berisiko overfitting jika dibandingkan dengan filter method', 'type': 'extractive', 'score': 0.0, 'context': 'pada kasus regresi  dapat berupa mse  sedangkan pada kasus klasifikasi dapat berupa akurasi  precision  recall atau yang lainnya biasanya membutuhkan biaya komputasi yang tinggi wrapper method dependen terhadap algoritma pembelajaran yang digunakan  sehingga lebih berisiko overfitting jika dibandingkan dengan filter method biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi', 'offsets_in_document': [{'start': 178, 'end': 324}], 'offsets_in_context': [{'start': 178, 'end': 324}], 'document_ids': ['a02e451bf8583a12996ae0a16505f104-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kenapa wrapper method lebih beresiko dibandingkan dengan filte method?', 'filters': None, 'id': '4a77357c34e96aae21b23585a2751253', 'no_answer': False, 'answers': ['wrapper method dependen terhadap algoritma pembelajaran yang digunakan  sehingga lebih berisiko overfitting jika dibandingkan dengan filter method'], 'offsets_in_documents': [{'start': 178, 'end': 324}], 'offsets_in_contexts': [{'start': 178, 'end': 324}], 'document_ids': ['a02e451bf8583a12996ae0a16505f104-0'], 'contexts': ['pada kasus regresi  dapat berupa mse  sedangkan pada kasus klasifikasi dapat berupa akurasi  precision  recall atau yang lainnya biasanya membutuhkan biaya komputasi yang tinggi wrapper method dependen terhadap algoritma pembelajaran yang digunakan  sehingga lebih berisiko overfitting jika dibandingkan dengan filter method biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'cc4c417a-409e-4f86-a869-b60cded554d2', 'query': 'kapan stacked bar chart digunakan?', 'document': {'id': 'fc2127a56b315dacb2de44e38c9747e7-0', 'content': 'jika ingin melihat distribusi di dua variabel  maka dapat menggunakan visualisasi data scatterplot tujuan visualisasi ke   tiga yaitu komposisi atau composition digunakan untuk melihat komposisi dari suatu variabel  jika di breakdown terhadap suatu dimensi data visualisasi yang biasa digunakan adalah stacked bar chart untuk data kategorikal  sedangkan jika data berkaitan dengan waktu maka visualisasi data yang biasa digunakan adalah stacked line chart tujuan visualisasi ke   empat yaitu relasi atau relationship digunakan untuk melihat keterhubungan antara suatu variabel dengan variabel lain visualisasi yang biasa digunakan adalah scatter plot', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1164, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'jika di breakdown terhadap suatu dimensi data visualisasi yang biasa digunakan adalah stacked bar chart', 'type': 'extractive', 'score': 0.0, 'context': 'jika ingin melihat distribusi di dua variabel  maka dapat menggunakan visualisasi data scatterplot tujuan visualisasi ke   tiga yaitu komposisi atau composition digunakan untuk melihat komposisi dari suatu variabel  jika di breakdown terhadap suatu dimensi data visualisasi yang biasa digunakan adalah stacked bar chart untuk data kategorikal  sedangkan jika data berkaitan dengan waktu maka visualisasi data yang biasa digunakan adalah stacked line chart tujuan visualisasi ke   empat yaitu relasi atau relationship digunakan untuk melihat keterhubungan antara suatu variabel dengan variabel lain visualisasi yang biasa digunakan adalah scatter plot', 'offsets_in_document': [{'start': 216, 'end': 319}], 'offsets_in_context': [{'start': 216, 'end': 319}], 'document_ids': ['fc2127a56b315dacb2de44e38c9747e7-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kapan stacked bar chart digunakan?', 'filters': None, 'id': 'b61b4f60e65bf002c1fa94560260de1c', 'no_answer': False, 'answers': ['jika di breakdown terhadap suatu dimensi data visualisasi yang biasa digunakan adalah stacked bar chart'], 'offsets_in_documents': [{'start': 216, 'end': 319}], 'offsets_in_contexts': [{'start': 216, 'end': 319}], 'document_ids': ['fc2127a56b315dacb2de44e38c9747e7-0'], 'contexts': ['jika ingin melihat distribusi di dua variabel  maka dapat menggunakan visualisasi data scatterplot tujuan visualisasi ke   tiga yaitu komposisi atau composition digunakan untuk melihat komposisi dari suatu variabel  jika di breakdown terhadap suatu dimensi data visualisasi yang biasa digunakan adalah stacked bar chart untuk data kategorikal  sedangkan jika data berkaitan dengan waktu maka visualisasi data yang biasa digunakan adalah stacked line chart tujuan visualisasi ke   empat yaitu relasi atau relationship digunakan untuk melihat keterhubungan antara suatu variabel dengan variabel lain visualisasi yang biasa digunakan adalah scatter plot']}>,\n",
              " <MultiLabel: {'labels': [{'id': '42c8006f-e9da-4ad0-9db2-b49e6f471597', 'query': 'mengapa data understanding memerlukan statistik dasar ', 'document': {'id': 'bfd9e2f8efaf42a5b9484b287405bc84-0', 'content': 'penyampelan kemudahan digunakan untuk mendapatkan informasi dengan cepat  murah  dan mudah yang biasanya untuk kuisioner menguji atau penelitian eksplorasi penyampelan pertimbangan didasarkan pada kriteria kriteria tertentu  misalnya penelitian tentang kualitas makanan  sampel sumber data ahli makanan atau ahli gizi penyampelan kuota pada dasarnya sama dengan penyampelan pertimbangan penyampelan bola salju merupakan suatu metode penarikan sampel dimana responden yang berhasil diperoleh diminta untuk menunjukkan responden lainnya secara berantai statistik dasar dapat digunakan untuk mendeskripsikan data  sehingga dapat memahami data terkait pusat distribusi  variasi  dan sebaran data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1144, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'statistik dasar dapat digunakan untuk mendeskripsikan data  sehingga dapat memahami data terkait pusat distribusi  variasi  dan sebaran data', 'type': 'extractive', 'score': 0.0, 'context': 'penyampelan kemudahan digunakan untuk mendapatkan informasi dengan cepat  murah  dan mudah yang biasanya untuk kuisioner menguji atau penelitian eksplorasi penyampelan pertimbangan didasarkan pada kriteria kriteria tertentu  misalnya penelitian tentang kualitas makanan  sampel sumber data ahli makanan atau ahli gizi penyampelan kuota pada dasarnya sama dengan penyampelan pertimbangan penyampelan bola salju merupakan suatu metode penarikan sampel dimana responden yang berhasil diperoleh diminta untuk menunjukkan responden lainnya secara berantai statistik dasar dapat digunakan untuk mendeskripsikan data  sehingga dapat memahami data terkait pusat distribusi  variasi  dan sebaran data', 'offsets_in_document': [{'start': 551, 'end': 691}], 'offsets_in_context': [{'start': 551, 'end': 691}], 'document_ids': ['bfd9e2f8efaf42a5b9484b287405bc84-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa data understanding memerlukan statistik dasar ', 'filters': None, 'id': '740fed368588085d7672a50b0db07c9f', 'no_answer': False, 'answers': ['statistik dasar dapat digunakan untuk mendeskripsikan data  sehingga dapat memahami data terkait pusat distribusi  variasi  dan sebaran data'], 'offsets_in_documents': [{'start': 551, 'end': 691}], 'offsets_in_contexts': [{'start': 551, 'end': 691}], 'document_ids': ['bfd9e2f8efaf42a5b9484b287405bc84-0'], 'contexts': ['penyampelan kemudahan digunakan untuk mendapatkan informasi dengan cepat  murah  dan mudah yang biasanya untuk kuisioner menguji atau penelitian eksplorasi penyampelan pertimbangan didasarkan pada kriteria kriteria tertentu  misalnya penelitian tentang kualitas makanan  sampel sumber data ahli makanan atau ahli gizi penyampelan kuota pada dasarnya sama dengan penyampelan pertimbangan penyampelan bola salju merupakan suatu metode penarikan sampel dimana responden yang berhasil diperoleh diminta untuk menunjukkan responden lainnya secara berantai statistik dasar dapat digunakan untuk mendeskripsikan data  sehingga dapat memahami data terkait pusat distribusi  variasi  dan sebaran data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '23e5c91f-44f0-4afc-b234-ccc338a436f7', 'query': 'apa tahap selanjutnya setelah data understanding ', 'document': {'id': '1921f4f4e5a1fffbb78076282820b849-0', 'content': ' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1149, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data preparation', 'type': 'extractive', 'score': 0.0, 'context': ' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll', 'offsets_in_document': [{'start': 309, 'end': 325}], 'offsets_in_context': [{'start': 309, 'end': 325}], 'document_ids': ['1921f4f4e5a1fffbb78076282820b849-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa tahap selanjutnya setelah data understanding ', 'filters': None, 'id': '1cd6370a70ed0e35d7cbe416c2425e15', 'no_answer': False, 'answers': ['data preparation'], 'offsets_in_documents': [{'start': 309, 'end': 325}], 'offsets_in_contexts': [{'start': 309, 'end': 325}], 'document_ids': ['1921f4f4e5a1fffbb78076282820b849-0'], 'contexts': [' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0c9552f6-438e-4f6a-9acc-b1d902f65e27', 'query': 'bagaimana data understanding diterapkan ', 'document': {'id': '1921f4f4e5a1fffbb78076282820b849-0', 'content': ' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1149, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan', 'type': 'extractive', 'score': 0.0, 'context': ' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll', 'offsets_in_document': [{'start': 110, 'end': 221}], 'offsets_in_context': [{'start': 110, 'end': 221}], 'document_ids': ['1921f4f4e5a1fffbb78076282820b849-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana data understanding diterapkan ', 'filters': None, 'id': 'd4c6795dbbfe79025f1e91f9cd25336d', 'no_answer': False, 'answers': ['data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan'], 'offsets_in_documents': [{'start': 110, 'end': 221}], 'offsets_in_contexts': [{'start': 110, 'end': 221}], 'document_ids': ['1921f4f4e5a1fffbb78076282820b849-0'], 'contexts': [' data understanding yang biasa disebut dengan telaah data dilakukan untuk mendapatkan gambaran utuh atas data data understanding dilakukan setelah langkah business understanding sehingga problem bisnis sudah terselesaikan setelah awal data dipahami atau data understanding telah dilakukan  dilanjutkan dengan data preparation data yang ada belum tentu bisa langsung dipakai karena bisa jadi datanya terpisah pisah atau justru terintegrasi secara ketat  sehingga diperlukan adanya data understanding data understanding dapat memberikan gambaran awal tentang kelebihan atau kekuatan data  kekurangan dan batasan penggunaan data  tingkat kesesuaian data dengan masalah bisnis yang akan dipecahkan  serta ketersediaan data  open data  data public  biaya akses  dll']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b435d41c-23f3-4190-b51e-5818f71a115c', 'query': 'bagaimana cara mendapatkan rata rata ', 'document': {'id': '5127eb6e27ea0b20a04ca0bb462ca7e4-0', 'content': 'penyampelan bola salju merupakan suatu metode penarikan sampel dimana responden yang berhasil diperoleh diminta untuk menunjukkan responden lainnya secara berantai statistik dasar dapat digunakan untuk mendeskripsikan data  sehingga dapat memahami data terkait pusat distribusi  variasi  dan sebaran data mean atau rata rata dapat diperoleh dengan cara membagi jumlah semua bilangan dengan banyaknya bilangan dalam kumpulan simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1158, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mean atau rata rata dapat diperoleh dengan cara membagi jumlah semua bilangan dengan banyaknya bilangan dalam kumpulan ', 'type': 'extractive', 'score': 0.0, 'context': 'penyampelan bola salju merupakan suatu metode penarikan sampel dimana responden yang berhasil diperoleh diminta untuk menunjukkan responden lainnya secara berantai statistik dasar dapat digunakan untuk mendeskripsikan data  sehingga dapat memahami data terkait pusat distribusi  variasi  dan sebaran data mean atau rata rata dapat diperoleh dengan cara membagi jumlah semua bilangan dengan banyaknya bilangan dalam kumpulan simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik', 'offsets_in_document': [{'start': 305, 'end': 424}], 'offsets_in_context': [{'start': 305, 'end': 424}], 'document_ids': ['5127eb6e27ea0b20a04ca0bb462ca7e4-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mendapatkan rata rata ', 'filters': None, 'id': '3e67ff149474b071d787424fd14b9f48', 'no_answer': False, 'answers': ['mean atau rata rata dapat diperoleh dengan cara membagi jumlah semua bilangan dengan banyaknya bilangan dalam kumpulan '], 'offsets_in_documents': [{'start': 305, 'end': 424}], 'offsets_in_contexts': [{'start': 305, 'end': 424}], 'document_ids': ['5127eb6e27ea0b20a04ca0bb462ca7e4-0'], 'contexts': ['penyampelan bola salju merupakan suatu metode penarikan sampel dimana responden yang berhasil diperoleh diminta untuk menunjukkan responden lainnya secara berantai statistik dasar dapat digunakan untuk mendeskripsikan data  sehingga dapat memahami data terkait pusat distribusi  variasi  dan sebaran data mean atau rata rata dapat diperoleh dengan cara membagi jumlah semua bilangan dengan banyaknya bilangan dalam kumpulan simpangan baku adalah salah satu ukuran sebaran data jika nilai simpangan baku besar  maka data secara umum tersebar jauh dari nilai rerata aritmetik']}>,\n",
              " <MultiLabel: {'labels': [{'id': '11e11c84-236f-4b1b-aa03-5269330e6d50', 'query': 'bagaimana tujuan visualisasi dalam konteks distribusi ', 'document': {'id': 'b947c51b638d21f75a5737f9cb16a0f1-0', 'content': 'tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram distribusi juga dapat digunakan untuk melihat apakah datanya banyak terkumpul di nilai nilai kecil  berat di kanan  atau simetris  ataupun berdistribusi normal jika ingin melihat distribusi di dua variabel  maka dapat menggunakan visualisasi data scatterplot tujuan visualisasi ke   tiga yaitu komposisi atau composition digunakan untuk melihat komposisi dari suatu variabel  jika di breakdown terhadap suatu dimensi data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1163, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'untuk melihat distribusi atau persebaran data di suatu variabel ', 'type': 'extractive', 'score': 0.0, 'context': 'tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram distribusi juga dapat digunakan untuk melihat apakah datanya banyak terkumpul di nilai nilai kecil  berat di kanan  atau simetris  ataupun berdistribusi normal jika ingin melihat distribusi di dua variabel  maka dapat menggunakan visualisasi data scatterplot tujuan visualisasi ke   tiga yaitu komposisi atau composition digunakan untuk melihat komposisi dari suatu variabel  jika di breakdown terhadap suatu dimensi data', 'offsets_in_document': [{'start': 73, 'end': 137}], 'offsets_in_context': [{'start': 73, 'end': 137}], 'document_ids': ['b947c51b638d21f75a5737f9cb16a0f1-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana tujuan visualisasi dalam konteks distribusi ', 'filters': None, 'id': '6d3caf55dd512b91f55a81b51f462a3c', 'no_answer': False, 'answers': ['untuk melihat distribusi atau persebaran data di suatu variabel '], 'offsets_in_documents': [{'start': 73, 'end': 137}], 'offsets_in_contexts': [{'start': 73, 'end': 137}], 'document_ids': ['b947c51b638d21f75a5737f9cb16a0f1-0'], 'contexts': ['tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram distribusi juga dapat digunakan untuk melihat apakah datanya banyak terkumpul di nilai nilai kecil  berat di kanan  atau simetris  ataupun berdistribusi normal jika ingin melihat distribusi di dua variabel  maka dapat menggunakan visualisasi data scatterplot tujuan visualisasi ke   tiga yaitu komposisi atau composition digunakan untuk melihat komposisi dari suatu variabel  jika di breakdown terhadap suatu dimensi data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '57490f80-1914-4493-8b65-6dd8a75975b0', 'query': 'bagaimana scatter plot digunakan untuk menggambarkan hubungan antara dua variabel ', 'document': {'id': '356295bc69107bba57163b9ee564ce62-0', 'content': 'sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1172, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy ', 'type': 'extractive', 'score': 0.0, 'context': 'sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal', 'offsets_in_document': [{'start': 270, 'end': 363}], 'offsets_in_context': [{'start': 270, 'end': 363}], 'document_ids': ['356295bc69107bba57163b9ee564ce62-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana scatter plot digunakan untuk menggambarkan hubungan antara dua variabel ', 'filters': None, 'id': '54ec8359ba7fefe357153f5aab5e5e5c', 'no_answer': False, 'answers': ['masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy '], 'offsets_in_documents': [{'start': 270, 'end': 363}], 'offsets_in_contexts': [{'start': 270, 'end': 363}], 'document_ids': ['356295bc69107bba57163b9ee564ce62-0'], 'contexts': ['sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f2b1094e-371f-40de-b3b6-643dcac0ca56', 'query': 'bagaimana data direpresentasikan dalam scatter plot ', 'document': {'id': '356295bc69107bba57163b9ee564ce62-0', 'content': 'sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1172, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal', 'type': 'extractive', 'score': 0.0, 'context': 'sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal', 'offsets_in_document': [{'start': 362, 'end': 564}], 'offsets_in_context': [{'start': 362, 'end': 564}], 'document_ids': ['356295bc69107bba57163b9ee564ce62-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana data direpresentasikan dalam scatter plot ', 'filters': None, 'id': '5ad25e31a47c211df7de3245246a8b0f', 'no_answer': False, 'answers': [' sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal'], 'offsets_in_documents': [{'start': 362, 'end': 564}], 'offsets_in_contexts': [{'start': 362, 'end': 564}], 'document_ids': ['356295bc69107bba57163b9ee564ce62-0'], 'contexts': ['sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'ae8ffc69-2983-45f9-9d1b-4e8013d215e7', 'query': 'pada scatter plot dengan contoh perbandingan apel dengan pear dengan data berat g  dan diameter cm  apa informasi yang dapat diperoleh ', 'document': {'id': '8e4278b809c6201b6e06d687de009e68-0', 'content': 'sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal dalam grafik ini juga dapat memberikan informasi tentang pola data atau data pencilan 5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1177, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal dalam grafik ini juga dapat memberikan informasi tentang pola data atau data pencilan', 'type': 'extractive', 'score': 0.0, 'context': 'sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal dalam grafik ini juga dapat memberikan informasi tentang pola data atau data pencilan 5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data', 'offsets_in_document': [{'start': 0, 'end': 287}], 'offsets_in_context': [{'start': 0, 'end': 287}], 'document_ids': ['8e4278b809c6201b6e06d687de009e68-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'pada scatter plot dengan contoh perbandingan apel dengan pear dengan data berat g  dan diameter cm  apa informasi yang dapat diperoleh ', 'filters': None, 'id': 'ad75d0fb7e14f3734aaeb5a123d9a187', 'no_answer': False, 'answers': ['sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal dalam grafik ini juga dapat memberikan informasi tentang pola data atau data pencilan'], 'offsets_in_documents': [{'start': 0, 'end': 287}], 'offsets_in_contexts': [{'start': 0, 'end': 287}], 'document_ids': ['8e4278b809c6201b6e06d687de009e68-0'], 'contexts': ['sumbu x biasanya mewakili nilai abstrak yang tidak tergantung variabel lain  sehingga disebut variabel independen sedangkan untuk nilai y merupakan variabel dependen dan ditempatkan pada sumbu vertikal dalam grafik ini juga dapat memberikan informasi tentang pola data atau data pencilan 5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '17b6965a-20eb-4795-82be-3550d5f4ebda', 'query': 'apa nama aspek yang berkaitan dengan kesiapan data dalam data preparation ', 'document': {'id': '5ba925dea865cdf40fd8be67faa815e9-0', 'content': 'data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi sementara itu  data usability mempertimbangkan kesiapan data tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1160, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data usability ', 'type': 'extractive', 'score': 0.0, 'context': 'data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi sementara itu  data usability mempertimbangkan kesiapan data tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan', 'offsets_in_document': [{'start': 182, 'end': 197}], 'offsets_in_context': [{'start': 182, 'end': 197}], 'document_ids': ['5ba925dea865cdf40fd8be67faa815e9-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa nama aspek yang berkaitan dengan kesiapan data dalam data preparation ', 'filters': None, 'id': 'e81e602bce1dc99a033785f51fe3cd03', 'no_answer': False, 'answers': ['data usability '], 'offsets_in_documents': [{'start': 182, 'end': 197}], 'offsets_in_contexts': [{'start': 182, 'end': 197}], 'document_ids': ['5ba925dea865cdf40fd8be67faa815e9-0'], 'contexts': ['data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi sementara itu  data usability mempertimbangkan kesiapan data tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e1f2f930-e81c-4698-9b86-99b3df2f5654', 'query': 'apa hal yang harus dipertimbangkan dalam data usability ', 'document': {'id': '5ba925dea865cdf40fd8be67faa815e9-0', 'content': 'data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi sementara itu  data usability mempertimbangkan kesiapan data tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1160, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data usability mempertimbangkan kesiapan data', 'type': 'extractive', 'score': 0.0, 'context': 'data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi sementara itu  data usability mempertimbangkan kesiapan data tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan', 'offsets_in_document': [{'start': 182, 'end': 227}], 'offsets_in_context': [{'start': 182, 'end': 227}], 'document_ids': ['5ba925dea865cdf40fd8be67faa815e9-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa hal yang harus dipertimbangkan dalam data usability ', 'filters': None, 'id': '2c04c2cf2f77fbd46970960deb541b59', 'no_answer': False, 'answers': ['data usability mempertimbangkan kesiapan data'], 'offsets_in_documents': [{'start': 182, 'end': 227}], 'offsets_in_contexts': [{'start': 182, 'end': 227}], 'document_ids': ['5ba925dea865cdf40fd8be67faa815e9-0'], 'contexts': ['data correctness berkaitan dengan akurasi data  termasuk kebersihan dan standardisasi data data quantity mengevaluasi apakah jumlah data yang tersedia sudah mencukupi sementara itu  data usability mempertimbangkan kesiapan data tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3dca988b-aa80-4394-af1d-1cf3eba92b52', 'query': 'apa saja istilah penting dalam data understanding ', 'document': {'id': 'de302ec734a981fc3d8e5e4fbf271499-0', 'content': 'data exploration report mencakup beberapa aspek penting  yaitu hipotesis data  atribut yang  menjanjikan  untuk dianalisis lebih lanjut  eksplorasi karakteristik baru dari data  bagaimana mengubah initial hipotesis  mengidentifikasi subset dari data untuk digunakan nantinya  serta perbandingan dengan tujuan data quality report mencakup beberapa aspek penting  yaitu missing data  kesalahan data  menghitung kesalahan  atribut yang hilang dan kosong  noise  memeriksa keberadaan redudansi data  pertimbangan untuk data pencicilan   outlier  serta bagaimana data disimpan ada beberapa istilah penting dalam data understanding  yaitu dataset  data objek  dan atribut data atau dataset merupakan kumpulan dari data objek yang merepresentasikan sebuah entitas  atribut sementara itu  data objek disebut juga sebagai record  point  sample  dan instance', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1155, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'dataset  data objek  dan atribut', 'type': 'extractive', 'score': 0.0, 'context': 'data exploration report mencakup beberapa aspek penting  yaitu hipotesis data  atribut yang  menjanjikan  untuk dianalisis lebih lanjut  eksplorasi karakteristik baru dari data  bagaimana mengubah initial hipotesis  mengidentifikasi subset dari data untuk digunakan nantinya  serta perbandingan dengan tujuan data quality report mencakup beberapa aspek penting  yaitu missing data  kesalahan data  menghitung kesalahan  atribut yang hilang dan kosong  noise  memeriksa keberadaan redudansi data  pertimbangan untuk data pencicilan   outlier  serta bagaimana data disimpan ada beberapa istilah penting dalam data understanding  yaitu dataset  data objek  dan atribut data atau dataset merupakan kumpulan dari data objek yang merepresentasikan sebuah entitas  atribut sementara itu  data objek disebut juga sebagai record  point  sample  dan instance', 'offsets_in_document': [{'start': 633, 'end': 665}], 'offsets_in_context': [{'start': 633, 'end': 665}], 'document_ids': ['de302ec734a981fc3d8e5e4fbf271499-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja istilah penting dalam data understanding ', 'filters': None, 'id': '4f03c0b1c09ca44e7ff96923e0ef4b68', 'no_answer': False, 'answers': ['dataset  data objek  dan atribut'], 'offsets_in_documents': [{'start': 633, 'end': 665}], 'offsets_in_contexts': [{'start': 633, 'end': 665}], 'document_ids': ['de302ec734a981fc3d8e5e4fbf271499-0'], 'contexts': ['data exploration report mencakup beberapa aspek penting  yaitu hipotesis data  atribut yang  menjanjikan  untuk dianalisis lebih lanjut  eksplorasi karakteristik baru dari data  bagaimana mengubah initial hipotesis  mengidentifikasi subset dari data untuk digunakan nantinya  serta perbandingan dengan tujuan data quality report mencakup beberapa aspek penting  yaitu missing data  kesalahan data  menghitung kesalahan  atribut yang hilang dan kosong  noise  memeriksa keberadaan redudansi data  pertimbangan untuk data pencicilan   outlier  serta bagaimana data disimpan ada beberapa istilah penting dalam data understanding  yaitu dataset  data objek  dan atribut data atau dataset merupakan kumpulan dari data objek yang merepresentasikan sebuah entitas  atribut sementara itu  data objek disebut juga sebagai record  point  sample  dan instance']}>,\n",
              " <MultiLabel: {'labels': [{'id': '24201dca-42c4-44d1-90d0-274b7ddc455f', 'query': 'untuk menonjolkan satu variabel  visualisasi data apa yang dipilih ', 'document': {'id': '5b41fb8b4665629c4cb841df6e78308f-0', 'content': '7  box plot  boxplot menggambarkan variable variable statistik seperti quartil 1  median   quartil 2  quartil 3  nilai maksimum  nilai minimum  dan outlier pemilihan jenis visualisasi dapat disesuaikan berdasarkan dengan kebutuhannya berikut merupakan jenis visualisasi data yang dapat dipilih sesuai dengan kebutuhan yang diinginkan   berdasarkan referensi evergreen  stephanie d h 2020. effective data visualization   the right chart for the right data   yaitu  1  menunjukkan satu nilai variabel dengan satu nilai   dalam bentuk persen  contoh 78     pie chart  dan grid 2  menunjukkan peringkat atau ranking dengan bubble chart  diagram batang  slopegraph  dot  dan lolipop', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1178, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'pie chart  dan grid ', 'type': 'extractive', 'score': 0.0, 'context': '7  box plot  boxplot menggambarkan variable variable statistik seperti quartil 1  median   quartil 2  quartil 3  nilai maksimum  nilai minimum  dan outlier pemilihan jenis visualisasi dapat disesuaikan berdasarkan dengan kebutuhannya berikut merupakan jenis visualisasi data yang dapat dipilih sesuai dengan kebutuhan yang diinginkan   berdasarkan referensi evergreen  stephanie d h 2020. effective data visualization   the right chart for the right data   yaitu  1  menunjukkan satu nilai variabel dengan satu nilai   dalam bentuk persen  contoh 78     pie chart  dan grid 2  menunjukkan peringkat atau ranking dengan bubble chart  diagram batang  slopegraph  dot  dan lolipop', 'offsets_in_document': [{'start': 554, 'end': 574}], 'offsets_in_context': [{'start': 554, 'end': 574}], 'document_ids': ['5b41fb8b4665629c4cb841df6e78308f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'untuk menonjolkan satu variabel  visualisasi data apa yang dipilih ', 'filters': None, 'id': 'ca35469b53284d58b4671f3a9b5658fb', 'no_answer': False, 'answers': ['pie chart  dan grid '], 'offsets_in_documents': [{'start': 554, 'end': 574}], 'offsets_in_contexts': [{'start': 554, 'end': 574}], 'document_ids': ['5b41fb8b4665629c4cb841df6e78308f-0'], 'contexts': ['7  box plot  boxplot menggambarkan variable variable statistik seperti quartil 1  median   quartil 2  quartil 3  nilai maksimum  nilai minimum  dan outlier pemilihan jenis visualisasi dapat disesuaikan berdasarkan dengan kebutuhannya berikut merupakan jenis visualisasi data yang dapat dipilih sesuai dengan kebutuhan yang diinginkan   berdasarkan referensi evergreen  stephanie d h 2020. effective data visualization   the right chart for the right data   yaitu  1  menunjukkan satu nilai variabel dengan satu nilai   dalam bentuk persen  contoh 78     pie chart  dan grid 2  menunjukkan peringkat atau ranking dengan bubble chart  diagram batang  slopegraph  dot  dan lolipop']}>,\n",
              " <MultiLabel: {'labels': [{'id': '26d0636d-96a6-41d2-a24c-13d0ca429773', 'query': 'bagaimana cara memuat data ke pandas?', 'document': {'id': '5cc8b81066cf610a6b6348f902782326-0', 'content': 'cara memuat data ke pandas yaitu pertama menyalakan jupyter notebook di folder kerja  lalu buka atau buat baru satu skrip ipynb  import pandas dan numpy  kemudian load file csv yang sudah diunduh sebelumnya  gunakan perintah read_csv untuk menampilkan beberapa baris pertama terakhir dari data yang kita muat  gunakan perintah head   dan tail   pada dataframe dataframe dengan perintah describe   akan membantu kita untuk menampilkan statistik dasar setiap kolom data yang bertipe numerik perintah describe include  all   akan menampilkan statistik kolom yang bertipe non numerik', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1146, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'cara memuat data ke pandas yaitu pertama menyalakan jupyter notebook di folder kerja  lalu buka atau buat baru satu skrip ipynb  import pandas dan numpy  kemudian load file csv yang sudah diunduh sebelumnya', 'type': 'extractive', 'score': 0.0, 'context': 'cara memuat data ke pandas yaitu pertama menyalakan jupyter notebook di folder kerja  lalu buka atau buat baru satu skrip ipynb  import pandas dan numpy  kemudian load file csv yang sudah diunduh sebelumnya  gunakan perintah read_csv untuk menampilkan beberapa baris pertama terakhir dari data yang kita muat  gunakan perintah head   dan tail   pada dataframe dataframe dengan perintah describe   akan membantu kita untuk menampilkan statistik dasar setiap kolom data yang bertipe numerik perintah describe include  all   akan menampilkan statistik kolom yang bertipe non numerik', 'offsets_in_document': [{'start': 0, 'end': 206}], 'offsets_in_context': [{'start': 0, 'end': 206}], 'document_ids': ['5cc8b81066cf610a6b6348f902782326-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara memuat data ke pandas?', 'filters': None, 'id': '4352bb3da89fc1045a5a8062e1cbcd50', 'no_answer': False, 'answers': ['cara memuat data ke pandas yaitu pertama menyalakan jupyter notebook di folder kerja  lalu buka atau buat baru satu skrip ipynb  import pandas dan numpy  kemudian load file csv yang sudah diunduh sebelumnya'], 'offsets_in_documents': [{'start': 0, 'end': 206}], 'offsets_in_contexts': [{'start': 0, 'end': 206}], 'document_ids': ['5cc8b81066cf610a6b6348f902782326-0'], 'contexts': ['cara memuat data ke pandas yaitu pertama menyalakan jupyter notebook di folder kerja  lalu buka atau buat baru satu skrip ipynb  import pandas dan numpy  kemudian load file csv yang sudah diunduh sebelumnya  gunakan perintah read_csv untuk menampilkan beberapa baris pertama terakhir dari data yang kita muat  gunakan perintah head   dan tail   pada dataframe dataframe dengan perintah describe   akan membantu kita untuk menampilkan statistik dasar setiap kolom data yang bertipe numerik perintah describe include  all   akan menampilkan statistik kolom yang bertipe non numerik']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f31c5154-ad78-4191-9d87-90b6fccf8b8f', 'query': 'bagaimana cara mencari modus?', 'document': {'id': '689ee3e7f7acb4e749583edcfb0b7ae4-0', 'content': 'median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data modus tidak dijamin unik dalam suatu distribusi data  karena bisa ada lebih dari satu modus dalam suatu distribusi cara memuat data ke pandas yaitu pertama menyalakan jupyter notebook di folder kerja  lalu buka atau buat baru satu skrip ipynb  import pandas dan numpy  kemudian load file csv yang sudah diunduh sebelumnya  gunakan perintah read_csv untuk menampilkan beberapa baris pertama terakhir dari data yang kita muat  gunakan perintah head   dan tail   pada dataframe', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1151, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mencari nilai yang paling sering muncul pada sekumpulan data', 'type': 'extractive', 'score': 0.0, 'context': 'median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data modus tidak dijamin unik dalam suatu distribusi data  karena bisa ada lebih dari satu modus dalam suatu distribusi cara memuat data ke pandas yaitu pertama menyalakan jupyter notebook di folder kerja  lalu buka atau buat baru satu skrip ipynb  import pandas dan numpy  kemudian load file csv yang sudah diunduh sebelumnya  gunakan perintah read_csv untuk menampilkan beberapa baris pertama terakhir dari data yang kita muat  gunakan perintah head   dan tail   pada dataframe', 'offsets_in_document': [{'start': 248, 'end': 308}], 'offsets_in_context': [{'start': 248, 'end': 308}], 'document_ids': ['689ee3e7f7acb4e749583edcfb0b7ae4-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mencari modus?', 'filters': None, 'id': '3225a1c03bb3b5ea20040eadfcef18ca', 'no_answer': False, 'answers': ['mencari nilai yang paling sering muncul pada sekumpulan data'], 'offsets_in_documents': [{'start': 248, 'end': 308}], 'offsets_in_contexts': [{'start': 248, 'end': 308}], 'document_ids': ['689ee3e7f7acb4e749583edcfb0b7ae4-0'], 'contexts': ['median merupakan nilai tengah atau biasa disebut q2. iqr dapat dicari dengan rumus iqr   q3   q1. outlier adalah nilai yang lebih tinggi rendah dari 1.5 x iqr modus dipakai sebagai ukuran pusat data untuk data bertipe nominal kategoris dengan cara mencari nilai yang paling sering muncul pada sekumpulan data modus tidak dijamin unik dalam suatu distribusi data  karena bisa ada lebih dari satu modus dalam suatu distribusi cara memuat data ke pandas yaitu pertama menyalakan jupyter notebook di folder kerja  lalu buka atau buat baru satu skrip ipynb  import pandas dan numpy  kemudian load file csv yang sudah diunduh sebelumnya  gunakan perintah read_csv untuk menampilkan beberapa baris pertama terakhir dari data yang kita muat  gunakan perintah head   dan tail   pada dataframe']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'ad66410f-7bf5-4778-94e8-48fb54225b78', 'query': 'apa saja jenis sampling probabilitas?', 'document': {'id': '9134f8019001bd0bc7f46d3e907f6283-0', 'content': 'sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut pada penyampelan acak berstrata  subsampel acak sederhana ditarik dari setiap strata yang kurang lebih sama dalam beberapa karakteristik penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1141, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut pada penyampelan acak berstrata  subsampel acak sederhana ditarik dari setiap strata yang kurang lebih sama dalam beberapa karakteristik penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut', 'type': 'extractive', 'score': 0.0, 'context': 'sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut pada penyampelan acak berstrata  subsampel acak sederhana ditarik dari setiap strata yang kurang lebih sama dalam beberapa karakteristik penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut', 'offsets_in_document': [{'start': 53, 'end': 626}], 'offsets_in_context': [{'start': 53, 'end': 626}], 'document_ids': ['9134f8019001bd0bc7f46d3e907f6283-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja jenis sampling probabilitas?', 'filters': None, 'id': '23fc68545f111e64242c24d847b77287', 'no_answer': False, 'answers': ['penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut pada penyampelan acak berstrata  subsampel acak sederhana ditarik dari setiap strata yang kurang lebih sama dalam beberapa karakteristik penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut'], 'offsets_in_documents': [{'start': 53, 'end': 626}], 'offsets_in_contexts': [{'start': 53, 'end': 626}], 'document_ids': ['9134f8019001bd0bc7f46d3e907f6283-0'], 'contexts': ['sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut pada penyampelan acak berstrata  subsampel acak sederhana ditarik dari setiap strata yang kurang lebih sama dalam beberapa karakteristik penyampelan kluster adalah suatu prosedur penarikan sampel probabilitas yang memilih subpopulasi yang disebut kluster  biasanya sumber datanya sangat luas penyampelan sistematis adalah teknik pengambilan sampel berdasarkan urutan dari anggota populasi yang telah diberi nomor urut']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c5f4fb4c-6edf-4074-8e72-93379cbc9970', 'query': 'apa perbedaan bar chart dengan diagram lingkaran?', 'document': {'id': 'e7c55ef34ed1de8e12fa02682511a521-0', 'content': 'terdapat beberapa macam visualisasi dengan berbagai macam yang diantaranya pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  boxplot  dan macam   macam visualisasi lainnya dengan berbagai macam visualisasi yang ada dapat diberikan penjelasan masing   masing macam visualisasi berikut merupakan penjelasan disetiap macam visualisasi  1  pie chart  pie chart digunakan untuk menunjukkan seberapa banyak dari setiap jenis kategori dalam dataset berbanding dengan keseluruhan 2  bar chart  bar chart merupakan tools visualisasi yang dapat digunakan untuk membandingkan data kategorikal bar chart mirip dengan diagram lingkaran  namun diagram ini dapat digunakan untuk membandingkan kategori data satu sama lain', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1165, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'bar chart mirip dengan diagram lingkaran  namun diagram ini dapat digunakan untuk membandingkan kategori data satu sama lai', 'type': 'extractive', 'score': 0.0, 'context': 'terdapat beberapa macam visualisasi dengan berbagai macam yang diantaranya pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  boxplot  dan macam   macam visualisasi lainnya dengan berbagai macam visualisasi yang ada dapat diberikan penjelasan masing   masing macam visualisasi berikut merupakan penjelasan disetiap macam visualisasi  1  pie chart  pie chart digunakan untuk menunjukkan seberapa banyak dari setiap jenis kategori dalam dataset berbanding dengan keseluruhan 2  bar chart  bar chart merupakan tools visualisasi yang dapat digunakan untuk membandingkan data kategorikal bar chart mirip dengan diagram lingkaran  namun diagram ini dapat digunakan untuk membandingkan kategori data satu sama lain', 'offsets_in_document': [{'start': 601, 'end': 724}], 'offsets_in_context': [{'start': 601, 'end': 724}], 'document_ids': ['e7c55ef34ed1de8e12fa02682511a521-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa perbedaan bar chart dengan diagram lingkaran?', 'filters': None, 'id': '5fa2002bf291c0541697f65fdbc3e0f8', 'no_answer': False, 'answers': ['bar chart mirip dengan diagram lingkaran  namun diagram ini dapat digunakan untuk membandingkan kategori data satu sama lai'], 'offsets_in_documents': [{'start': 601, 'end': 724}], 'offsets_in_contexts': [{'start': 601, 'end': 724}], 'document_ids': ['e7c55ef34ed1de8e12fa02682511a521-0'], 'contexts': ['terdapat beberapa macam visualisasi dengan berbagai macam yang diantaranya pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  boxplot  dan macam   macam visualisasi lainnya dengan berbagai macam visualisasi yang ada dapat diberikan penjelasan masing   masing macam visualisasi berikut merupakan penjelasan disetiap macam visualisasi  1  pie chart  pie chart digunakan untuk menunjukkan seberapa banyak dari setiap jenis kategori dalam dataset berbanding dengan keseluruhan 2  bar chart  bar chart merupakan tools visualisasi yang dapat digunakan untuk membandingkan data kategorikal bar chart mirip dengan diagram lingkaran  namun diagram ini dapat digunakan untuk membandingkan kategori data satu sama lain']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e3962761-244a-4edf-a420-e73ee0819579', 'query': 'apa yang digambarkan pada box plot?', 'document': {'id': 'b8ac9cba95cc0f37bba8d263234ca8ed-0', 'content': 'grafik histogram digunakan untuk menunjukkan jumlah nilai atau kumpulan nilai secara seial dua histogram atau lebih juga dapat diplot dalam satu grafik salah satu contoh grafik yang dapat di implementasikan yaitu membuat dua histogram untuk atribut bore dan stroke sebuah mesin mobil 7  box plot  boxplot menggambarkan variable variable statistik seperti quartil 1  median   quartil 2  quartil 3  nilai maksimum  nilai minimum  dan outlier pemilihan jenis visualisasi dapat disesuaikan berdasarkan dengan kebutuhannya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1169, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'boxplot menggambarkan variable variable statistik seperti quartil 1  median   quartil 2  quartil 3  nilai maksimum  nilai minimum  dan outlier', 'type': 'extractive', 'score': 0.0, 'context': 'grafik histogram digunakan untuk menunjukkan jumlah nilai atau kumpulan nilai secara seial dua histogram atau lebih juga dapat diplot dalam satu grafik salah satu contoh grafik yang dapat di implementasikan yaitu membuat dua histogram untuk atribut bore dan stroke sebuah mesin mobil 7  box plot  boxplot menggambarkan variable variable statistik seperti quartil 1  median   quartil 2  quartil 3  nilai maksimum  nilai minimum  dan outlier pemilihan jenis visualisasi dapat disesuaikan berdasarkan dengan kebutuhannya', 'offsets_in_document': [{'start': 297, 'end': 439}], 'offsets_in_context': [{'start': 297, 'end': 439}], 'document_ids': ['b8ac9cba95cc0f37bba8d263234ca8ed-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang digambarkan pada box plot?', 'filters': None, 'id': 'ff9c9440928690a966ceccafb40063d4', 'no_answer': False, 'answers': ['boxplot menggambarkan variable variable statistik seperti quartil 1  median   quartil 2  quartil 3  nilai maksimum  nilai minimum  dan outlier'], 'offsets_in_documents': [{'start': 297, 'end': 439}], 'offsets_in_contexts': [{'start': 297, 'end': 439}], 'document_ids': ['b8ac9cba95cc0f37bba8d263234ca8ed-0'], 'contexts': ['grafik histogram digunakan untuk menunjukkan jumlah nilai atau kumpulan nilai secara seial dua histogram atau lebih juga dapat diplot dalam satu grafik salah satu contoh grafik yang dapat di implementasikan yaitu membuat dua histogram untuk atribut bore dan stroke sebuah mesin mobil 7  box plot  boxplot menggambarkan variable variable statistik seperti quartil 1  median   quartil 2  quartil 3  nilai maksimum  nilai minimum  dan outlier pemilihan jenis visualisasi dapat disesuaikan berdasarkan dengan kebutuhannya']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'bc9dabef-ca97-47ae-b750-ea3b053f2ccd', 'query': 'mengapa metode sampling probabilitas digunakan ketika menginginkan sampel yang akurat ', 'document': {'id': 'd35e317fd853d04fb2700c2fe73ac696-0', 'content': 'ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas untuk membuat sampel yang akurat  para peneliti menggunakan metode statistik yang teruji untuk menentukan ukuran sampel yang tepat guna memperoleh data yang terdefinisi dengan baik sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1140, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'untuk membuat sampel yang akurat  para peneliti menggunakan metode statistik yang teruji untuk menentukan ukuran sampel yang tepat guna memperoleh data yang terdefinisi dengan baik ', 'type': 'extractive', 'score': 0.0, 'context': 'ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas untuk membuat sampel yang akurat  para peneliti menggunakan metode statistik yang teruji untuk menentukan ukuran sampel yang tepat guna memperoleh data yang terdefinisi dengan baik sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut', 'offsets_in_document': [{'start': 361, 'end': 542}], 'offsets_in_context': [{'start': 361, 'end': 542}], 'document_ids': ['d35e317fd853d04fb2700c2fe73ac696-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa metode sampling probabilitas digunakan ketika menginginkan sampel yang akurat ', 'filters': None, 'id': 'de8e4c6cf28d43f3aa0dfa0c967525da', 'no_answer': False, 'answers': ['untuk membuat sampel yang akurat  para peneliti menggunakan metode statistik yang teruji untuk menentukan ukuran sampel yang tepat guna memperoleh data yang terdefinisi dengan baik '], 'offsets_in_documents': [{'start': 361, 'end': 542}], 'offsets_in_contexts': [{'start': 361, 'end': 542}], 'document_ids': ['d35e317fd853d04fb2700c2fe73ac696-0'], 'contexts': ['ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas untuk membuat sampel yang akurat  para peneliti menggunakan metode statistik yang teruji untuk menentukan ukuran sampel yang tepat guna memperoleh data yang terdefinisi dengan baik sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd66b9c7a-a812-41f5-a07c-96846240626d', 'query': 'mengapa metode sampling probabilitas digunakan ketika populasinya beragam ', 'document': {'id': 'd35e317fd853d04fb2700c2fe73ac696-0', 'content': 'ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas untuk membuat sampel yang akurat  para peneliti menggunakan metode statistik yang teruji untuk menentukan ukuran sampel yang tepat guna memperoleh data yang terdefinisi dengan baik sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1140, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll ', 'type': 'extractive', 'score': 0.0, 'context': 'ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas untuk membuat sampel yang akurat  para peneliti menggunakan metode statistik yang teruji untuk menentukan ukuran sampel yang tepat guna memperoleh data yang terdefinisi dengan baik sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut', 'offsets_in_document': [{'start': 238, 'end': 319}], 'offsets_in_context': [{'start': 238, 'end': 319}], 'document_ids': ['d35e317fd853d04fb2700c2fe73ac696-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa metode sampling probabilitas digunakan ketika populasinya beragam ', 'filters': None, 'id': '2ad22b7f9af28baf733e44ed5b5201db', 'no_answer': False, 'answers': ['membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll '], 'offsets_in_documents': [{'start': 238, 'end': 319}], 'offsets_in_contexts': [{'start': 238, 'end': 319}], 'document_ids': ['d35e317fd853d04fb2700c2fe73ac696-0'], 'contexts': ['ketika kita ingin mengurangi bias sampel  metode ini akan menghasilkan temuan yang berkualitas lebih tinggi karena memberikan representasi yang tidak bias dari populasi ketika populasi biasanya beragam  metode pengambilan sampel ini akan membantu memilih sampel dari berbagai strata sosial ekonomi  latar belakang  dll untuk mewakili populasi secara lebih luas untuk membuat sampel yang akurat  para peneliti menggunakan metode statistik yang teruji untuk menentukan ukuran sampel yang tepat guna memperoleh data yang terdefinisi dengan baik sampling probabilitas terbagi menjadi beberapa jenis penyampelan acak sederhana yaitu pengambilan anggota sampel dari populasi dilakukan secara acak tanpa memperhatikan strata yang ada dalam populasi tersebut']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'ee8978d6-123e-4548-a540-9ff2b71d1575', 'query': 'data dikelompokkan menjadi apa saja ', 'document': {'id': 'a38dce38dc4af2664ff15853adb97af9-0', 'content': 'data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia data yang berbentuk record terdiri atas data matrix  document data  dan transaction data data yang berbentuk graph dan network mencakup world wide web  molecular structures  dan social networks data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1145, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia ', 'type': 'extractive', 'score': 0.0, 'context': 'data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia data yang berbentuk record terdiri atas data matrix  document data  dan transaction data data yang berbentuk graph dan network mencakup world wide web  molecular structures  dan social networks data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data', 'offsets_in_document': [{'start': 0, 'end': 136}], 'offsets_in_context': [{'start': 0, 'end': 136}], 'document_ids': ['a38dce38dc4af2664ff15853adb97af9-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'data dikelompokkan menjadi apa saja ', 'filters': None, 'id': '6e7d971bcd9b0a954e1d58a60034e050', 'no_answer': False, 'answers': ['data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia '], 'offsets_in_documents': [{'start': 0, 'end': 136}], 'offsets_in_contexts': [{'start': 0, 'end': 136}], 'document_ids': ['a38dce38dc4af2664ff15853adb97af9-0'], 'contexts': ['data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia data yang berbentuk record terdiri atas data matrix  document data  dan transaction data data yang berbentuk graph dan network mencakup world wide web  molecular structures  dan social networks data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2ae62cbd-5a89-4e12-b21a-6b5a6dc2a9c1', 'query': 'apa informasi yang dapat diberikan oleh scatter plot ', 'document': {'id': '14264de67999f43d8f6b584a740e9091-0', 'content': 'maka dari itu  bar chart memiliki keunggulan daripada diagram lingkaran 3  line graphs  line graph adalah bentuk visualisasi lainya selain diagram lingkaran dan diagram batang diagram garis   line graphs   lebih berguna untuk menunjukkan bagaimana kemajuan data selama beberapa periode sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1166, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel', 'type': 'extractive', 'score': 0.0, 'context': 'maka dari itu  bar chart memiliki keunggulan daripada diagram lingkaran 3  line graphs  line graph adalah bentuk visualisasi lainya selain diagram lingkaran dan diagram batang diagram garis   line graphs   lebih berguna untuk menunjukkan bagaimana kemajuan data selama beberapa periode sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel', 'offsets_in_document': [{'start': 472, 'end': 555}], 'offsets_in_context': [{'start': 472, 'end': 555}], 'document_ids': ['14264de67999f43d8f6b584a740e9091-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa informasi yang dapat diberikan oleh scatter plot ', 'filters': None, 'id': '7221cb702adad423798f627b81f886ac', 'no_answer': False, 'answers': ['scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel'], 'offsets_in_documents': [{'start': 472, 'end': 555}], 'offsets_in_contexts': [{'start': 472, 'end': 555}], 'document_ids': ['14264de67999f43d8f6b584a740e9091-0'], 'contexts': ['maka dari itu  bar chart memiliki keunggulan daripada diagram lingkaran 3  line graphs  line graph adalah bentuk visualisasi lainya selain diagram lingkaran dan diagram batang diagram garis   line graphs   lebih berguna untuk menunjukkan bagaimana kemajuan data selama beberapa periode sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'edbf29c7-7441-4d2e-a02e-b4cc71fd46cc', 'query': 'apa bentuk visualisasi lainnya selain pie chart dan bar chart ', 'document': {'id': '14264de67999f43d8f6b584a740e9091-0', 'content': 'maka dari itu  bar chart memiliki keunggulan daripada diagram lingkaran 3  line graphs  line graph adalah bentuk visualisasi lainya selain diagram lingkaran dan diagram batang diagram garis   line graphs   lebih berguna untuk menunjukkan bagaimana kemajuan data selama beberapa periode sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1166, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' line graph adalah bentuk visualisasi lainya selain diagram lingkaran dan diagram batang diagram garis', 'type': 'extractive', 'score': 0.0, 'context': 'maka dari itu  bar chart memiliki keunggulan daripada diagram lingkaran 3  line graphs  line graph adalah bentuk visualisasi lainya selain diagram lingkaran dan diagram batang diagram garis   line graphs   lebih berguna untuk menunjukkan bagaimana kemajuan data selama beberapa periode sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel', 'offsets_in_document': [{'start': 87, 'end': 189}], 'offsets_in_context': [{'start': 87, 'end': 189}], 'document_ids': ['14264de67999f43d8f6b584a740e9091-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa bentuk visualisasi lainnya selain pie chart dan bar chart ', 'filters': None, 'id': '4c97fe20a7378b56534f7f82e0cd38c6', 'no_answer': False, 'answers': [' line graph adalah bentuk visualisasi lainya selain diagram lingkaran dan diagram batang diagram garis'], 'offsets_in_documents': [{'start': 87, 'end': 189}], 'offsets_in_contexts': [{'start': 87, 'end': 189}], 'document_ids': ['14264de67999f43d8f6b584a740e9091-0'], 'contexts': ['maka dari itu  bar chart memiliki keunggulan daripada diagram lingkaran 3  line graphs  line graph adalah bentuk visualisasi lainya selain diagram lingkaran dan diagram batang diagram garis   line graphs   lebih berguna untuk menunjukkan bagaimana kemajuan data selama beberapa periode sebagai contoh misalnya  grafik garis dapat berguna dalam membuat grafik berat badan balita tiap bulan  temperatur dari waktu ke waktu  harga saham dari waktu ke waktu  dan lain lain 4  scatter plot  grafik ini digunakan untuk menggambarkan hubungan antara dua variabel']}>,\n",
              " <MultiLabel: {'labels': [{'id': '472ca9b5-316c-4f8c-98fe-845a68cbf310', 'query': 'tipe metode sampling non probabilitas penyampelan kemudahan digunakan untuk apa?', 'document': {'id': '291ec3699b64c58a882fcedf2da6b8fe-0', 'content': 'sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih ada  beberapa tipe metode sampling non probabilitas penyampelan kemudahan digunakan untuk mendapatkan informasi dengan cepat  murah  dan mudah yang biasanya untuk kuisioner menguji atau penelitian eksplorasi penyampelan pertimbangan didasarkan pada kriteria kriteria tertentu  misalnya penelitian tentang kualitas makanan  sampel sumber data ahli makanan atau ahli gizi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1150, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' tipe metode sampling non probabilitas penyampelan kemudahan digunakan untuk mendapatkan informasi dengan cepat  murah  dan mudah yang biasanya untuk kuisioner menguji atau penelitian', 'type': 'extractive', 'score': 0.0, 'context': 'sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih ada  beberapa tipe metode sampling non probabilitas penyampelan kemudahan digunakan untuk mendapatkan informasi dengan cepat  murah  dan mudah yang biasanya untuk kuisioner menguji atau penelitian eksplorasi penyampelan pertimbangan didasarkan pada kriteria kriteria tertentu  misalnya penelitian tentang kualitas makanan  sampel sumber data ahli makanan atau ahli gizi', 'offsets_in_document': [{'start': 190, 'end': 373}], 'offsets_in_context': [{'start': 190, 'end': 373}], 'document_ids': ['291ec3699b64c58a882fcedf2da6b8fe-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'tipe metode sampling non probabilitas penyampelan kemudahan digunakan untuk apa?', 'filters': None, 'id': '25c088da981d404a0d2014e022ae6474', 'no_answer': False, 'answers': [' tipe metode sampling non probabilitas penyampelan kemudahan digunakan untuk mendapatkan informasi dengan cepat  murah  dan mudah yang biasanya untuk kuisioner menguji atau penelitian'], 'offsets_in_documents': [{'start': 190, 'end': 373}], 'offsets_in_contexts': [{'start': 190, 'end': 373}], 'document_ids': ['291ec3699b64c58a882fcedf2da6b8fe-0'], 'contexts': ['sementara itu  metode sampling non probabilitas adalah prosedur penarikan sampel yang bersifat subjektif setiap elemen penelitian tidak memiliki peluang yang sama untuk dipilih ada  beberapa tipe metode sampling non probabilitas penyampelan kemudahan digunakan untuk mendapatkan informasi dengan cepat  murah  dan mudah yang biasanya untuk kuisioner menguji atau penelitian eksplorasi penyampelan pertimbangan didasarkan pada kriteria kriteria tertentu  misalnya penelitian tentang kualitas makanan  sampel sumber data ahli makanan atau ahli gizi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '30415202-2370-439d-b2a8-c7e8b9efcd77', 'query': 'data matrix dan document data termasuk data bebentuk apa?', 'document': {'id': '1f7c8776c985ffc4bb790df59ac865b5-0', 'content': 'atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah sedangkan atribut numeric mencakup quantity  bilangan integer atau real   interval  ukuran skala unit  misal suhu  tanggal   dan ratio  misal panjang  harga  umur data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia data yang berbentuk record terdiri atas data matrix  document data  dan transaction data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1156, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data yang berbentuk record terdiri atas data matrix  document data  dan transaction data', 'type': 'extractive', 'score': 0.0, 'context': 'atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah sedangkan atribut numeric mencakup quantity  bilangan integer atau real   interval  ukuran skala unit  misal suhu  tanggal   dan ratio  misal panjang  harga  umur data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia data yang berbentuk record terdiri atas data matrix  document data  dan transaction data', 'offsets_in_document': [{'start': 551, 'end': 639}], 'offsets_in_context': [{'start': 551, 'end': 639}], 'document_ids': ['1f7c8776c985ffc4bb790df59ac865b5-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'data matrix dan document data termasuk data bebentuk apa?', 'filters': None, 'id': 'edca7a8396a96ab76534e29b8f1ef3a2', 'no_answer': False, 'answers': ['data yang berbentuk record terdiri atas data matrix  document data  dan transaction data'], 'offsets_in_documents': [{'start': 551, 'end': 639}], 'offsets_in_contexts': [{'start': 551, 'end': 639}], 'document_ids': ['1f7c8776c985ffc4bb790df59ac865b5-0'], 'contexts': ['atribut nominal berupa kategori  misal jenis kelamin  status perkawinan  agama  dll atribut binary merupakan atribut nominal dengan hanya 2 nilai  yaitu 0 dan 1. atribut ordinal merupakan nilai yang merepresentasikan urutan  misalkan nilai mata kuliah sedangkan atribut numeric mencakup quantity  bilangan integer atau real   interval  ukuran skala unit  misal suhu  tanggal   dan ratio  misal panjang  harga  umur data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia data yang berbentuk record terdiri atas data matrix  document data  dan transaction data']}>,\n",
              " <MultiLabel: {'labels': [{'id': '1c841d9e-6a44-49d9-9fe2-216f2ac0b7ab', 'query': 'apa yang harus dilakukan agar audiens dapat memahami informasi yang ingin disampaikan?', 'document': {'id': '594182fa296e990a5ab84480fffe549f-0', 'content': 'visualisasi yang tepat akan memudahkan pemahaman dan mengungkap pola  tren  serta hubungan yang terdapat dalam data yang kompleks dalam menginterpretasikan hasil visualisasi  penting untuk memperhatikan konteks data dan memastikan bahwa visualisasi yang dipilih memenuhi tujuan yang diinginkan untuk mengkomunikasikan informasi dengan jelas dan efektif kepada audiens maka dapat diberi kesimpulan bahwa visualisasi data adalah teknik yang penting dalam membantu kita memahami dan menganalisis data yang kompleks dengan menggunakan berbagai macam visualisasi seperti pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  box plot  dan lainnya  kita dapat mengungkap pola  tren  dan hubungan yang tersembunyi dalam data pemilihan jenis visualisasi yang tepat dan pemahaman tentang tujuan visualisasi akan memastikan bahwa informasi yang ingin disampaikan dapat dipahami dengan jelas dan efektif oleh audiens', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1161, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'pemilihan jenis visualisasi yang tepat dan pemahaman tentang tujuan visualisasi akan memastikan bahwa informasi yang ingin disampaikan dapat dipahami dengan jelas dan efektif oleh audiens', 'type': 'extractive', 'score': 0.0, 'context': 'visualisasi yang tepat akan memudahkan pemahaman dan mengungkap pola  tren  serta hubungan yang terdapat dalam data yang kompleks dalam menginterpretasikan hasil visualisasi  penting untuk memperhatikan konteks data dan memastikan bahwa visualisasi yang dipilih memenuhi tujuan yang diinginkan untuk mengkomunikasikan informasi dengan jelas dan efektif kepada audiens maka dapat diberi kesimpulan bahwa visualisasi data adalah teknik yang penting dalam membantu kita memahami dan menganalisis data yang kompleks dengan menggunakan berbagai macam visualisasi seperti pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  box plot  dan lainnya  kita dapat mengungkap pola  tren  dan hubungan yang tersembunyi dalam data pemilihan jenis visualisasi yang tepat dan pemahaman tentang tujuan visualisasi akan memastikan bahwa informasi yang ingin disampaikan dapat dipahami dengan jelas dan efektif oleh audiens', 'offsets_in_document': [{'start': 733, 'end': 920}], 'offsets_in_context': [{'start': 733, 'end': 920}], 'document_ids': ['594182fa296e990a5ab84480fffe549f-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang harus dilakukan agar audiens dapat memahami informasi yang ingin disampaikan?', 'filters': None, 'id': '3514c3d28151c89c44cacb7b4d40dc93', 'no_answer': False, 'answers': ['pemilihan jenis visualisasi yang tepat dan pemahaman tentang tujuan visualisasi akan memastikan bahwa informasi yang ingin disampaikan dapat dipahami dengan jelas dan efektif oleh audiens'], 'offsets_in_documents': [{'start': 733, 'end': 920}], 'offsets_in_contexts': [{'start': 733, 'end': 920}], 'document_ids': ['594182fa296e990a5ab84480fffe549f-0'], 'contexts': ['visualisasi yang tepat akan memudahkan pemahaman dan mengungkap pola  tren  serta hubungan yang terdapat dalam data yang kompleks dalam menginterpretasikan hasil visualisasi  penting untuk memperhatikan konteks data dan memastikan bahwa visualisasi yang dipilih memenuhi tujuan yang diinginkan untuk mengkomunikasikan informasi dengan jelas dan efektif kepada audiens maka dapat diberi kesimpulan bahwa visualisasi data adalah teknik yang penting dalam membantu kita memahami dan menganalisis data yang kompleks dengan menggunakan berbagai macam visualisasi seperti pie chart  bar chart  line graphs  scatter plot  heatmap  histogram  box plot  dan lainnya  kita dapat mengungkap pola  tren  dan hubungan yang tersembunyi dalam data pemilihan jenis visualisasi yang tepat dan pemahaman tentang tujuan visualisasi akan memastikan bahwa informasi yang ingin disampaikan dapat dipahami dengan jelas dan efektif oleh audiens']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3ef77d80-95b9-4e04-bfde-f991cfde6974', 'query': 'apa akibat dari adanya outlier?', 'document': {'id': 'f4fd02e54d185179ca336106d715861e-0', 'content': 'outlier dapat disebabkan oleh data entry error  measurement error  sampling error  data corruption  serta pengamatan outlier yang sebenarnya keberadaan outlier dapat menyebabkan masalah saat memodelkan  terutama dalam model linear  dan dapat mempengaruhi evaluasi seperti perhitungan mse  mean squared error statistics based outlier detection techniques mengasumsikan bahwa normal data point akan muncul pada area dengan probabilitas tinggi  sedangkan outlier biasanya muncul pada area dengan probabilitas rendah pada model stokastik beberapa cara untuk mengidentifikasi outlier yaitu menggunakan standard deviation method  interquartile range method  dan automatic outlier detection metode pertama adalah standard deviation method  yang cocok digunakan pada sample dengan distribusi nilai gaussian atau mendekati distribusi gaussian', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1180, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' outlier dapat menyebabkan masalah saat memodelkan  terutama dalam model linear  dan dapat mempengaruhi evaluasi seperti perhitungan mse  mean squared error statistics based outlier detection techniques', 'type': 'extractive', 'score': 0.0, 'context': 'outlier dapat disebabkan oleh data entry error  measurement error  sampling error  data corruption  serta pengamatan outlier yang sebenarnya keberadaan outlier dapat menyebabkan masalah saat memodelkan  terutama dalam model linear  dan dapat mempengaruhi evaluasi seperti perhitungan mse  mean squared error statistics based outlier detection techniques mengasumsikan bahwa normal data point akan muncul pada area dengan probabilitas tinggi  sedangkan outlier biasanya muncul pada area dengan probabilitas rendah pada model stokastik beberapa cara untuk mengidentifikasi outlier yaitu menggunakan standard deviation method  interquartile range method  dan automatic outlier detection metode pertama adalah standard deviation method  yang cocok digunakan pada sample dengan distribusi nilai gaussian atau mendekati distribusi gaussian', 'offsets_in_document': [{'start': 151, 'end': 353}], 'offsets_in_context': [{'start': 151, 'end': 353}], 'document_ids': ['f4fd02e54d185179ca336106d715861e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa akibat dari adanya outlier?', 'filters': None, 'id': '23d6daae432c33a074d920ab9b5a8d7f', 'no_answer': False, 'answers': [' outlier dapat menyebabkan masalah saat memodelkan  terutama dalam model linear  dan dapat mempengaruhi evaluasi seperti perhitungan mse  mean squared error statistics based outlier detection techniques'], 'offsets_in_documents': [{'start': 151, 'end': 353}], 'offsets_in_contexts': [{'start': 151, 'end': 353}], 'document_ids': ['f4fd02e54d185179ca336106d715861e-0'], 'contexts': ['outlier dapat disebabkan oleh data entry error  measurement error  sampling error  data corruption  serta pengamatan outlier yang sebenarnya keberadaan outlier dapat menyebabkan masalah saat memodelkan  terutama dalam model linear  dan dapat mempengaruhi evaluasi seperti perhitungan mse  mean squared error statistics based outlier detection techniques mengasumsikan bahwa normal data point akan muncul pada area dengan probabilitas tinggi  sedangkan outlier biasanya muncul pada area dengan probabilitas rendah pada model stokastik beberapa cara untuk mengidentifikasi outlier yaitu menggunakan standard deviation method  interquartile range method  dan automatic outlier detection metode pertama adalah standard deviation method  yang cocok digunakan pada sample dengan distribusi nilai gaussian atau mendekati distribusi gaussian']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c2c08478-b116-493f-8514-c1f52845dbfe', 'query': 'hal apa yang melatarbelakangi reduksi dimensi ', 'document': {'id': '7d7a6d0684af498dccf39bdd614dc3b-0', 'content': 'reduksi dimensi menjadi solusi untuk permasalahan dimana ketika dimensi data meningkat  performa classifier juga meningkat hingga jumlah fitur yang optimal tertentu dan jika dimensi data terus ditambah  tanpa ada peningkatan jumlah sampel pelatihan  maka performa classifier juga akan menurun reduksi dimensi adalah proses mengurangi dimensi data ke ruang dimensi yang lebih rendah  namun tetap mempertahankan karakteristik asli dari data tersebut teknik reduksi dimensi dibagi menjadi seleksi fitur dan ekstraksi fitur seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1179, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'reduksi dimensi menjadi solusi untuk permasalahan dimana ketika dimensi data meningkat  performa classifier juga meningkat hingga jumlah fitur yang optimal tertentu dan jika dimensi data terus ditambah  tanpa ada peningkatan jumlah sampel pelatihan  maka performa classifier juga akan menurun ', 'type': 'extractive', 'score': 0.0, 'context': 'reduksi dimensi menjadi solusi untuk permasalahan dimana ketika dimensi data meningkat  performa classifier juga meningkat hingga jumlah fitur yang optimal tertentu dan jika dimensi data terus ditambah  tanpa ada peningkatan jumlah sampel pelatihan  maka performa classifier juga akan menurun reduksi dimensi adalah proses mengurangi dimensi data ke ruang dimensi yang lebih rendah  namun tetap mempertahankan karakteristik asli dari data tersebut teknik reduksi dimensi dibagi menjadi seleksi fitur dan ekstraksi fitur seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya', 'offsets_in_document': [{'start': 0, 'end': 293}], 'offsets_in_context': [{'start': 0, 'end': 293}], 'document_ids': ['7d7a6d0684af498dccf39bdd614dc3b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'hal apa yang melatarbelakangi reduksi dimensi ', 'filters': None, 'id': 'fde9573426d97b1b14c98ae46a5d2ac2', 'no_answer': False, 'answers': ['reduksi dimensi menjadi solusi untuk permasalahan dimana ketika dimensi data meningkat  performa classifier juga meningkat hingga jumlah fitur yang optimal tertentu dan jika dimensi data terus ditambah  tanpa ada peningkatan jumlah sampel pelatihan  maka performa classifier juga akan menurun '], 'offsets_in_documents': [{'start': 0, 'end': 293}], 'offsets_in_contexts': [{'start': 0, 'end': 293}], 'document_ids': ['7d7a6d0684af498dccf39bdd614dc3b-0'], 'contexts': ['reduksi dimensi menjadi solusi untuk permasalahan dimana ketika dimensi data meningkat  performa classifier juga meningkat hingga jumlah fitur yang optimal tertentu dan jika dimensi data terus ditambah  tanpa ada peningkatan jumlah sampel pelatihan  maka performa classifier juga akan menurun reduksi dimensi adalah proses mengurangi dimensi data ke ruang dimensi yang lebih rendah  namun tetap mempertahankan karakteristik asli dari data tersebut teknik reduksi dimensi dibagi menjadi seleksi fitur dan ekstraksi fitur seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya']}>,\n",
              " <MultiLabel: {'labels': [{'id': '5fcb0b3b-6773-47d6-b965-1151996c8042', 'query': 'bagaimana peran penyampelan dalam data understanding ', 'document': {'id': '3bd6137a94c09a217f87282769c07a1-0', 'content': 'populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi penyampelan bertujuan untuk memperoleh informasi tentang populasi ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1157, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'penyampelan bertujuan untuk memperoleh informasi tentang populasi ', 'type': 'extractive', 'score': 0.0, 'context': 'populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi penyampelan bertujuan untuk memperoleh informasi tentang populasi ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi', 'offsets_in_document': [{'start': 190, 'end': 256}], 'offsets_in_context': [{'start': 190, 'end': 256}], 'document_ids': ['3bd6137a94c09a217f87282769c07a1-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana peran penyampelan dalam data understanding ', 'filters': None, 'id': 'ad46f1c91420c5e9352917f16f28d74d', 'no_answer': False, 'answers': ['penyampelan bertujuan untuk memperoleh informasi tentang populasi '], 'offsets_in_documents': [{'start': 190, 'end': 256}], 'offsets_in_contexts': [{'start': 190, 'end': 256}], 'document_ids': ['3bd6137a94c09a217f87282769c07a1-0'], 'contexts': ['populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi penyampelan bertujuan untuk memperoleh informasi tentang populasi ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': '9fabceef-9088-48ea-a998-b717a3efb288', 'query': 'populasi pada data understanding mengacu pada apa ', 'document': {'id': '3bd6137a94c09a217f87282769c07a1-0', 'content': 'populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi penyampelan bertujuan untuk memperoleh informasi tentang populasi ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1157, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya ', 'type': 'extractive', 'score': 0.0, 'context': 'populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi penyampelan bertujuan untuk memperoleh informasi tentang populasi ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi', 'offsets_in_document': [{'start': 0, 'end': 83}], 'offsets_in_context': [{'start': 0, 'end': 83}], 'document_ids': ['3bd6137a94c09a217f87282769c07a1-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'populasi pada data understanding mengacu pada apa ', 'filters': None, 'id': 'fce8d1c2a84de7063e6374e67f518676', 'no_answer': False, 'answers': ['populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya '], 'offsets_in_documents': [{'start': 0, 'end': 83}], 'offsets_in_contexts': [{'start': 0, 'end': 83}], 'document_ids': ['3bd6137a94c09a217f87282769c07a1-0'], 'contexts': ['populasi bukan hanya mengacu pada orang  tetapi bisa objek atau benda alam lainnya sementara itu  sampel merupakan bagian dari populasi  mencakup sejumlah anggota yang dipilih dari populasi penyampelan bertujuan untuk memperoleh informasi tentang populasi ada dua metode sampling  yaitu probabilitas dan non probabilitas metode sampling probabilitas adalah suatu prosedur dimana probabilitas pemilihan diketahui terlebih dulu untuk setiap unit atau elemen populasi']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b277b1fa-be22-4bfd-8e48-ad29c61dc6b7', 'query': 'apakah tujuan memvisualisasi data bagi user ', 'document': {'id': '46b7e44f8d927ed59def62f77d7c83da-0', 'content': ' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1162, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks', 'type': 'extractive', 'score': 0.0, 'context': ' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram', 'offsets_in_document': [{'start': 174, 'end': 295}], 'offsets_in_context': [{'start': 174, 'end': 295}], 'document_ids': ['46b7e44f8d927ed59def62f77d7c83da-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apakah tujuan memvisualisasi data bagi user ', 'filters': None, 'id': '157b08a5425c90c041292bb57a424e84', 'no_answer': False, 'answers': ['membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks'], 'offsets_in_documents': [{'start': 174, 'end': 295}], 'offsets_in_contexts': [{'start': 174, 'end': 295}], 'document_ids': ['46b7e44f8d927ed59def62f77d7c83da-0'], 'contexts': [' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram']}>,\n",
              " <MultiLabel: {'labels': [{'id': '9f2a6fa5-f8bf-4cd0-b997-ed157d93a94f', 'query': 'apa saja tujuan dari visualisasi data ', 'document': {'id': '46b7e44f8d927ed59def62f77d7c83da-0', 'content': ' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1162, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram', 'type': 'extractive', 'score': 0.0, 'context': ' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram', 'offsets_in_document': [{'start': 296, 'end': 908}], 'offsets_in_context': [{'start': 296, 'end': 908}], 'document_ids': ['46b7e44f8d927ed59def62f77d7c83da-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja tujuan dari visualisasi data ', 'filters': None, 'id': '428a992745b8c7dcc10405220a108052', 'no_answer': False, 'answers': ['visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram'], 'offsets_in_documents': [{'start': 296, 'end': 908}], 'offsets_in_contexts': [{'start': 296, 'end': 908}], 'document_ids': ['46b7e44f8d927ed59def62f77d7c83da-0'], 'contexts': [' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b0ed23bb-e123-4693-9fe0-a1772e6c924d', 'query': 'barchart digunakan untuk apa ', 'document': {'id': '46b7e44f8d927ed59def62f77d7c83da-0', 'content': ' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1162, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'untuk data kategorikal ', 'type': 'extractive', 'score': 0.0, 'context': ' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram', 'offsets_in_document': [{'start': 627, 'end': 650}], 'offsets_in_context': [{'start': 627, 'end': 650}], 'document_ids': ['46b7e44f8d927ed59def62f77d7c83da-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'barchart digunakan untuk apa ', 'filters': None, 'id': 'bec026ad442a88c9d8262cbf8ba55afa', 'no_answer': False, 'answers': ['untuk data kategorikal '], 'offsets_in_documents': [{'start': 627, 'end': 650}], 'offsets_in_contexts': [{'start': 627, 'end': 650}], 'document_ids': ['46b7e44f8d927ed59def62f77d7c83da-0'], 'contexts': [' visualisasi merupakan suatu teknik dalam pembuatan grafik atau diagram untuk menampilkan atau merepresentasikan secara visual sebuah informasi dengan memvisualisasikan data membantu pengguna untuk memahami pola  tren  hubungan  dan outlier yang tersembunyi di dalam data yang besar dan kompleks visualisasi sendiri memiliki empat tujuan antara lain pertama perbandingan atau comparison digunakan untuk membandingkan suatu sekumpulan nilai dengan nilai lainnya  mana yang lebih besar  lebih tinggi  mana yang naik  dsb untuk data yang berkaitan dengan waktu maka visualisasi yang digunakan biasanya berupa line chart sedangkan untuk data kategorikal maka lebih cocok menggunakan bar chart tujuan visualisasi ke   dua yaitu distribusi atau distribution digunakan untuk melihat distribusi atau persebaran data di suatu variabel visualisasi yang lazim digunakan pada distribusi adalah visualisasi data histogram']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3b870028-8d94-486a-8805-d967c177b2f9', 'query': 'apa yang dapat digunakan untuk dapat membuat histogram dengan mudah ', 'document': {'id': '98de1c941764592b91dcdde8fd17da9b-0', 'content': '5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data untuk mengimplementasikannya dalam bahasa python  biasanya menggunakan library seaborn 6  histogram  histogram adalah salah satu visualisasi yang dapat digunakan untuk memahami distribusi pada data pandas histogram menyediakan metode yang memudahkan kita untuk membuat histogram', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1167, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'pandas histogram ', 'type': 'extractive', 'score': 0.0, 'context': '5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data untuk mengimplementasikannya dalam bahasa python  biasanya menggunakan library seaborn 6  histogram  histogram adalah salah satu visualisasi yang dapat digunakan untuk memahami distribusi pada data pandas histogram menyediakan metode yang memudahkan kita untuk membuat histogram', 'offsets_in_document': [{'start': 500, 'end': 517}], 'offsets_in_context': [{'start': 500, 'end': 517}], 'document_ids': ['98de1c941764592b91dcdde8fd17da9b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dapat digunakan untuk dapat membuat histogram dengan mudah ', 'filters': None, 'id': '89ddb07afe26b5b15586a1d1647344b6', 'no_answer': False, 'answers': ['pandas histogram '], 'offsets_in_documents': [{'start': 500, 'end': 517}], 'offsets_in_contexts': [{'start': 500, 'end': 517}], 'document_ids': ['98de1c941764592b91dcdde8fd17da9b-0'], 'contexts': ['5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data untuk mengimplementasikannya dalam bahasa python  biasanya menggunakan library seaborn 6  histogram  histogram adalah salah satu visualisasi yang dapat digunakan untuk memahami distribusi pada data pandas histogram menyediakan metode yang memudahkan kita untuk membuat histogram']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'df4fb83d-cc1a-4eaa-b6d7-bde9a88b6ba9', 'query': 'apa yang dimaksud heatmap ', 'document': {'id': '98de1c941764592b91dcdde8fd17da9b-0', 'content': '5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data untuk mengimplementasikannya dalam bahasa python  biasanya menggunakan library seaborn 6  histogram  histogram adalah salah satu visualisasi yang dapat digunakan untuk memahami distribusi pada data pandas histogram menyediakan metode yang memudahkan kita untuk membuat histogram', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1167, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna', 'type': 'extractive', 'score': 0.0, 'context': '5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data untuk mengimplementasikannya dalam bahasa python  biasanya menggunakan library seaborn 6  histogram  histogram adalah salah satu visualisasi yang dapat digunakan untuk memahami distribusi pada data pandas histogram menyediakan metode yang memudahkan kita untuk membuat histogram', 'offsets_in_document': [{'start': 11, 'end': 145}], 'offsets_in_context': [{'start': 11, 'end': 145}], 'document_ids': ['98de1c941764592b91dcdde8fd17da9b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud heatmap ', 'filters': None, 'id': '3894e58659708d9d14a6dfb5407b7cd7', 'no_answer': False, 'answers': [' heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna'], 'offsets_in_documents': [{'start': 11, 'end': 145}], 'offsets_in_contexts': [{'start': 11, 'end': 145}], 'document_ids': ['98de1c941764592b91dcdde8fd17da9b-0'], 'contexts': ['5  heatmap  heatmap adalah jenis visualisasi yang menggunakan kode warna untuk mewakili nilai   kepadatan relatif data di seluruh permukaan warna   warna pada heatmap kemudian dapat digunakan untuk memeriksa data secara visual guna menemukan kelompok dengan nilai serupa dan mendeteksi tren dalam data untuk mengimplementasikannya dalam bahasa python  biasanya menggunakan library seaborn 6  histogram  histogram adalah salah satu visualisasi yang dapat digunakan untuk memahami distribusi pada data pandas histogram menyediakan metode yang memudahkan kita untuk membuat histogram']}>,\n",
              " <MultiLabel: {'labels': [{'id': '95bbab4e-bf9c-4b8e-9b65-ec2fac63a055', 'query': 'apa nama algoritma yang biasa digunakan pada iterative imputation ', 'document': {'id': '7a628483944103e982d20bbd37700a74-0', 'content': 'disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur algoritma ini sering digunakan karena kesederhanannya untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1190, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'algoritma regresi ', 'type': 'extractive', 'score': 0.0, 'context': 'disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur algoritma ini sering digunakan karena kesederhanannya untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus', 'offsets_in_document': [{'start': 238, 'end': 256}], 'offsets_in_context': [{'start': 238, 'end': 256}], 'document_ids': ['7a628483944103e982d20bbd37700a74-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa nama algoritma yang biasa digunakan pada iterative imputation ', 'filters': None, 'id': '599a2531536c398a4edfbcfcc80d3fee', 'no_answer': False, 'answers': ['algoritma regresi '], 'offsets_in_documents': [{'start': 238, 'end': 256}], 'offsets_in_contexts': [{'start': 238, 'end': 256}], 'document_ids': ['7a628483944103e982d20bbd37700a74-0'], 'contexts': ['disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur algoritma ini sering digunakan karena kesederhanannya untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f4807767-ceb2-4278-8c0a-1459bf336950', 'query': 'mengapa algoritma regresi yang berbeda bisa digunakan dalam iterative imputation ', 'document': {'id': '7a628483944103e982d20bbd37700a74-0', 'content': 'disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur algoritma ini sering digunakan karena kesederhanannya untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1190, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'karena kesederhanannya untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value ', 'type': 'extractive', 'score': 0.0, 'context': 'disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur algoritma ini sering digunakan karena kesederhanannya untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus', 'offsets_in_document': [{'start': 369, 'end': 611}], 'offsets_in_context': [{'start': 369, 'end': 611}], 'document_ids': ['7a628483944103e982d20bbd37700a74-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa algoritma regresi yang berbeda bisa digunakan dalam iterative imputation ', 'filters': None, 'id': '4f2c480b2a49bdd018f7cf89b4bd1faf', 'no_answer': False, 'answers': ['karena kesederhanannya untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value '], 'offsets_in_documents': [{'start': 369, 'end': 611}], 'offsets_in_contexts': [{'start': 369, 'end': 611}], 'document_ids': ['7a628483944103e982d20bbd37700a74-0'], 'contexts': ['disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur algoritma ini sering digunakan karena kesederhanannya untuk menghapus kolom baris berisi missing value yaitu menggunakan fungsi dropna   secara langsung berpotensi kehilangan banyak informasi penting karena baris kolom akan dihapus  meskipun hanya terdapat 1 missing value akan tetapi  terdapat langkah alternatif yang dapat dilakukan yaitu menghitung jumlah missing value pada setiap kolom  kolom yang memiliki hampir lebih dari separuh missing value dapat dihapus  dan jika ada beberapa baris data saja yang terdapat missing value juga dapat dihapus']}>,\n",
              " <MultiLabel: {'labels': [{'id': '5b95bf15-9ee8-4dfd-9fa0-65d56360aa0e', 'query': 'apa manfaat dari pemilihan visualisasi yang sesuai?', 'document': {'id': '835833f8bb965709ca21287505cd5ab8-0', 'content': '10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar dengan berbagai macam jenis visualisasi yang ada  kita dapat memilih visualisasi yang paling sesuai untuk data yang ingin dijelaskan atau dianalisis visualisasi yang tepat akan memudahkan pemahaman dan mengungkap pola  tren  serta hubungan yang terdapat dalam data yang kompleks dalam menginterpretasikan hasil visualisasi  penting untuk memperhatikan konteks data dan memastikan bahwa visualisasi yang dipilih memenuhi tujuan yang diinginkan untuk mengkomunikasikan informasi dengan jelas dan efektif kepada audiens', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1173, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'memilih visualisasi yang paling sesuai untuk data yang ingin dijelaskan atau dianalisis visualisasi yang tepat akan memudahkan pemahaman dan mengungkap pola  tren  serta hubungan yang terdapat dalam data yang kompleks', 'type': 'extractive', 'score': 0.0, 'context': '10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar dengan berbagai macam jenis visualisasi yang ada  kita dapat memilih visualisasi yang paling sesuai untuk data yang ingin dijelaskan atau dianalisis visualisasi yang tepat akan memudahkan pemahaman dan mengungkap pola  tren  serta hubungan yang terdapat dalam data yang kompleks dalam menginterpretasikan hasil visualisasi  penting untuk memperhatikan konteks data dan memastikan bahwa visualisasi yang dipilih memenuhi tujuan yang diinginkan untuk mengkomunikasikan informasi dengan jelas dan efektif kepada audiens', 'offsets_in_document': [{'start': 259, 'end': 476}], 'offsets_in_context': [{'start': 259, 'end': 476}], 'document_ids': ['835833f8bb965709ca21287505cd5ab8-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa manfaat dari pemilihan visualisasi yang sesuai?', 'filters': None, 'id': 'c0c869e8fd4320d2cd83b0b282f95902', 'no_answer': False, 'answers': ['memilih visualisasi yang paling sesuai untuk data yang ingin dijelaskan atau dianalisis visualisasi yang tepat akan memudahkan pemahaman dan mengungkap pola  tren  serta hubungan yang terdapat dalam data yang kompleks'], 'offsets_in_documents': [{'start': 259, 'end': 476}], 'offsets_in_contexts': [{'start': 259, 'end': 476}], 'document_ids': ['835833f8bb965709ca21287505cd5ab8-0'], 'contexts': ['10  menggunakan teks sebagai data utama dengan heatmap  word cloud  dan teks 11  menampilkan perubahan nilai dari waktu ke waktu atau tren dengan slopegraph  line chart  dot plot  dan deviation bar dengan berbagai macam jenis visualisasi yang ada  kita dapat memilih visualisasi yang paling sesuai untuk data yang ingin dijelaskan atau dianalisis visualisasi yang tepat akan memudahkan pemahaman dan mengungkap pola  tren  serta hubungan yang terdapat dalam data yang kompleks dalam menginterpretasikan hasil visualisasi  penting untuk memperhatikan konteks data dan memastikan bahwa visualisasi yang dipilih memenuhi tujuan yang diinginkan untuk mengkomunikasikan informasi dengan jelas dan efektif kepada audiens']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0b3e048d-4530-4a0b-bde3-ef64638c75f0', 'query': 'bagaimana jika nilai unik sebuah kolom hanya 1?', 'document': {'id': 'db406de0e9cafafe1d9f02caf36a95b4-0', 'content': 'jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik besarnya variance threshold dapat diuji coba dengan beberapa nilai melalui eksperimen untuk mendapatkan hasil yang terbaik mengidentifikasi dan hapus duplikasi baris data yang dapat terjadi saat pengumpulan data dari beberapa sumber atau pengulangan observasi eksperimen duplikasi baris tidak memberikan informasi tambahan dan dapat mengakibatkan bias pada evaluasi model', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1184, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling', 'type': 'extractive', 'score': 0.0, 'context': 'jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik besarnya variance threshold dapat diuji coba dengan beberapa nilai melalui eksperimen untuk mendapatkan hasil yang terbaik mengidentifikasi dan hapus duplikasi baris data yang dapat terjadi saat pengumpulan data dari beberapa sumber atau pengulangan observasi eksperimen duplikasi baris tidak memberikan informasi tambahan dan dapat mengakibatkan bias pada evaluasi model', 'offsets_in_document': [{'start': 0, 'end': 90}], 'offsets_in_context': [{'start': 0, 'end': 90}], 'document_ids': ['db406de0e9cafafe1d9f02caf36a95b4-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana jika nilai unik sebuah kolom hanya 1?', 'filters': None, 'id': '664a51dc90c2e87f7d9fbfb4faaea3bf', 'no_answer': False, 'answers': ['jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling'], 'offsets_in_documents': [{'start': 0, 'end': 90}], 'offsets_in_contexts': [{'start': 0, 'end': 90}], 'document_ids': ['db406de0e9cafafe1d9f02caf36a95b4-0'], 'contexts': ['jika nilai unik pada sebuah kolom   1  kolom tersebut tidak berguna untuk proses modelling mengidentifikasi dan hapus kolom dengan low variance  yaitu kolom yang memiliki sedikit nilai unik dan variance mendekati nol  terutama pada atribut numerik besarnya variance threshold dapat diuji coba dengan beberapa nilai melalui eksperimen untuk mendapatkan hasil yang terbaik mengidentifikasi dan hapus duplikasi baris data yang dapat terjadi saat pengumpulan data dari beberapa sumber atau pengulangan observasi eksperimen duplikasi baris tidak memberikan informasi tambahan dan dapat mengakibatkan bias pada evaluasi model']}>,\n",
              " <MultiLabel: {'labels': [{'id': '6bd3627b-423a-4ae1-bc50-fec143665504', 'query': 'apa sebutan lain untuk discretization ', 'document': {'id': '2491e4b24ed74013da4c4542dbfbb5e5-0', 'content': 'discretization disebut juga sebagai binning atau k bins  dengan k adalah jumlah kelompok hasil mapping variabel numerik terdapat 3 jenis strategi dalam discretization yaitu uniform  quantile  dan clustered uniform discretization transform berarti setiap bin memiliki lebar yang sama dalam rentang nilai yang mungkin untuk sebuah variabel  transformasi ini akan mempertahankan distribusi probabilitas dari setiap input variabel dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  uniform   parameter encode  ordinal  dan menentukan jumlah bin pada parameter n_bin k means cluster discretization transform dilakukan dengan cara membentuk k cluster untuk setiap variabel input  lalu memasukkan setiap nilai ke sebuah kluster yang sesuai', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1200, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'discretization disebut juga sebagai binning atau k bins', 'type': 'extractive', 'score': 0.0, 'context': 'discretization disebut juga sebagai binning atau k bins  dengan k adalah jumlah kelompok hasil mapping variabel numerik terdapat 3 jenis strategi dalam discretization yaitu uniform  quantile  dan clustered uniform discretization transform berarti setiap bin memiliki lebar yang sama dalam rentang nilai yang mungkin untuk sebuah variabel  transformasi ini akan mempertahankan distribusi probabilitas dari setiap input variabel dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  uniform   parameter encode  ordinal  dan menentukan jumlah bin pada parameter n_bin k means cluster discretization transform dilakukan dengan cara membentuk k cluster untuk setiap variabel input  lalu memasukkan setiap nilai ke sebuah kluster yang sesuai', 'offsets_in_document': [{'start': 0, 'end': 55}], 'offsets_in_context': [{'start': 0, 'end': 55}], 'document_ids': ['2491e4b24ed74013da4c4542dbfbb5e5-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa sebutan lain untuk discretization ', 'filters': None, 'id': 'ea659fab638bad0ef964ef7fb1d240e0', 'no_answer': False, 'answers': ['discretization disebut juga sebagai binning atau k bins'], 'offsets_in_documents': [{'start': 0, 'end': 55}], 'offsets_in_contexts': [{'start': 0, 'end': 55}], 'document_ids': ['2491e4b24ed74013da4c4542dbfbb5e5-0'], 'contexts': ['discretization disebut juga sebagai binning atau k bins  dengan k adalah jumlah kelompok hasil mapping variabel numerik terdapat 3 jenis strategi dalam discretization yaitu uniform  quantile  dan clustered uniform discretization transform berarti setiap bin memiliki lebar yang sama dalam rentang nilai yang mungkin untuk sebuah variabel  transformasi ini akan mempertahankan distribusi probabilitas dari setiap input variabel dapat diimplementasikan menggunakan class kbinsdiscretizer dari library skylearn dengan mengeset parameter strategy  uniform   parameter encode  ordinal  dan menentukan jumlah bin pada parameter n_bin k means cluster discretization transform dilakukan dengan cara membentuk k cluster untuk setiap variabel input  lalu memasukkan setiap nilai ke sebuah kluster yang sesuai']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'a1665403-9522-494d-9937-29edeb3eda29', 'query': 'apa tujuan dari proses reduksi dimensi ', 'document': {'id': '2a00f533a90744bcde0477fdaf124185-0', 'content': 'data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis sedangkan reduksi dimensi bertujuan mengurangi dimensi data yang terlalu besar untuk efisiensi dalam proses analisis dan mendapatkan hasil yang lebih baik proses data preparation dapat bervariasi karena tergantung pada tujuan analisis  keahlian pengguna data  dan pertanyaan yang ingin dijawab aspek dalam data preparation meliputi data completeness  data correctness  data quantity  dan data usability', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1187, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'reduksi dimensi bertujuan mengurangi dimensi data yang terlalu besar untuk efisiensi dalam proses analisis dan mendapatkan hasil yang lebih baik ', 'type': 'extractive', 'score': 0.0, 'context': 'data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis sedangkan reduksi dimensi bertujuan mengurangi dimensi data yang terlalu besar untuk efisiensi dalam proses analisis dan mendapatkan hasil yang lebih baik proses data preparation dapat bervariasi karena tergantung pada tujuan analisis  keahlian pengguna data  dan pertanyaan yang ingin dijawab aspek dalam data preparation meliputi data completeness  data correctness  data quantity  dan data usability', 'offsets_in_document': [{'start': 244, 'end': 389}], 'offsets_in_context': [{'start': 244, 'end': 389}], 'document_ids': ['2a00f533a90744bcde0477fdaf124185-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa tujuan dari proses reduksi dimensi ', 'filters': None, 'id': '03d3a5be36b0f3ce805c7f8b81e63e26', 'no_answer': False, 'answers': ['reduksi dimensi bertujuan mengurangi dimensi data yang terlalu besar untuk efisiensi dalam proses analisis dan mendapatkan hasil yang lebih baik '], 'offsets_in_documents': [{'start': 244, 'end': 389}], 'offsets_in_contexts': [{'start': 244, 'end': 389}], 'document_ids': ['2a00f533a90744bcde0477fdaf124185-0'], 'contexts': ['data cleaning melibatkan identifikasi dan perbaikan error dalam dataset  seperti menghapus duplikasi  menangani missing data  dan outlier transformasi data dilakukan untuk mengubah format data agar lebih mudah dipahami dan dianalisis sedangkan reduksi dimensi bertujuan mengurangi dimensi data yang terlalu besar untuk efisiensi dalam proses analisis dan mendapatkan hasil yang lebih baik proses data preparation dapat bervariasi karena tergantung pada tujuan analisis  keahlian pengguna data  dan pertanyaan yang ingin dijawab aspek dalam data preparation meliputi data completeness  data correctness  data quantity  dan data usability']}>,\n",
              " <MultiLabel: {'labels': [{'id': '25b74fc3-a0cc-4c98-8e2f-1114bd6bf070', 'query': 'apa yang dimaksud dengan iterative imputation ', 'document': {'id': '7bbc3a36629d7dfa3c38ae7076b81444-0', 'content': 'parameter tersebut seperti ukuran jarak  jumlah tetangga terdekat  dan bobot iterative imputation adalah proses di mana setiap fitur dimodelkan sebagai fungsi dari fitur lainnya  misalnya sebagai masalah regresi untuk memprediksi missing value setiap fitur diperhitungkan secara berurutan  satu demi satu  memungkinkan nilai yang telah diperhitungkan sebelumnya untuk digunakan sebagai bagian dari model untuk memprediksi fitur berikutnya disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1192, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' iterative imputation adalah proses di mana setiap fitur dimodelkan sebagai fungsi dari fitur lainnya', 'type': 'extractive', 'score': 0.0, 'context': 'parameter tersebut seperti ukuran jarak  jumlah tetangga terdekat  dan bobot iterative imputation adalah proses di mana setiap fitur dimodelkan sebagai fungsi dari fitur lainnya  misalnya sebagai masalah regresi untuk memprediksi missing value setiap fitur diperhitungkan secara berurutan  satu demi satu  memungkinkan nilai yang telah diperhitungkan sebelumnya untuk digunakan sebagai bagian dari model untuk memprediksi fitur berikutnya disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur', 'offsets_in_document': [{'start': 76, 'end': 177}], 'offsets_in_context': [{'start': 76, 'end': 177}], 'document_ids': ['7bbc3a36629d7dfa3c38ae7076b81444-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dimaksud dengan iterative imputation ', 'filters': None, 'id': '14222fdfab6166bfa1d212d37aba7d7d', 'no_answer': False, 'answers': [' iterative imputation adalah proses di mana setiap fitur dimodelkan sebagai fungsi dari fitur lainnya'], 'offsets_in_documents': [{'start': 76, 'end': 177}], 'offsets_in_contexts': [{'start': 76, 'end': 177}], 'document_ids': ['7bbc3a36629d7dfa3c38ae7076b81444-0'], 'contexts': ['parameter tersebut seperti ukuran jarak  jumlah tetangga terdekat  dan bobot iterative imputation adalah proses di mana setiap fitur dimodelkan sebagai fungsi dari fitur lainnya  misalnya sebagai masalah regresi untuk memprediksi missing value setiap fitur diperhitungkan secara berurutan  satu demi satu  memungkinkan nilai yang telah diperhitungkan sebelumnya untuk digunakan sebagai bagian dari model untuk memprediksi fitur berikutnya disebut iteratif karena proses ini diulang beberapa kali  memungkinkan hasil estimasi missing value yang semakin meningkat sebagaimana missing value pada semua fitur diestimasi jumlah iterasi umumnya diset dengan nilai kecil  seperti 10. algoritma regresi yang berbeda dapat digunakan untuk memperkirakan missing value untuk setiap fitur']}>,\n",
              " <MultiLabel: {'labels': [{'id': '4aa1cfae-f4f5-458d-ac15-6f76ae532586', 'query': 'apa contoh regularization method pada embedded method ', 'document': {'id': '2fb61d272e4f2006a5c82c6eb3491307-0', 'content': 'embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method embedded method melibatkan algoritma pembelajaran tertentu   sehinga tidak independen sepenuhnya terhadap algoritma pembelajaran seperti pada filter method secara umum embedded method ini terdiri atas  1  pruning methods yang menggunakan semua fitur untuk melatih model di awal   lalu mengeliminasi beberapa fitur secara iteratif mengeset koefisiennya menjadi 0  sambil mempertahankan performa model contoh   svm rfe  svm recursive feature elimination    2  model dengan proses seleksi fitur di dalamnya  seperti decision tree id3 dan c4.5   3  regularization method dengan fungsi objektif yang meminimalkan fitting error   sementara itu memaksa koefisien menjadi kecil atau tepat nol', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1195, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'regularization method dengan fungsi objektif yang meminimalkan fitting error   sementara itu memaksa koefisien menjadi kecil atau tepat nol', 'type': 'extractive', 'score': 0.0, 'context': 'embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method embedded method melibatkan algoritma pembelajaran tertentu   sehinga tidak independen sepenuhnya terhadap algoritma pembelajaran seperti pada filter method secara umum embedded method ini terdiri atas  1  pruning methods yang menggunakan semua fitur untuk melatih model di awal   lalu mengeliminasi beberapa fitur secara iteratif mengeset koefisiennya menjadi 0  sambil mempertahankan performa model contoh   svm rfe  svm recursive feature elimination    2  model dengan proses seleksi fitur di dalamnya  seperti decision tree id3 dan c4.5   3  regularization method dengan fungsi objektif yang meminimalkan fitting error   sementara itu memaksa koefisien menjadi kecil atau tepat nol', 'offsets_in_document': [{'start': 918, 'end': 1057}], 'offsets_in_context': [{'start': 918, 'end': 1057}], 'document_ids': ['2fb61d272e4f2006a5c82c6eb3491307-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa contoh regularization method pada embedded method ', 'filters': None, 'id': '2218c45bbafb9a1212efe8ae5434a968', 'no_answer': False, 'answers': ['regularization method dengan fungsi objektif yang meminimalkan fitting error   sementara itu memaksa koefisien menjadi kecil atau tepat nol'], 'offsets_in_documents': [{'start': 918, 'end': 1057}], 'offsets_in_contexts': [{'start': 918, 'end': 1057}], 'document_ids': ['2fb61d272e4f2006a5c82c6eb3491307-0'], 'contexts': ['embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method embedded method melibatkan algoritma pembelajaran tertentu   sehinga tidak independen sepenuhnya terhadap algoritma pembelajaran seperti pada filter method secara umum embedded method ini terdiri atas  1  pruning methods yang menggunakan semua fitur untuk melatih model di awal   lalu mengeliminasi beberapa fitur secara iteratif mengeset koefisiennya menjadi 0  sambil mempertahankan performa model contoh   svm rfe  svm recursive feature elimination    2  model dengan proses seleksi fitur di dalamnya  seperti decision tree id3 dan c4.5   3  regularization method dengan fungsi objektif yang meminimalkan fitting error   sementara itu memaksa koefisien menjadi kecil atau tepat nol']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd35e32b2-c78b-45b4-97cc-b669442224ce', 'query': 'bagaimana cara kerja random oversampling ', 'document': {'id': '4ae36c63f7f7cfffd5516c6e7e521a79-0', 'content': 'kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn random oversampling merupakan metode non heuristik yang mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1205, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas ', 'type': 'extractive', 'score': 0.0, 'context': 'kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn random oversampling merupakan metode non heuristik yang mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn', 'offsets_in_document': [{'start': 266, 'end': 377}], 'offsets_in_context': [{'start': 266, 'end': 377}], 'document_ids': ['4ae36c63f7f7cfffd5516c6e7e521a79-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara kerja random oversampling ', 'filters': None, 'id': 'c2169e1c5e99b8f271d4a322f2b0d256', 'no_answer': False, 'answers': ['mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas '], 'offsets_in_documents': [{'start': 266, 'end': 377}], 'offsets_in_contexts': [{'start': 266, 'end': 377}], 'document_ids': ['4ae36c63f7f7cfffd5516c6e7e521a79-0'], 'contexts': ['kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn random oversampling merupakan metode non heuristik yang mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd4836a04-dc08-473b-84f9-f101feb99238', 'query': 'bagaimana cara random oversampling menyamakan sampel kelas minoritas dengan sampel mayoritas ', 'document': {'id': '4ae36c63f7f7cfffd5516c6e7e521a79-0', 'content': 'kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn random oversampling merupakan metode non heuristik yang mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1205, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas ', 'type': 'extractive', 'score': 0.0, 'context': 'kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn random oversampling merupakan metode non heuristik yang mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn', 'offsets_in_document': [{'start': 266, 'end': 377}], 'offsets_in_context': [{'start': 266, 'end': 377}], 'document_ids': ['4ae36c63f7f7cfffd5516c6e7e521a79-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara random oversampling menyamakan sampel kelas minoritas dengan sampel mayoritas ', 'filters': None, 'id': 'de284f98b53519f14953abf369f77578', 'no_answer': False, 'answers': ['mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas '], 'offsets_in_documents': [{'start': 266, 'end': 377}], 'offsets_in_contexts': [{'start': 266, 'end': 377}], 'document_ids': ['4ae36c63f7f7cfffd5516c6e7e521a79-0'], 'contexts': ['kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn random oversampling merupakan metode non heuristik yang mereplikasi sampel dari kelas minoritas secara random  sehingga jumlahnya menyamai sampel dari kelas mayoritas kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn']}>,\n",
              " <MultiLabel: {'labels': [{'id': '67860431-3a86-4fe6-b94a-7f94ce2860ae', 'query': 'data biasanya terdiri atas apa ', 'document': {'id': 'c08f54d4b4c261687bf272322bb5703-0', 'content': ' data biasanya terdiri dari banyak variabel input atau fitur yang mana masing masing fitur dapat memiliki rentang nilai atau satuan yang berbeda variabel input dengan rentang nilai yang besar dapat mendominasi variabel input yang nilainya lebih kecil beberapa algoritma machine learning  ml  menjadi lebih condong ke nilai besar tersebut dan mengabaikan variabel dengan rentang nilai yang kecil  maka dari itu diperlukan data scaling sebelum proses modelling banyak algoritma ml yang dapat bekerja lebih baik ketika variabel input numerik diskalakan ke rentang standar tertentu hal ini berlaku untuk algoritma ml yang menggunakan weighted sum of inputs seperti linear regression  logistic regression  dan jaringan syaraf tiruan serta algoritma ml yang memperhitungkan jarak antar sampel data seperti k nearest neighbors  k means clustering  dan support vector machines  svm', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1208, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'data biasanya terdiri dari banyak variabel input atau fitur', 'type': 'extractive', 'score': 0.0, 'context': ' data biasanya terdiri dari banyak variabel input atau fitur yang mana masing masing fitur dapat memiliki rentang nilai atau satuan yang berbeda variabel input dengan rentang nilai yang besar dapat mendominasi variabel input yang nilainya lebih kecil beberapa algoritma machine learning  ml  menjadi lebih condong ke nilai besar tersebut dan mengabaikan variabel dengan rentang nilai yang kecil  maka dari itu diperlukan data scaling sebelum proses modelling banyak algoritma ml yang dapat bekerja lebih baik ketika variabel input numerik diskalakan ke rentang standar tertentu hal ini berlaku untuk algoritma ml yang menggunakan weighted sum of inputs seperti linear regression  logistic regression  dan jaringan syaraf tiruan serta algoritma ml yang memperhitungkan jarak antar sampel data seperti k nearest neighbors  k means clustering  dan support vector machines  svm', 'offsets_in_document': [{'start': 1, 'end': 60}], 'offsets_in_context': [{'start': 1, 'end': 60}], 'document_ids': ['c08f54d4b4c261687bf272322bb5703-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'data biasanya terdiri atas apa ', 'filters': None, 'id': '20a1c2646a57604fd14f80638427d246', 'no_answer': False, 'answers': ['data biasanya terdiri dari banyak variabel input atau fitur'], 'offsets_in_documents': [{'start': 1, 'end': 60}], 'offsets_in_contexts': [{'start': 1, 'end': 60}], 'document_ids': ['c08f54d4b4c261687bf272322bb5703-0'], 'contexts': [' data biasanya terdiri dari banyak variabel input atau fitur yang mana masing masing fitur dapat memiliki rentang nilai atau satuan yang berbeda variabel input dengan rentang nilai yang besar dapat mendominasi variabel input yang nilainya lebih kecil beberapa algoritma machine learning  ml  menjadi lebih condong ke nilai besar tersebut dan mengabaikan variabel dengan rentang nilai yang kecil  maka dari itu diperlukan data scaling sebelum proses modelling banyak algoritma ml yang dapat bekerja lebih baik ketika variabel input numerik diskalakan ke rentang standar tertentu hal ini berlaku untuk algoritma ml yang menggunakan weighted sum of inputs seperti linear regression  logistic regression  dan jaringan syaraf tiruan serta algoritma ml yang memperhitungkan jarak antar sampel data seperti k nearest neighbors  k means clustering  dan support vector machines  svm']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'fc74918d-f0d9-4482-8a40-9edb33284877', 'query': 'apa kelebihan dari metode oversampling ', 'document': {'id': 'a138c9bab99461184b803dccb7878b02-0', 'content': 'under_sampling oversampling akan meningkatkan jumlah sampel dari kelas minoritas kelebihannya tidak ada informasi yang hilang  karena tidak ada eliminasi sampel kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1213, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kelebihannya tidak ada informasi yang hilang  karena tidak ada eliminasi sampel ', 'type': 'extractive', 'score': 0.0, 'context': 'under_sampling oversampling akan meningkatkan jumlah sampel dari kelas minoritas kelebihannya tidak ada informasi yang hilang  karena tidak ada eliminasi sampel kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn', 'offsets_in_document': [{'start': 81, 'end': 161}], 'offsets_in_context': [{'start': 81, 'end': 161}], 'document_ids': ['a138c9bab99461184b803dccb7878b02-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa kelebihan dari metode oversampling ', 'filters': None, 'id': 'aa9179cc75276453f70098d4f9f8e544', 'no_answer': False, 'answers': ['kelebihannya tidak ada informasi yang hilang  karena tidak ada eliminasi sampel '], 'offsets_in_documents': [{'start': 81, 'end': 161}], 'offsets_in_contexts': [{'start': 81, 'end': 161}], 'document_ids': ['a138c9bab99461184b803dccb7878b02-0'], 'contexts': ['under_sampling oversampling akan meningkatkan jumlah sampel dari kelas minoritas kelebihannya tidak ada informasi yang hilang  karena tidak ada eliminasi sampel kelemahannya jumlah data semakin bertambah  dapat memperlambat waktu pembelajaran contohnya yaitu random oversampling  synthetic minority oversampling technique  smote   dan adaptive synthetic sampling  adasyn']}>,\n",
              " <MultiLabel: {'labels': [{'id': '896fe25b-47ef-4789-ba05-e8b22a3807de', 'query': 'kapan waktu yang tepat untuk menggunakan standardisasi ', 'document': {'id': '7e10b74be485cde487f3dd1d2a7c7113-0', 'content': 'banyak algoritma ml yang dapat bekerja lebih baik ketika variabel input numerik diskalakan ke rentang standar tertentu hal ini berlaku untuk algoritma ml yang menggunakan weighted sum of inputs seperti linear regression  logistic regression  dan jaringan syaraf tiruan serta algoritma ml yang memperhitungkan jarak antar sampel data seperti k nearest neighbors  k means clustering  dan support vector machines  svm beberapa algoritma ml tidak terpengaruh oleh perbedaan sckala pada data numerik  seperti decision tree dan random forest teknik data scaling terbagi menjadi normalization dan standardization normalization menskalakan nilai variabel numerik ke dalam rentang nilai tertentu  biasanya 0 1 sedangkan standardization menskalakan nilai variabel numerik hingga memiliki mean   0 dan standar deviasi   1. jika nilai variabel input mengikuti distribusi gaussian  maka standardisasi kemungkinan lebih tepat untuk dilakukan', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1219, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'jika nilai variabel input mengikuti distribusi gaussian  maka standardisasi kemungkinan lebih tepat untuk dilakukan', 'type': 'extractive', 'score': 0.0, 'context': 'banyak algoritma ml yang dapat bekerja lebih baik ketika variabel input numerik diskalakan ke rentang standar tertentu hal ini berlaku untuk algoritma ml yang menggunakan weighted sum of inputs seperti linear regression  logistic regression  dan jaringan syaraf tiruan serta algoritma ml yang memperhitungkan jarak antar sampel data seperti k nearest neighbors  k means clustering  dan support vector machines  svm beberapa algoritma ml tidak terpengaruh oleh perbedaan sckala pada data numerik  seperti decision tree dan random forest teknik data scaling terbagi menjadi normalization dan standardization normalization menskalakan nilai variabel numerik ke dalam rentang nilai tertentu  biasanya 0 1 sedangkan standardization menskalakan nilai variabel numerik hingga memiliki mean   0 dan standar deviasi   1. jika nilai variabel input mengikuti distribusi gaussian  maka standardisasi kemungkinan lebih tepat untuk dilakukan', 'offsets_in_document': [{'start': 812, 'end': 927}], 'offsets_in_context': [{'start': 812, 'end': 927}], 'document_ids': ['7e10b74be485cde487f3dd1d2a7c7113-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kapan waktu yang tepat untuk menggunakan standardisasi ', 'filters': None, 'id': '1457bd3d5fc7567e30fc1a2018395ca8', 'no_answer': False, 'answers': ['jika nilai variabel input mengikuti distribusi gaussian  maka standardisasi kemungkinan lebih tepat untuk dilakukan'], 'offsets_in_documents': [{'start': 812, 'end': 927}], 'offsets_in_contexts': [{'start': 812, 'end': 927}], 'document_ids': ['7e10b74be485cde487f3dd1d2a7c7113-0'], 'contexts': ['banyak algoritma ml yang dapat bekerja lebih baik ketika variabel input numerik diskalakan ke rentang standar tertentu hal ini berlaku untuk algoritma ml yang menggunakan weighted sum of inputs seperti linear regression  logistic regression  dan jaringan syaraf tiruan serta algoritma ml yang memperhitungkan jarak antar sampel data seperti k nearest neighbors  k means clustering  dan support vector machines  svm beberapa algoritma ml tidak terpengaruh oleh perbedaan sckala pada data numerik  seperti decision tree dan random forest teknik data scaling terbagi menjadi normalization dan standardization normalization menskalakan nilai variabel numerik ke dalam rentang nilai tertentu  biasanya 0 1 sedangkan standardization menskalakan nilai variabel numerik hingga memiliki mean   0 dan standar deviasi   1. jika nilai variabel input mengikuti distribusi gaussian  maka standardisasi kemungkinan lebih tepat untuk dilakukan']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c1b96dfe-b096-4d71-be46-3aa9232b0d29', 'query': 'apa itu tomek link?', 'document': {'id': 'd613c04a1aa5d0149582c27f088b3279-0', 'content': 'tomek link adalah pasangan instance dari dua kelas berbeda yang merupakan tetangga terdekatnya algoritma ini menemukan pasangan tersebut  lalu menghapus instance dari kelas mayoritas untuk mempertegas pemisah antara kelas yang berbeda tomek link dapat diimplementasikan menggunakan class tomeklinks dari library imblearn under_sampling cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1215, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'tomek link adalah pasangan instance dari dua kelas berbeda yang merupakan tetangga terdekatnya', 'type': 'extractive', 'score': 0.0, 'context': 'tomek link adalah pasangan instance dari dua kelas berbeda yang merupakan tetangga terdekatnya algoritma ini menemukan pasangan tersebut  lalu menghapus instance dari kelas mayoritas untuk mempertegas pemisah antara kelas yang berbeda tomek link dapat diimplementasikan menggunakan class tomeklinks dari library imblearn under_sampling cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya', 'offsets_in_document': [{'start': 0, 'end': 94}], 'offsets_in_context': [{'start': 0, 'end': 94}], 'document_ids': ['d613c04a1aa5d0149582c27f088b3279-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu tomek link?', 'filters': None, 'id': '4c4449134c900956f92a417338f42e6c', 'no_answer': False, 'answers': ['tomek link adalah pasangan instance dari dua kelas berbeda yang merupakan tetangga terdekatnya'], 'offsets_in_documents': [{'start': 0, 'end': 94}], 'offsets_in_contexts': [{'start': 0, 'end': 94}], 'document_ids': ['d613c04a1aa5d0149582c27f088b3279-0'], 'contexts': ['tomek link adalah pasangan instance dari dua kelas berbeda yang merupakan tetangga terdekatnya algoritma ini menemukan pasangan tersebut  lalu menghapus instance dari kelas mayoritas untuk mempertegas pemisah antara kelas yang berbeda tomek link dapat diimplementasikan menggunakan class tomeklinks dari library imblearn under_sampling cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2b59e305-daa6-4c9e-b5dd-43d68549b69d', 'query': 'apa yang tidak dapat ditangani oleh algoritma relief?', 'document': {'id': 'cd0b8414ac45a94a5921b8f1a72dbf3e-0', 'content': 'sebaliknya  jika nilai fitur a pada sampel r i dan nearest miss adalah berbeda  maka bobot fitur a akan ditingkatkan algoritma relief mampu menangani fitur kategorikal maupun numerik  namun tidak dapat menangani data yang tidak lengkap  dan terbatas untuk permasalahan klasifikasi biner algoritma relieff merupakan ekstensi dari algoritma basic relief untuk menangani permasalahan klasifikasi multikelas algoritma relieff lebih robost sehingga mampu menangani data yang tidak lengkap dan data yang mengandung noise wrapper method mencari kombinasi subset fitur yang optimal dengan mengevaluasinya secara langsung pada algoritma pembelajaran tertentu', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1225, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'algoritma relief mampu menangani fitur kategorikal maupun numerik  namun tidak dapat menangani data yang tidak lengkap  dan terbatas ', 'type': 'extractive', 'score': 0.0, 'context': 'sebaliknya  jika nilai fitur a pada sampel r i dan nearest miss adalah berbeda  maka bobot fitur a akan ditingkatkan algoritma relief mampu menangani fitur kategorikal maupun numerik  namun tidak dapat menangani data yang tidak lengkap  dan terbatas untuk permasalahan klasifikasi biner algoritma relieff merupakan ekstensi dari algoritma basic relief untuk menangani permasalahan klasifikasi multikelas algoritma relieff lebih robost sehingga mampu menangani data yang tidak lengkap dan data yang mengandung noise wrapper method mencari kombinasi subset fitur yang optimal dengan mengevaluasinya secara langsung pada algoritma pembelajaran tertentu', 'offsets_in_document': [{'start': 117, 'end': 250}], 'offsets_in_context': [{'start': 117, 'end': 250}], 'document_ids': ['cd0b8414ac45a94a5921b8f1a72dbf3e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang tidak dapat ditangani oleh algoritma relief?', 'filters': None, 'id': '8cea856139cd2e8894d110f66e80fa4a', 'no_answer': False, 'answers': ['algoritma relief mampu menangani fitur kategorikal maupun numerik  namun tidak dapat menangani data yang tidak lengkap  dan terbatas '], 'offsets_in_documents': [{'start': 117, 'end': 250}], 'offsets_in_contexts': [{'start': 117, 'end': 250}], 'document_ids': ['cd0b8414ac45a94a5921b8f1a72dbf3e-0'], 'contexts': ['sebaliknya  jika nilai fitur a pada sampel r i dan nearest miss adalah berbeda  maka bobot fitur a akan ditingkatkan algoritma relief mampu menangani fitur kategorikal maupun numerik  namun tidak dapat menangani data yang tidak lengkap  dan terbatas untuk permasalahan klasifikasi biner algoritma relieff merupakan ekstensi dari algoritma basic relief untuk menangani permasalahan klasifikasi multikelas algoritma relieff lebih robost sehingga mampu menangani data yang tidak lengkap dan data yang mengandung noise wrapper method mencari kombinasi subset fitur yang optimal dengan mengevaluasinya secara langsung pada algoritma pembelajaran tertentu']}>,\n",
              " <MultiLabel: {'labels': [{'id': '50d76d61-8492-4d80-ab24-af40f09a4ab7', 'query': 'apa itu teknik data scaling normalization ', 'document': {'id': '3cd824d22dde3c0e29ce74835af1ddfb-0', 'content': 'teknik data scaling terbagi menjadi normalization dan standardization normalization menskalakan nilai variabel numerik ke dalam rentang nilai tertentu  biasanya 0 1 sedangkan standardization menskalakan nilai variabel numerik hingga memiliki mean   0 dan standar deviasi   1. jika nilai variabel input mengikuti distribusi gaussian  maka standardisasi kemungkinan lebih tepat untuk dilakukan jika tidak  maka normalisasi dapat diterapkan khususnya saat range data cukup besar atau kecil alternatifnya dapat dilakukan eksperimen menggunakan normalisasi dan standardisasi  atau bahkan kombinasi keduanya lalu dilihat hasil yang didapat pada saat evaluasi mana yang lebih baik categorical data dapat berupa variabel nominal dan variabel ordinal', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1198, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'normalization menskalakan nilai variabel numerik ke dalam rentang nilai tertentu  biasanya 0 1', 'type': 'extractive', 'score': 0.0, 'context': 'teknik data scaling terbagi menjadi normalization dan standardization normalization menskalakan nilai variabel numerik ke dalam rentang nilai tertentu  biasanya 0 1 sedangkan standardization menskalakan nilai variabel numerik hingga memiliki mean   0 dan standar deviasi   1. jika nilai variabel input mengikuti distribusi gaussian  maka standardisasi kemungkinan lebih tepat untuk dilakukan jika tidak  maka normalisasi dapat diterapkan khususnya saat range data cukup besar atau kecil alternatifnya dapat dilakukan eksperimen menggunakan normalisasi dan standardisasi  atau bahkan kombinasi keduanya lalu dilihat hasil yang didapat pada saat evaluasi mana yang lebih baik categorical data dapat berupa variabel nominal dan variabel ordinal', 'offsets_in_document': [{'start': 70, 'end': 164}], 'offsets_in_context': [{'start': 70, 'end': 164}], 'document_ids': ['3cd824d22dde3c0e29ce74835af1ddfb-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu teknik data scaling normalization ', 'filters': None, 'id': '72240047a00ae95ce28b38db9bef7afb', 'no_answer': False, 'answers': ['normalization menskalakan nilai variabel numerik ke dalam rentang nilai tertentu  biasanya 0 1'], 'offsets_in_documents': [{'start': 70, 'end': 164}], 'offsets_in_contexts': [{'start': 70, 'end': 164}], 'document_ids': ['3cd824d22dde3c0e29ce74835af1ddfb-0'], 'contexts': ['teknik data scaling terbagi menjadi normalization dan standardization normalization menskalakan nilai variabel numerik ke dalam rentang nilai tertentu  biasanya 0 1 sedangkan standardization menskalakan nilai variabel numerik hingga memiliki mean   0 dan standar deviasi   1. jika nilai variabel input mengikuti distribusi gaussian  maka standardisasi kemungkinan lebih tepat untuk dilakukan jika tidak  maka normalisasi dapat diterapkan khususnya saat range data cukup besar atau kecil alternatifnya dapat dilakukan eksperimen menggunakan normalisasi dan standardisasi  atau bahkan kombinasi keduanya lalu dilihat hasil yang didapat pada saat evaluasi mana yang lebih baik categorical data dapat berupa variabel nominal dan variabel ordinal']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0f81185c-1b14-4dcd-8e71-588e9f03a76a', 'query': 'apa class yang digunakan untuk mengimplementasikan one hot encoding ', 'document': {'id': 'b64dcbc366f49264bf1c078d7e1b1083-0', 'content': 'urutan nilai integer secara natural juga merepresentasikan urutan di dalam nilai nilai variabel ordinal tersebut dalam diimplementasikan menggunakan class ordinalencoder dari library scikit learn one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1220, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' class onehotencoder dari library scikit learn', 'type': 'extractive', 'score': 0.0, 'context': 'urutan nilai integer secara natural juga merepresentasikan urutan di dalam nilai nilai variabel ordinal tersebut dalam diimplementasikan menggunakan class ordinalencoder dari library scikit learn one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn', 'offsets_in_document': [{'start': 376, 'end': 422}], 'offsets_in_context': [{'start': 376, 'end': 422}], 'document_ids': ['b64dcbc366f49264bf1c078d7e1b1083-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa class yang digunakan untuk mengimplementasikan one hot encoding ', 'filters': None, 'id': '89a8e3f4900aca4e2596ddfc11b3dd40', 'no_answer': False, 'answers': [' class onehotencoder dari library scikit learn'], 'offsets_in_documents': [{'start': 376, 'end': 422}], 'offsets_in_contexts': [{'start': 376, 'end': 422}], 'document_ids': ['b64dcbc366f49264bf1c078d7e1b1083-0'], 'contexts': ['urutan nilai integer secara natural juga merepresentasikan urutan di dalam nilai nilai variabel ordinal tersebut dalam diimplementasikan menggunakan class ordinalencoder dari library scikit learn one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn']}>,\n",
              " <MultiLabel: {'labels': [{'id': '7f71d0f8-5b0d-4380-a2a8-d8e754b55323', 'query': 'mengapa one hote encoding digunakan ', 'document': {'id': 'b64dcbc366f49264bf1c078d7e1b1083-0', 'content': 'urutan nilai integer secara natural juga merepresentasikan urutan di dalam nilai nilai variabel ordinal tersebut dalam diimplementasikan menggunakan class ordinalencoder dari library scikit learn one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1220, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik ', 'type': 'extractive', 'score': 0.0, 'context': 'urutan nilai integer secara natural juga merepresentasikan urutan di dalam nilai nilai variabel ordinal tersebut dalam diimplementasikan menggunakan class ordinalencoder dari library scikit learn one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn', 'offsets_in_document': [{'start': 196, 'end': 272}], 'offsets_in_context': [{'start': 196, 'end': 272}], 'document_ids': ['b64dcbc366f49264bf1c078d7e1b1083-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa one hote encoding digunakan ', 'filters': None, 'id': '083237a88843f6f2d52cb3e25d411128', 'no_answer': False, 'answers': ['one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik '], 'offsets_in_documents': [{'start': 196, 'end': 272}], 'offsets_in_contexts': [{'start': 196, 'end': 272}], 'document_ids': ['b64dcbc366f49264bf1c078d7e1b1083-0'], 'contexts': ['urutan nilai integer secara natural juga merepresentasikan urutan di dalam nilai nilai variabel ordinal tersebut dalam diimplementasikan menggunakan class ordinalencoder dari library scikit learn one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn']}>,\n",
              " <MultiLabel: {'labels': [{'id': '54f7e337-06f2-4c3a-b39a-2f63ca0ac1f4', 'query': 'bagaimana suatu fitur dapat dikatakan sebagai fitur yang berkualitas ', 'document': {'id': 'b7cdcdea070337d723e2f620d742c0e5-0', 'content': 'kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur fitur yang berkualitas adalah fitur yang memiliki nilai sama pada instance yang berasal dari kelas yang sama dan nilai yang berbeda terhadap instance yang berasal dari kelas berbeda algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1231, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'fitur yang berkualitas adalah fitur yang memiliki nilai sama pada instance yang berasal dari kelas yang sama dan nilai yang berbeda terhadap instance yang berasal dari kelas berbeda ', 'type': 'extractive', 'score': 0.0, 'context': 'kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur fitur yang berkualitas adalah fitur yang memiliki nilai sama pada instance yang berasal dari kelas yang sama dan nilai yang berbeda terhadap instance yang berasal dari kelas berbeda algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas', 'offsets_in_document': [{'start': 365, 'end': 547}], 'offsets_in_context': [{'start': 365, 'end': 547}], 'document_ids': ['b7cdcdea070337d723e2f620d742c0e5-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana suatu fitur dapat dikatakan sebagai fitur yang berkualitas ', 'filters': None, 'id': '4ee58b0ff718b005e9fb7001999fa863', 'no_answer': False, 'answers': ['fitur yang berkualitas adalah fitur yang memiliki nilai sama pada instance yang berasal dari kelas yang sama dan nilai yang berbeda terhadap instance yang berasal dari kelas berbeda '], 'offsets_in_documents': [{'start': 365, 'end': 547}], 'offsets_in_contexts': [{'start': 365, 'end': 547}], 'document_ids': ['b7cdcdea070337d723e2f620d742c0e5-0'], 'contexts': ['kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur fitur yang berkualitas adalah fitur yang memiliki nilai sama pada instance yang berasal dari kelas yang sama dan nilai yang berbeda terhadap instance yang berasal dari kelas berbeda algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'ec95583b-c7f4-4867-b433-962714e160b4', 'query': 'berasal dari kelas mana instance fitur yang memiliki nilai berbeda supaya fitur tersebut bisa dikatakan fitur yang berkualitas ', 'document': {'id': 'b7cdcdea070337d723e2f620d742c0e5-0', 'content': 'kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur fitur yang berkualitas adalah fitur yang memiliki nilai sama pada instance yang berasal dari kelas yang sama dan nilai yang berbeda terhadap instance yang berasal dari kelas berbeda algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1231, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' kelas berbeda', 'type': 'extractive', 'score': 0.0, 'context': 'kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur fitur yang berkualitas adalah fitur yang memiliki nilai sama pada instance yang berasal dari kelas yang sama dan nilai yang berbeda terhadap instance yang berasal dari kelas berbeda algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas', 'offsets_in_document': [{'start': 532, 'end': 546}], 'offsets_in_context': [{'start': 532, 'end': 546}], 'document_ids': ['b7cdcdea070337d723e2f620d742c0e5-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'berasal dari kelas mana instance fitur yang memiliki nilai berbeda supaya fitur tersebut bisa dikatakan fitur yang berkualitas ', 'filters': None, 'id': '43ddb3bf7ab46854c81a43f70cfae28d', 'no_answer': False, 'answers': [' kelas berbeda'], 'offsets_in_documents': [{'start': 532, 'end': 546}], 'offsets_in_contexts': [{'start': 532, 'end': 546}], 'document_ids': ['b7cdcdea070337d723e2f620d742c0e5-0'], 'contexts': ['kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur fitur yang berkualitas adalah fitur yang memiliki nilai sama pada instance yang berasal dari kelas yang sama dan nilai yang berbeda terhadap instance yang berasal dari kelas berbeda algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'ab9a68c9-ce75-43dc-a4dd-5978bb66eb1b', 'query': 'apakah missing value dapat meningkat?', 'document': {'id': '1b17fbe5d7783220b9b7dc91984d264b-0', 'content': 'kemungkinan missing value semakin meningkat seiring bertambahnya ukuran dataset penyebab missing value dapat bervariasi seperti masalah spesifik pada problem domain  observasi yang tidak tercatat  data corruption  data unavailability  malfunctioning measurement equipment penanganan missing value sangat penting karena beberapa algoritma machine learning tidak mendukung adanya missing value terdapat beberapa strategi untuk menangani missing value  seperti menghapus kolom atau baris yang berisi missing value  serta missing value imputation yang mana mengidentifikasi missing value dalam dataset  lalu mengisinya dengan nilai hasil estimasi contohnya seperti statistical imputation  knn imputation  iterative imputation', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1182, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kemungkinan missing value semakin meningkat seiring bertambahnya ukuran dataset ', 'type': 'extractive', 'score': 0.0, 'context': 'kemungkinan missing value semakin meningkat seiring bertambahnya ukuran dataset penyebab missing value dapat bervariasi seperti masalah spesifik pada problem domain  observasi yang tidak tercatat  data corruption  data unavailability  malfunctioning measurement equipment penanganan missing value sangat penting karena beberapa algoritma machine learning tidak mendukung adanya missing value terdapat beberapa strategi untuk menangani missing value  seperti menghapus kolom atau baris yang berisi missing value  serta missing value imputation yang mana mengidentifikasi missing value dalam dataset  lalu mengisinya dengan nilai hasil estimasi contohnya seperti statistical imputation  knn imputation  iterative imputation', 'offsets_in_document': [{'start': 0, 'end': 80}], 'offsets_in_context': [{'start': 0, 'end': 80}], 'document_ids': ['1b17fbe5d7783220b9b7dc91984d264b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apakah missing value dapat meningkat?', 'filters': None, 'id': '07c040afa780ebc09b15cab4ea624411', 'no_answer': False, 'answers': ['kemungkinan missing value semakin meningkat seiring bertambahnya ukuran dataset '], 'offsets_in_documents': [{'start': 0, 'end': 80}], 'offsets_in_contexts': [{'start': 0, 'end': 80}], 'document_ids': ['1b17fbe5d7783220b9b7dc91984d264b-0'], 'contexts': ['kemungkinan missing value semakin meningkat seiring bertambahnya ukuran dataset penyebab missing value dapat bervariasi seperti masalah spesifik pada problem domain  observasi yang tidak tercatat  data corruption  data unavailability  malfunctioning measurement equipment penanganan missing value sangat penting karena beberapa algoritma machine learning tidak mendukung adanya missing value terdapat beberapa strategi untuk menangani missing value  seperti menghapus kolom atau baris yang berisi missing value  serta missing value imputation yang mana mengidentifikasi missing value dalam dataset  lalu mengisinya dengan nilai hasil estimasi contohnya seperti statistical imputation  knn imputation  iterative imputation']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e340b90e-3506-447d-af3f-b60beba852c4', 'query': 'bagaimana mengidentifikasi outlier pada data yang tidak memiliki distribusi normal?', 'document': {'id': '34cd81755a6f0525dc32f20460c669bc-0', 'content': 'menggunakan cut off standard deviasi dari mean untuk identifikasi outlier metode kedua adalah interquartile range  iqr  method  yang dapat digunakan pada sample yang datanya tidak mengikuti distribusi normal  gaussian metode ketiga adalah automatic outlier detection yang menggunakan metode lof  local outlier factor metode ini menggunakan ide konsep nearest neighbors untuk identifikasi outlier lof menghitung deviasi local density dari titik data tertentu terhadap tetangganya', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1186, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'interquartile range  iqr  method  yang dapat digunakan pada sample yang datanya tidak mengikuti distribusi normal  gaussian ', 'type': 'extractive', 'score': 0.0, 'context': 'menggunakan cut off standard deviasi dari mean untuk identifikasi outlier metode kedua adalah interquartile range  iqr  method  yang dapat digunakan pada sample yang datanya tidak mengikuti distribusi normal  gaussian metode ketiga adalah automatic outlier detection yang menggunakan metode lof  local outlier factor metode ini menggunakan ide konsep nearest neighbors untuk identifikasi outlier lof menghitung deviasi local density dari titik data tertentu terhadap tetangganya', 'offsets_in_document': [{'start': 94, 'end': 218}], 'offsets_in_context': [{'start': 94, 'end': 218}], 'document_ids': ['34cd81755a6f0525dc32f20460c669bc-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana mengidentifikasi outlier pada data yang tidak memiliki distribusi normal?', 'filters': None, 'id': '0d1d340aa6a782e3a3d7f4cfc8e2eb31', 'no_answer': False, 'answers': ['interquartile range  iqr  method  yang dapat digunakan pada sample yang datanya tidak mengikuti distribusi normal  gaussian '], 'offsets_in_documents': [{'start': 94, 'end': 218}], 'offsets_in_contexts': [{'start': 94, 'end': 218}], 'document_ids': ['34cd81755a6f0525dc32f20460c669bc-0'], 'contexts': ['menggunakan cut off standard deviasi dari mean untuk identifikasi outlier metode kedua adalah interquartile range  iqr  method  yang dapat digunakan pada sample yang datanya tidak mengikuti distribusi normal  gaussian metode ketiga adalah automatic outlier detection yang menggunakan metode lof  local outlier factor metode ini menggunakan ide konsep nearest neighbors untuk identifikasi outlier lof menghitung deviasi local density dari titik data tertentu terhadap tetangganya']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'fd947a55-237a-4291-8e23-d75aab6cbb84', 'query': 'kapan missing value terjadi?', 'document': {'id': '22d652e6743cac46dc53a5d9fb33c526-0', 'content': 'setiap sampel akan diberi skor untuk mengindikasikan tingkat isolasi berdasarkan ukurannya terhadap local neighborhood tugas keempat dalam data cleaning yaitu menangani missing value missing value terjadi ketika satu atau beberapa nilai kolom dalam dataset tidak tersedia kemungkinan missing value semakin meningkat seiring bertambahnya ukuran dataset penyebab missing value dapat bervariasi seperti masalah spesifik pada problem domain  observasi yang tidak tercatat  data corruption  data unavailability  malfunctioning measurement equipment', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1194, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'missing value terjadi ketika satu atau beberapa nilai kolom dalam dataset tidak tersedia ', 'type': 'extractive', 'score': 0.0, 'context': 'setiap sampel akan diberi skor untuk mengindikasikan tingkat isolasi berdasarkan ukurannya terhadap local neighborhood tugas keempat dalam data cleaning yaitu menangani missing value missing value terjadi ketika satu atau beberapa nilai kolom dalam dataset tidak tersedia kemungkinan missing value semakin meningkat seiring bertambahnya ukuran dataset penyebab missing value dapat bervariasi seperti masalah spesifik pada problem domain  observasi yang tidak tercatat  data corruption  data unavailability  malfunctioning measurement equipment', 'offsets_in_document': [{'start': 183, 'end': 272}], 'offsets_in_context': [{'start': 183, 'end': 272}], 'document_ids': ['22d652e6743cac46dc53a5d9fb33c526-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kapan missing value terjadi?', 'filters': None, 'id': 'eb982a3eb6f53d3a60711edfe748d160', 'no_answer': False, 'answers': ['missing value terjadi ketika satu atau beberapa nilai kolom dalam dataset tidak tersedia '], 'offsets_in_documents': [{'start': 183, 'end': 272}], 'offsets_in_contexts': [{'start': 183, 'end': 272}], 'document_ids': ['22d652e6743cac46dc53a5d9fb33c526-0'], 'contexts': ['setiap sampel akan diberi skor untuk mengindikasikan tingkat isolasi berdasarkan ukurannya terhadap local neighborhood tugas keempat dalam data cleaning yaitu menangani missing value missing value terjadi ketika satu atau beberapa nilai kolom dalam dataset tidak tersedia kemungkinan missing value semakin meningkat seiring bertambahnya ukuran dataset penyebab missing value dapat bervariasi seperti masalah spesifik pada problem domain  observasi yang tidak tercatat  data corruption  data unavailability  malfunctioning measurement equipment']}>,\n",
              " <MultiLabel: {'labels': [{'id': '0c49da95-b3f1-4e7f-b050-26e30d31692f', 'query': 'bagaimana resampling bekerja?', 'document': {'id': 'd87d29ea1225f1501b08366cfa411001-0', 'content': 'imbalanced ratio dapat dihitung dengan membagi jumlah sampel di kelas mayoritas dengan jumlah sampel di kelas minoritas metode resampling menyeimbangkan ruang sampel dengan cara menyamakan jumlah sampel data di setiap kelas lebih fleksibel karena independen terhadap algoritma klasifikasi yang digunakan metodenya yaitu undersampling  oversampling  dan hybrid sampling undersampling akan mengeliminasi sampel dari kelas mayoritas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1203, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'metode resampling menyeimbangkan ruang sampel dengan cara menyamakan jumlah sampel data di setiap kelas ', 'type': 'extractive', 'score': 0.0, 'context': 'imbalanced ratio dapat dihitung dengan membagi jumlah sampel di kelas mayoritas dengan jumlah sampel di kelas minoritas metode resampling menyeimbangkan ruang sampel dengan cara menyamakan jumlah sampel data di setiap kelas lebih fleksibel karena independen terhadap algoritma klasifikasi yang digunakan metodenya yaitu undersampling  oversampling  dan hybrid sampling undersampling akan mengeliminasi sampel dari kelas mayoritas', 'offsets_in_document': [{'start': 120, 'end': 224}], 'offsets_in_context': [{'start': 120, 'end': 224}], 'document_ids': ['d87d29ea1225f1501b08366cfa411001-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana resampling bekerja?', 'filters': None, 'id': 'fffce4e28b764d23fbe5397ab5b98f3f', 'no_answer': False, 'answers': ['metode resampling menyeimbangkan ruang sampel dengan cara menyamakan jumlah sampel data di setiap kelas '], 'offsets_in_documents': [{'start': 120, 'end': 224}], 'offsets_in_contexts': [{'start': 120, 'end': 224}], 'document_ids': ['d87d29ea1225f1501b08366cfa411001-0'], 'contexts': ['imbalanced ratio dapat dihitung dengan membagi jumlah sampel di kelas mayoritas dengan jumlah sampel di kelas minoritas metode resampling menyeimbangkan ruang sampel dengan cara menyamakan jumlah sampel data di setiap kelas lebih fleksibel karena independen terhadap algoritma klasifikasi yang digunakan metodenya yaitu undersampling  oversampling  dan hybrid sampling undersampling akan mengeliminasi sampel dari kelas mayoritas']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e6774182-0549-4bbd-9f77-4e833615e3a5', 'query': 'apa kegunaan image augmentation?', 'document': {'id': '37507cb5a228b1279a2b1e1647877bcd-0', 'content': 'image augmentation digunakan untuk meningkatkan jumlah training image dengan memodifikasi training image yang ada menggunakan metode atau teknik tertentu image baru yang dihasilkan memiliki kelas data yang sama sesuai dengan image asli yang dimodifikasi keuntungannya dapat dilakukan dengan mudah  tanpa harus melakukan pengumpulan data lagi beberapa cara populer yang dapat dilakukan antara lain image rotation  image shifting  image flipping  image noising dan image blurring', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1209, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'image augmentation digunakan untuk meningkatkan jumlah training image dengan memodifikasi training image yang ada menggunakan metode atau teknik tertentu', 'type': 'extractive', 'score': 0.0, 'context': 'image augmentation digunakan untuk meningkatkan jumlah training image dengan memodifikasi training image yang ada menggunakan metode atau teknik tertentu image baru yang dihasilkan memiliki kelas data yang sama sesuai dengan image asli yang dimodifikasi keuntungannya dapat dilakukan dengan mudah  tanpa harus melakukan pengumpulan data lagi beberapa cara populer yang dapat dilakukan antara lain image rotation  image shifting  image flipping  image noising dan image blurring', 'offsets_in_document': [{'start': 0, 'end': 153}], 'offsets_in_context': [{'start': 0, 'end': 153}], 'document_ids': ['37507cb5a228b1279a2b1e1647877bcd-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa kegunaan image augmentation?', 'filters': None, 'id': 'c8aa3f6040e2365bcfb288e58307ce56', 'no_answer': False, 'answers': ['image augmentation digunakan untuk meningkatkan jumlah training image dengan memodifikasi training image yang ada menggunakan metode atau teknik tertentu'], 'offsets_in_documents': [{'start': 0, 'end': 153}], 'offsets_in_contexts': [{'start': 0, 'end': 153}], 'document_ids': ['37507cb5a228b1279a2b1e1647877bcd-0'], 'contexts': ['image augmentation digunakan untuk meningkatkan jumlah training image dengan memodifikasi training image yang ada menggunakan metode atau teknik tertentu image baru yang dihasilkan memiliki kelas data yang sama sesuai dengan image asli yang dimodifikasi keuntungannya dapat dilakukan dengan mudah  tanpa harus melakukan pengumpulan data lagi beberapa cara populer yang dapat dilakukan antara lain image rotation  image shifting  image flipping  image noising dan image blurring']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e02219ad-39ed-46e7-85c9-fa07647dc75d', 'query': 'apa saja metode untuk mengidentifikasi outlier?', 'document': {'id': '521aa1ef0045e6c6c9159c97a45a1e9b-0', 'content': 'beberapa cara untuk mengidentifikasi outlier yaitu menggunakan standard deviation method  interquartile range method  dan automatic outlier detection metode pertama adalah standard deviation method  yang cocok digunakan pada sample dengan distribusi nilai gaussian atau mendekati distribusi gaussian pada distribusi gaussian  sebarapa jauh standard deviasi dari nilai rata ratanya menunjukkan persentase nilai pada sample menggunakan cut off standard deviasi dari mean untuk identifikasi outlier metode kedua adalah interquartile range  iqr  method  yang dapat digunakan pada sample yang datanya tidak mengikuti distribusi normal  gaussian', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1188, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'beberapa cara untuk mengidentifikasi outlier yaitu menggunakan standard deviation method  interquartile range method  dan automatic outlier detection', 'type': 'extractive', 'score': 0.0, 'context': 'beberapa cara untuk mengidentifikasi outlier yaitu menggunakan standard deviation method  interquartile range method  dan automatic outlier detection metode pertama adalah standard deviation method  yang cocok digunakan pada sample dengan distribusi nilai gaussian atau mendekati distribusi gaussian pada distribusi gaussian  sebarapa jauh standard deviasi dari nilai rata ratanya menunjukkan persentase nilai pada sample menggunakan cut off standard deviasi dari mean untuk identifikasi outlier metode kedua adalah interquartile range  iqr  method  yang dapat digunakan pada sample yang datanya tidak mengikuti distribusi normal  gaussian', 'offsets_in_document': [{'start': 0, 'end': 149}], 'offsets_in_context': [{'start': 0, 'end': 149}], 'document_ids': ['521aa1ef0045e6c6c9159c97a45a1e9b-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja metode untuk mengidentifikasi outlier?', 'filters': None, 'id': 'c62956c78a956aca61b0f372cdfbbd91', 'no_answer': False, 'answers': ['beberapa cara untuk mengidentifikasi outlier yaitu menggunakan standard deviation method  interquartile range method  dan automatic outlier detection'], 'offsets_in_documents': [{'start': 0, 'end': 149}], 'offsets_in_contexts': [{'start': 0, 'end': 149}], 'document_ids': ['521aa1ef0045e6c6c9159c97a45a1e9b-0'], 'contexts': ['beberapa cara untuk mengidentifikasi outlier yaitu menggunakan standard deviation method  interquartile range method  dan automatic outlier detection metode pertama adalah standard deviation method  yang cocok digunakan pada sample dengan distribusi nilai gaussian atau mendekati distribusi gaussian pada distribusi gaussian  sebarapa jauh standard deviasi dari nilai rata ratanya menunjukkan persentase nilai pada sample menggunakan cut off standard deviasi dari mean untuk identifikasi outlier metode kedua adalah interquartile range  iqr  method  yang dapat digunakan pada sample yang datanya tidak mengikuti distribusi normal  gaussian']}>,\n",
              " <MultiLabel: {'labels': [{'id': '7e93123a-5a8b-4622-892c-0b3163af4076', 'query': 'apa fungsi method df.drop   pada library pandas ', 'document': {'id': 'e64db31249f8a925c59dc39e25232c66-0', 'content': 'tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan menggunakan method df drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1183, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'df drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data ', 'type': 'extractive', 'score': 0.0, 'context': 'tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan menggunakan method df drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df', 'offsets_in_document': [{'start': 387, 'end': 521}], 'offsets_in_context': [{'start': 387, 'end': 521}], 'document_ids': ['e64db31249f8a925c59dc39e25232c66-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa fungsi method df.drop   pada library pandas ', 'filters': None, 'id': '8fdc73d7409c7966de88127a914894a3', 'no_answer': False, 'answers': ['df drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data '], 'offsets_in_documents': [{'start': 387, 'end': 521}], 'offsets_in_contexts': [{'start': 387, 'end': 521}], 'document_ids': ['e64db31249f8a925c59dc39e25232c66-0'], 'contexts': ['tugas dalam data cleaning meliputi menghilangkan data yang tidak relevan termasuk duplikasi data  memperbaiki structural errors  menangani outliers  menangani missing data  serta validasi data dan qa  quality assurance yang pertama yaitu menghilangkan data yang tidak relevan  menghapus data yang tidak sesuai dengan permasalahan dan kolom fitur yang tidak diperlukan menggunakan method df drop   dari library pandas untuk menghapus kolom yang berisi nilai unik pada setiap barisnya atau kolom dengan banyak missing data mengidentifikasi dan hapus kolom yang hanya berisi single value  zero variance  menggunakan method df']}>,\n",
              " <MultiLabel: {'labels': [{'id': '5b6610ba-b547-4ccf-8b39-589c9d951d23', 'query': 'apa kelemahan dari metode undersampling ', 'document': {'id': '4d720ee72bdf820ec50a5aefd8b13577-0', 'content': 'metodenya yaitu undersampling  oversampling  dan hybrid sampling undersampling akan mengeliminasi sampel dari kelas mayoritas kelemahannya berpotensi mengeliminasi sampel yang berkualitas untuk proses pembelajaran keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1204, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'erpotensi mengeliminasi sampel yang berkualitas untuk proses pembelajaran', 'type': 'extractive', 'score': 0.0, 'context': 'metodenya yaitu undersampling  oversampling  dan hybrid sampling undersampling akan mengeliminasi sampel dari kelas mayoritas kelemahannya berpotensi mengeliminasi sampel yang berkualitas untuk proses pembelajaran keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn', 'offsets_in_document': [{'start': 140, 'end': 213}], 'offsets_in_context': [{'start': 140, 'end': 213}], 'document_ids': ['4d720ee72bdf820ec50a5aefd8b13577-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa kelemahan dari metode undersampling ', 'filters': None, 'id': '6c44d088699a1da35078eda728c13fc8', 'no_answer': False, 'answers': ['erpotensi mengeliminasi sampel yang berkualitas untuk proses pembelajaran'], 'offsets_in_documents': [{'start': 140, 'end': 213}], 'offsets_in_contexts': [{'start': 140, 'end': 213}], 'document_ids': ['4d720ee72bdf820ec50a5aefd8b13577-0'], 'contexts': ['metodenya yaitu undersampling  oversampling  dan hybrid sampling undersampling akan mengeliminasi sampel dari kelas mayoritas kelemahannya berpotensi mengeliminasi sampel yang berkualitas untuk proses pembelajaran keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn']}>,\n",
              " <MultiLabel: {'labels': [{'id': '02e07c73-afaf-47a3-8822-0e529d7d0c70', 'query': 'bagaimana cara mengimplementasikan adasyn ', 'document': {'id': '76e5ae290bcffa38a1f1f668a8ba7ce8-0', 'content': 'adasyn adalah sampel dari kelas minoritas diciptakan berdasarkan distribusi densitasnya adasyn menghasilkan lebih banyak data sintetis dari kelas minortas yang lebih sulit dipelajari dibandingkan dengan sampel minoritas yang lebih mudah dipelajari dapat diimplementasikan dengan class adasyn dari library imblearn over_sampling hybrid method adalah kombinasi dari oversampling dan undersampling untuk menyeimbangkan jumlah sampel dari setiap kelas dalam dataset', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1224, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' class adasyn dari library imblearn ', 'type': 'extractive', 'score': 0.0, 'context': 'adasyn adalah sampel dari kelas minoritas diciptakan berdasarkan distribusi densitasnya adasyn menghasilkan lebih banyak data sintetis dari kelas minortas yang lebih sulit dipelajari dibandingkan dengan sampel minoritas yang lebih mudah dipelajari dapat diimplementasikan dengan class adasyn dari library imblearn over_sampling hybrid method adalah kombinasi dari oversampling dan undersampling untuk menyeimbangkan jumlah sampel dari setiap kelas dalam dataset', 'offsets_in_document': [{'start': 278, 'end': 314}], 'offsets_in_context': [{'start': 278, 'end': 314}], 'document_ids': ['76e5ae290bcffa38a1f1f668a8ba7ce8-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mengimplementasikan adasyn ', 'filters': None, 'id': '9bfc65b6c43842c9090216ffc69db427', 'no_answer': False, 'answers': [' class adasyn dari library imblearn '], 'offsets_in_documents': [{'start': 278, 'end': 314}], 'offsets_in_contexts': [{'start': 278, 'end': 314}], 'document_ids': ['76e5ae290bcffa38a1f1f668a8ba7ce8-0'], 'contexts': ['adasyn adalah sampel dari kelas minoritas diciptakan berdasarkan distribusi densitasnya adasyn menghasilkan lebih banyak data sintetis dari kelas minortas yang lebih sulit dipelajari dibandingkan dengan sampel minoritas yang lebih mudah dipelajari dapat diimplementasikan dengan class adasyn dari library imblearn over_sampling hybrid method adalah kombinasi dari oversampling dan undersampling untuk menyeimbangkan jumlah sampel dari setiap kelas dalam dataset']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f9a85744-57d4-4e12-a25f-17ed52730953', 'query': 'apa itu missing value imputation?', 'document': {'id': 'a06b1894fb3187c6703b6b5f8c0e5d5e-0', 'content': 'terdapat beberapa strategi untuk menangani missing value  seperti menghapus kolom atau baris yang berisi missing value  serta missing value imputation yang mana mengidentifikasi missing value dalam dataset  lalu mengisinya dengan nilai hasil estimasi contohnya seperti statistical imputation  knn imputation  iterative imputation statistical imputation memiliki salah satu keuntungan yaitu sederhana karena nilai statistik dapat dihitung dengan cepat dan terbukti efektif pada berbagai kasus statistical imputation menggunakan metode statistik untuk mengestimasi nilai pada sebuah kolom yang terdapat missing value  lalu menggantikan semua missing value dengan nilai tersebut contohnya yaitu column mean value  median value  mode value  serta constant value', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1193, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'missing value imputation yang mana mengidentifikasi missing value dalam dataset  lalu mengisinya dengan nilai hasil estimasi ', 'type': 'extractive', 'score': 0.0, 'context': 'terdapat beberapa strategi untuk menangani missing value  seperti menghapus kolom atau baris yang berisi missing value  serta missing value imputation yang mana mengidentifikasi missing value dalam dataset  lalu mengisinya dengan nilai hasil estimasi contohnya seperti statistical imputation  knn imputation  iterative imputation statistical imputation memiliki salah satu keuntungan yaitu sederhana karena nilai statistik dapat dihitung dengan cepat dan terbukti efektif pada berbagai kasus statistical imputation menggunakan metode statistik untuk mengestimasi nilai pada sebuah kolom yang terdapat missing value  lalu menggantikan semua missing value dengan nilai tersebut contohnya yaitu column mean value  median value  mode value  serta constant value', 'offsets_in_document': [{'start': 126, 'end': 251}], 'offsets_in_context': [{'start': 126, 'end': 251}], 'document_ids': ['a06b1894fb3187c6703b6b5f8c0e5d5e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu missing value imputation?', 'filters': None, 'id': '1747456af91845f4f1aa2140449a3772', 'no_answer': False, 'answers': ['missing value imputation yang mana mengidentifikasi missing value dalam dataset  lalu mengisinya dengan nilai hasil estimasi '], 'offsets_in_documents': [{'start': 126, 'end': 251}], 'offsets_in_contexts': [{'start': 126, 'end': 251}], 'document_ids': ['a06b1894fb3187c6703b6b5f8c0e5d5e-0'], 'contexts': ['terdapat beberapa strategi untuk menangani missing value  seperti menghapus kolom atau baris yang berisi missing value  serta missing value imputation yang mana mengidentifikasi missing value dalam dataset  lalu mengisinya dengan nilai hasil estimasi contohnya seperti statistical imputation  knn imputation  iterative imputation statistical imputation memiliki salah satu keuntungan yaitu sederhana karena nilai statistik dapat dihitung dengan cepat dan terbukti efektif pada berbagai kasus statistical imputation menggunakan metode statistik untuk mengestimasi nilai pada sebuah kolom yang terdapat missing value  lalu menggantikan semua missing value dengan nilai tersebut contohnya yaitu column mean value  median value  mode value  serta constant value']}>,\n",
              " <MultiLabel: {'labels': [{'id': '2c62b9a9-231e-404f-94e8-5089e09fdfe4', 'query': 'algoritma apa yang hanya menggunakan nilai numerik?', 'document': {'id': 'd3fb60630af82ecf494a23f77bffab91-0', 'content': 'alternatifnya dapat dilakukan eksperimen menggunakan normalisasi dan standardisasi  atau bahkan kombinasi keduanya lalu dilihat hasil yang didapat pada saat evaluasi mana yang lebih baik categorical data dapat berupa variabel nominal dan variabel ordinal variabel nominal adalah variabel yang terdiri atas sekumpulan nilai diskret tanpa adanya hubungan urutan di antara nilai nilai tersebut variabel ordinal adalah variabel yang terdiri atas sekumpulan nilai diskret dengan adanya keterurutan di antara nilai nilai tersebut beberapa algoritma ml mengharuskan semua nilai variabel input dan output berupa nilai numerik  seperti artificial neural network  support vector machine  dsb', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1199, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'beberapa algoritma ml mengharuskan semua nilai variabel input dan output berupa nilai numerik  seperti artificial neural network  support vector machine', 'type': 'extractive', 'score': 0.0, 'context': 'alternatifnya dapat dilakukan eksperimen menggunakan normalisasi dan standardisasi  atau bahkan kombinasi keduanya lalu dilihat hasil yang didapat pada saat evaluasi mana yang lebih baik categorical data dapat berupa variabel nominal dan variabel ordinal variabel nominal adalah variabel yang terdiri atas sekumpulan nilai diskret tanpa adanya hubungan urutan di antara nilai nilai tersebut variabel ordinal adalah variabel yang terdiri atas sekumpulan nilai diskret dengan adanya keterurutan di antara nilai nilai tersebut beberapa algoritma ml mengharuskan semua nilai variabel input dan output berupa nilai numerik  seperti artificial neural network  support vector machine  dsb', 'offsets_in_document': [{'start': 524, 'end': 676}], 'offsets_in_context': [{'start': 524, 'end': 676}], 'document_ids': ['d3fb60630af82ecf494a23f77bffab91-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'algoritma apa yang hanya menggunakan nilai numerik?', 'filters': None, 'id': 'ec66e459cfa8bf6662b24df72bd7e501', 'no_answer': False, 'answers': ['beberapa algoritma ml mengharuskan semua nilai variabel input dan output berupa nilai numerik  seperti artificial neural network  support vector machine'], 'offsets_in_documents': [{'start': 524, 'end': 676}], 'offsets_in_contexts': [{'start': 524, 'end': 676}], 'document_ids': ['d3fb60630af82ecf494a23f77bffab91-0'], 'contexts': ['alternatifnya dapat dilakukan eksperimen menggunakan normalisasi dan standardisasi  atau bahkan kombinasi keduanya lalu dilihat hasil yang didapat pada saat evaluasi mana yang lebih baik categorical data dapat berupa variabel nominal dan variabel ordinal variabel nominal adalah variabel yang terdiri atas sekumpulan nilai diskret tanpa adanya hubungan urutan di antara nilai nilai tersebut variabel ordinal adalah variabel yang terdiri atas sekumpulan nilai diskret dengan adanya keterurutan di antara nilai nilai tersebut beberapa algoritma ml mengharuskan semua nilai variabel input dan output berupa nilai numerik  seperti artificial neural network  support vector machine  dsb']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b5b1b9a9-5076-4e5f-95db-8320d8cdabda', 'query': 'kenapa diperlukan transformasi data numeric ke categorical?', 'document': {'id': '260459b2d4e07d5a4678ad87d3c5b494-0', 'content': 'setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn transformasi nilai numerik ke categorical diperlukan karena beberapa algoritma ml  seperti decision tree atau rule based algorithm memerlukan input berupa kategori atau variabel ordinal serta banyak algoritma ml yang bekerja lebih baik saat variabel input numerik yang digunakan mengikuti standard probability distribution solusinya transformasi variabel numerik menjadi nilai diskret dimana setiap nilai diberikan label yang memiliki keterurutan discretization adalah transformasi nilai variabel input output numerik menjadi nilai diskret atau label ordinal', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1210, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'transformasi nilai numerik ke categorical diperlukan karena beberapa algoritma ml  seperti decision tree atau rule based algorithm memerlukan input berupa kategori atau variabel ordinal serta banyak algoritma ml yang bekerja lebih baik saat variabel input numerik yang digunakan mengikuti standard probability distribution', 'type': 'extractive', 'score': 0.0, 'context': 'setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn transformasi nilai numerik ke categorical diperlukan karena beberapa algoritma ml  seperti decision tree atau rule based algorithm memerlukan input berupa kategori atau variabel ordinal serta banyak algoritma ml yang bekerja lebih baik saat variabel input numerik yang digunakan mengikuti standard probability distribution solusinya transformasi variabel numerik menjadi nilai diskret dimana setiap nilai diberikan label yang memiliki keterurutan discretization adalah transformasi nilai variabel input output numerik menjadi nilai diskret atau label ordinal', 'offsets_in_document': [{'start': 151, 'end': 473}], 'offsets_in_context': [{'start': 151, 'end': 473}], 'document_ids': ['260459b2d4e07d5a4678ad87d3c5b494-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'kenapa diperlukan transformasi data numeric ke categorical?', 'filters': None, 'id': '8d0f98e7c94d43eaf89baa60e159a802', 'no_answer': False, 'answers': ['transformasi nilai numerik ke categorical diperlukan karena beberapa algoritma ml  seperti decision tree atau rule based algorithm memerlukan input berupa kategori atau variabel ordinal serta banyak algoritma ml yang bekerja lebih baik saat variabel input numerik yang digunakan mengikuti standard probability distribution'], 'offsets_in_documents': [{'start': 151, 'end': 473}], 'offsets_in_contexts': [{'start': 151, 'end': 473}], 'document_ids': ['260459b2d4e07d5a4678ad87d3c5b494-0'], 'contexts': ['setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn transformasi nilai numerik ke categorical diperlukan karena beberapa algoritma ml  seperti decision tree atau rule based algorithm memerlukan input berupa kategori atau variabel ordinal serta banyak algoritma ml yang bekerja lebih baik saat variabel input numerik yang digunakan mengikuti standard probability distribution solusinya transformasi variabel numerik menjadi nilai diskret dimana setiap nilai diberikan label yang memiliki keterurutan discretization adalah transformasi nilai variabel input output numerik menjadi nilai diskret atau label ordinal']}>,\n",
              " <MultiLabel: {'labels': [{'id': '698b40db-16af-4498-9a42-8620d3606d5a', 'query': 'bagaimana linear feature extraction bekerja?', 'document': {'id': 'fe1054ff949aa343ae97c94501805627-0', 'content': 'dua kategori utama ekstraksi fitur adalah linear dan non linear linear feature extraction memproyeksikan mentransformasi data secara linear contoh   pca  principle component analysis   lda  linear discriminant analysis nonlinear feature extraction memetakan data ke high dimensional space menggunakan lifting function sehingga relasi antarfitur dapat dipandang secara linear dan lebih mudah dideteksi  lalu data dipetakan lagi ke ruang dimensi yang lebih rendah sehingga relasi antar fitur kini dapat dipandang secara non linear contoh   wavelet transform  radon transform', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1230, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'linear feature extraction memproyeksikan mentransformasi data secara linear', 'type': 'extractive', 'score': 0.0, 'context': 'dua kategori utama ekstraksi fitur adalah linear dan non linear linear feature extraction memproyeksikan mentransformasi data secara linear contoh   pca  principle component analysis   lda  linear discriminant analysis nonlinear feature extraction memetakan data ke high dimensional space menggunakan lifting function sehingga relasi antarfitur dapat dipandang secara linear dan lebih mudah dideteksi  lalu data dipetakan lagi ke ruang dimensi yang lebih rendah sehingga relasi antar fitur kini dapat dipandang secara non linear contoh   wavelet transform  radon transform', 'offsets_in_document': [{'start': 64, 'end': 139}], 'offsets_in_context': [{'start': 64, 'end': 139}], 'document_ids': ['fe1054ff949aa343ae97c94501805627-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana linear feature extraction bekerja?', 'filters': None, 'id': '0c36e07ea3da6798e49dc9d40c4f1ef6', 'no_answer': False, 'answers': ['linear feature extraction memproyeksikan mentransformasi data secara linear'], 'offsets_in_documents': [{'start': 64, 'end': 139}], 'offsets_in_contexts': [{'start': 64, 'end': 139}], 'document_ids': ['fe1054ff949aa343ae97c94501805627-0'], 'contexts': ['dua kategori utama ekstraksi fitur adalah linear dan non linear linear feature extraction memproyeksikan mentransformasi data secara linear contoh   pca  principle component analysis   lda  linear discriminant analysis nonlinear feature extraction memetakan data ke high dimensional space menggunakan lifting function sehingga relasi antarfitur dapat dipandang secara linear dan lebih mudah dideteksi  lalu data dipetakan lagi ke ruang dimensi yang lebih rendah sehingga relasi antar fitur kini dapat dipandang secara non linear contoh   wavelet transform  radon transform']}>,\n",
              " <MultiLabel: {'labels': [{'id': '52bdc95f-aebf-48ff-a809-d84104d920cd', 'query': 'apa saja yang termasuk dalam perbaikan structural errors ', 'document': {'id': 'a8b7fa8e13e284ae47bc26f01b2dbbbc-0', 'content': 'mengidentifikasi dan hapus duplikasi baris data yang dapat terjadi saat pengumpulan data dari beberapa sumber atau pengulangan observasi eksperimen duplikasi baris tidak memberikan informasi tambahan dan dapat mengakibatkan bias pada evaluasi model tugas kedua dalam data cleaning selanjutnya yaitu memperbaiki structural errors yang mana meliputi perbaikan kesalahan pada penamaan  typo  incorrect capitalization  atau inkonsistensi pada kategori atau pelabelan data  merapikan isi kolom date of publication dan isi kolom place of publication selain itu  terdapat penanganan outlier dalam tugas data cleaning tidak ada definisi dan metode yang pasti untuk mengidentifikasi outlier karena bergantung pada dataset yang spesifik', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1196, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'perbaikan kesalahan pada penamaan  typo  incorrect capitalization  atau inkonsistensi pada kategori atau pelabelan data  merapikan isi kolom date of publication dan isi kolom place of publication', 'type': 'extractive', 'score': 0.0, 'context': 'mengidentifikasi dan hapus duplikasi baris data yang dapat terjadi saat pengumpulan data dari beberapa sumber atau pengulangan observasi eksperimen duplikasi baris tidak memberikan informasi tambahan dan dapat mengakibatkan bias pada evaluasi model tugas kedua dalam data cleaning selanjutnya yaitu memperbaiki structural errors yang mana meliputi perbaikan kesalahan pada penamaan  typo  incorrect capitalization  atau inkonsistensi pada kategori atau pelabelan data  merapikan isi kolom date of publication dan isi kolom place of publication selain itu  terdapat penanganan outlier dalam tugas data cleaning tidak ada definisi dan metode yang pasti untuk mengidentifikasi outlier karena bergantung pada dataset yang spesifik', 'offsets_in_document': [{'start': 348, 'end': 543}], 'offsets_in_context': [{'start': 348, 'end': 543}], 'document_ids': ['a8b7fa8e13e284ae47bc26f01b2dbbbc-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja yang termasuk dalam perbaikan structural errors ', 'filters': None, 'id': '8e3159b205898c1690725e46c1182d4b', 'no_answer': False, 'answers': ['perbaikan kesalahan pada penamaan  typo  incorrect capitalization  atau inkonsistensi pada kategori atau pelabelan data  merapikan isi kolom date of publication dan isi kolom place of publication'], 'offsets_in_documents': [{'start': 348, 'end': 543}], 'offsets_in_contexts': [{'start': 348, 'end': 543}], 'document_ids': ['a8b7fa8e13e284ae47bc26f01b2dbbbc-0'], 'contexts': ['mengidentifikasi dan hapus duplikasi baris data yang dapat terjadi saat pengumpulan data dari beberapa sumber atau pengulangan observasi eksperimen duplikasi baris tidak memberikan informasi tambahan dan dapat mengakibatkan bias pada evaluasi model tugas kedua dalam data cleaning selanjutnya yaitu memperbaiki structural errors yang mana meliputi perbaikan kesalahan pada penamaan  typo  incorrect capitalization  atau inkonsistensi pada kategori atau pelabelan data  merapikan isi kolom date of publication dan isi kolom place of publication selain itu  terdapat penanganan outlier dalam tugas data cleaning tidak ada definisi dan metode yang pasti untuk mengidentifikasi outlier karena bergantung pada dataset yang spesifik']}>,\n",
              " <MultiLabel: {'labels': [{'id': '6ecc0a8f-0e90-4434-9184-49be75ac1e88', 'query': 'bagaimana cara menerapkan filter method ', 'document': {'id': '6f90b04350dc02bb2c88f525a54af0ba-0', 'content': 'filter method tidak memiliki ketergantungan  indenpenden  terhadap algoritma pembelajaran yang digunakan contoh  fisher score  information gain  relief  relieff  fast correlation based filter  fcbf   chi squared  dll metode filter biasanya terdiri atas dua langkah  yaitu  1  merangking fitur berdasarkan kriteria tertentu dengan melakukan evaluasi fitur yang dapat dilakukan secara univariate  setiap feature di ranking secara independen  sehingga tidak mampu menangani redundant features  atau multivariate  pe rangking an fitur dilakukan secara batch  mampu menangani redundant features  dan  2  sejumlah fitur dengan ranking tertinggi dipilih sebagai input ke algoritma pembelajaran kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1226, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'metode filter biasanya terdiri atas dua langkah  yaitu  1  merangking fitur berdasarkan kriteria tertentu dengan melakukan evaluasi fitur yang dapat dilakukan secara univariate  setiap feature di ranking secara independen  sehingga tidak mampu menangani redundant features  atau multivariate  pe rangking an fitur dilakukan secara batch  mampu menangani redundant features  dan  2  sejumlah fitur dengan ranking tertinggi dipilih sebagai input ke algoritma pembelajaran ', 'type': 'extractive', 'score': 0.0, 'context': 'filter method tidak memiliki ketergantungan  indenpenden  terhadap algoritma pembelajaran yang digunakan contoh  fisher score  information gain  relief  relieff  fast correlation based filter  fcbf   chi squared  dll metode filter biasanya terdiri atas dua langkah  yaitu  1  merangking fitur berdasarkan kriteria tertentu dengan melakukan evaluasi fitur yang dapat dilakukan secara univariate  setiap feature di ranking secara independen  sehingga tidak mampu menangani redundant features  atau multivariate  pe rangking an fitur dilakukan secara batch  mampu menangani redundant features  dan  2  sejumlah fitur dengan ranking tertinggi dipilih sebagai input ke algoritma pembelajaran kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur', 'offsets_in_document': [{'start': 217, 'end': 687}], 'offsets_in_context': [{'start': 217, 'end': 687}], 'document_ids': ['6f90b04350dc02bb2c88f525a54af0ba-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara menerapkan filter method ', 'filters': None, 'id': '6096013ca84bcd7ccfabc0d613e0c3fe', 'no_answer': False, 'answers': ['metode filter biasanya terdiri atas dua langkah  yaitu  1  merangking fitur berdasarkan kriteria tertentu dengan melakukan evaluasi fitur yang dapat dilakukan secara univariate  setiap feature di ranking secara independen  sehingga tidak mampu menangani redundant features  atau multivariate  pe rangking an fitur dilakukan secara batch  mampu menangani redundant features  dan  2  sejumlah fitur dengan ranking tertinggi dipilih sebagai input ke algoritma pembelajaran '], 'offsets_in_documents': [{'start': 217, 'end': 687}], 'offsets_in_contexts': [{'start': 217, 'end': 687}], 'document_ids': ['6f90b04350dc02bb2c88f525a54af0ba-0'], 'contexts': ['filter method tidak memiliki ketergantungan  indenpenden  terhadap algoritma pembelajaran yang digunakan contoh  fisher score  information gain  relief  relieff  fast correlation based filter  fcbf   chi squared  dll metode filter biasanya terdiri atas dua langkah  yaitu  1  merangking fitur berdasarkan kriteria tertentu dengan melakukan evaluasi fitur yang dapat dilakukan secara univariate  setiap feature di ranking secara independen  sehingga tidak mampu menangani redundant features  atau multivariate  pe rangking an fitur dilakukan secara batch  mampu menangani redundant features  dan  2  sejumlah fitur dengan ranking tertinggi dipilih sebagai input ke algoritma pembelajaran kelebihan filter method yaitu  1  independen terhadap algoritma pembelajaran sehingga bias pada algoritma pembelajaran tidak akan berpengaruh terhadap proses seleksi fitur dan  2  waktu komputasi yang lebih cepat   efisien kekurangan filter method yaitu akurasi pada algoritma pembelajaran bisa jadi tidak optimal karena tidak dilibatkan dalam proses seleksi fitur']}>,\n",
              " <MultiLabel: {'labels': [{'id': '5f0a9874-716c-4713-bcff-978d122cae63', 'query': 'bagaimana cara kerja algoritma relief ', 'document': {'id': 'ffadbf583f52637f1737ce916f8d6fde-0', 'content': 'algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1234, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat', 'type': 'extractive', 'score': 0.0, 'context': 'algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist', 'offsets_in_document': [{'start': 258, 'end': 389}], 'offsets_in_context': [{'start': 258, 'end': 389}], 'document_ids': ['ffadbf583f52637f1737ce916f8d6fde-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara kerja algoritma relief ', 'filters': None, 'id': '4719568c2e3ce78a0640d0d9ac2e6579', 'no_answer': False, 'answers': ['algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat'], 'offsets_in_documents': [{'start': 258, 'end': 389}], 'offsets_in_contexts': [{'start': 258, 'end': 389}], 'document_ids': ['ffadbf583f52637f1737ce916f8d6fde-0'], 'contexts': ['algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e0ecabed-570f-45c8-b05e-1a0f26edf86c', 'query': 'apa salah satu yang dihitung pada algoritma relief ', 'document': {'id': 'ffadbf583f52637f1737ce916f8d6fde-0', 'content': 'algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1234, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'algoritma relief menghitung bobot  quality estimation  untuk setiap fitur ', 'type': 'extractive', 'score': 0.0, 'context': 'algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist', 'offsets_in_document': [{'start': 258, 'end': 332}], 'offsets_in_context': [{'start': 258, 'end': 332}], 'document_ids': ['ffadbf583f52637f1737ce916f8d6fde-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa salah satu yang dihitung pada algoritma relief ', 'filters': None, 'id': '22ad33ce3487caedd557e17c6d4b7ecc', 'no_answer': False, 'answers': ['algoritma relief menghitung bobot  quality estimation  untuk setiap fitur '], 'offsets_in_documents': [{'start': 258, 'end': 332}], 'offsets_in_contexts': [{'start': 258, 'end': 332}], 'document_ids': ['ffadbf583f52637f1737ce916f8d6fde-0'], 'contexts': ['algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'fe74c2f7-2962-4286-ac3b-891f28fa985f', 'query': 'mengapa algoritma fisher score tidak mampu menangani redundant features ', 'document': {'id': 'ffadbf583f52637f1737ce916f8d6fde-0', 'content': 'algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1234, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'algoritma fisher score mengevaluasi fitur secara independen', 'type': 'extractive', 'score': 0.0, 'context': 'algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist', 'offsets_in_document': [{'start': 0, 'end': 59}], 'offsets_in_context': [{'start': 0, 'end': 59}], 'document_ids': ['ffadbf583f52637f1737ce916f8d6fde-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'mengapa algoritma fisher score tidak mampu menangani redundant features ', 'filters': None, 'id': 'd27c7fbea214f5ffd587b3d271bb3397', 'no_answer': False, 'answers': ['algoritma fisher score mengevaluasi fitur secara independen'], 'offsets_in_documents': [{'start': 0, 'end': 59}], 'offsets_in_contexts': [{'start': 0, 'end': 59}], 'document_ids': ['ffadbf583f52637f1737ce916f8d6fde-0'], 'contexts': ['algoritma fisher score mengevaluasi fitur secara independen sehingga tidak mampu menangani redundant features algoritma information gain mengukur dependensi antara fitur terhadap label kelas sebuah fitur dikatakan relevan jika mempunyai nilai ig yang tinggi algoritma relief menghitung bobot  quality estimation  untuk setiap fitur berdasarkan kemampuannya untuk membedakan sampel terdekat algoritma relief memilih sampel r i secara random  lalu mencari sampel terdekat dari kelas yang sama  nearest hit  dan sampel terdekat dari kelas yang berbeda  nearest mist']}>,\n",
              " <MultiLabel: {'labels': [{'id': '84dfd02e-6b4c-4035-84a8-e68b7d5ed610', 'query': 'bagaimana cara mengubah data categorical ke bentuk numerik?', 'document': {'id': '3df32a4e6239011adee7d90653f9d7da-0', 'content': 'variabel ordinal adalah variabel yang terdiri atas sekumpulan nilai diskret dengan adanya keterurutan di antara nilai nilai tersebut beberapa algoritma ml mengharuskan semua nilai variabel input dan output berupa nilai numerik  seperti artificial neural network  support vector machine  dsb jika variabel input atau output mengandung categorical data  maka harus diubah ke bentuk numerik cara yang populer untuk mengubah data categorical ke numerik adalah dengan ordinal encoding dan one hot encoding ordinal encoding digunakan untuk encoding variabel ordinal ke nilai numerik', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1201, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'cara yang populer untuk mengubah data categorical ke numerik adalah dengan ordinal encoding dan one hot encoding ', 'type': 'extractive', 'score': 0.0, 'context': 'variabel ordinal adalah variabel yang terdiri atas sekumpulan nilai diskret dengan adanya keterurutan di antara nilai nilai tersebut beberapa algoritma ml mengharuskan semua nilai variabel input dan output berupa nilai numerik  seperti artificial neural network  support vector machine  dsb jika variabel input atau output mengandung categorical data  maka harus diubah ke bentuk numerik cara yang populer untuk mengubah data categorical ke numerik adalah dengan ordinal encoding dan one hot encoding ordinal encoding digunakan untuk encoding variabel ordinal ke nilai numerik', 'offsets_in_document': [{'start': 388, 'end': 501}], 'offsets_in_context': [{'start': 388, 'end': 501}], 'document_ids': ['3df32a4e6239011adee7d90653f9d7da-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara mengubah data categorical ke bentuk numerik?', 'filters': None, 'id': '3d87465b36b042f4ab2ea496b9fb091e', 'no_answer': False, 'answers': ['cara yang populer untuk mengubah data categorical ke numerik adalah dengan ordinal encoding dan one hot encoding '], 'offsets_in_documents': [{'start': 388, 'end': 501}], 'offsets_in_contexts': [{'start': 388, 'end': 501}], 'document_ids': ['3df32a4e6239011adee7d90653f9d7da-0'], 'contexts': ['variabel ordinal adalah variabel yang terdiri atas sekumpulan nilai diskret dengan adanya keterurutan di antara nilai nilai tersebut beberapa algoritma ml mengharuskan semua nilai variabel input dan output berupa nilai numerik  seperti artificial neural network  support vector machine  dsb jika variabel input atau output mengandung categorical data  maka harus diubah ke bentuk numerik cara yang populer untuk mengubah data categorical ke numerik adalah dengan ordinal encoding dan one hot encoding ordinal encoding digunakan untuk encoding variabel ordinal ke nilai numerik']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e4284fac-6542-422c-a059-23f5c41e583d', 'query': 'apa itu smote?', 'document': {'id': 'bb4cf4fadff991f270e01b540ca5f5a1-0', 'content': 'kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn over_sampling smote adalah sebuah sampel sintetis dari kelas minoritas dibuat dengan cara interpolasi di antara k tetangga terdekatnya dapat diimlementasikan menggunakan class smote dari library imblearn', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1206, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'smote adalah sebuah sampel sintetis dari kelas minoritas dibuat dengan cara interpolasi di antara k tetangga terdekatnya ', 'type': 'extractive', 'score': 0.0, 'context': 'kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn over_sampling smote adalah sebuah sampel sintetis dari kelas minoritas dibuat dengan cara interpolasi di antara k tetangga terdekatnya dapat diimlementasikan menggunakan class smote dari library imblearn', 'offsets_in_document': [{'start': 176, 'end': 297}], 'offsets_in_context': [{'start': 176, 'end': 297}], 'document_ids': ['bb4cf4fadff991f270e01b540ca5f5a1-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa itu smote?', 'filters': None, 'id': 'e29379f510008c57a0a8e30cd944e332', 'no_answer': False, 'answers': ['smote adalah sebuah sampel sintetis dari kelas minoritas dibuat dengan cara interpolasi di antara k tetangga terdekatnya '], 'offsets_in_documents': [{'start': 176, 'end': 297}], 'offsets_in_contexts': [{'start': 176, 'end': 297}], 'document_ids': ['bb4cf4fadff991f270e01b540ca5f5a1-0'], 'contexts': ['kelemahannya dapat meningkatkan resiko overfitting karena adanya replikasi data dapat diimplementasikan menggunakan class randomoversampler dari library imblearn over_sampling smote adalah sebuah sampel sintetis dari kelas minoritas dibuat dengan cara interpolasi di antara k tetangga terdekatnya dapat diimlementasikan menggunakan class smote dari library imblearn']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c593d638-818a-47fe-b37c-3f7b1944ff25', 'query': 'bagaimana cara kerja estimated nearest neighbor?', 'document': {'id': '6c4b691406cbf5c0aea520bf23ed478c-0', 'content': 'under_sampling cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya dapat diimplementasikan menggunakan class editednearestneighbours dari library imblearn under_sampling oversampling akan meningkatkan jumlah sampel dari kelas minoritas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1216, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya', 'type': 'extractive', 'score': 0.0, 'context': 'under_sampling cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya dapat diimplementasikan menggunakan class editednearestneighbours dari library imblearn under_sampling oversampling akan meningkatkan jumlah sampel dari kelas minoritas', 'offsets_in_document': [{'start': 15, 'end': 132}], 'offsets_in_context': [{'start': 15, 'end': 132}], 'document_ids': ['6c4b691406cbf5c0aea520bf23ed478c-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara kerja estimated nearest neighbor?', 'filters': None, 'id': '183390ca83fad68a8db282e098000649', 'no_answer': False, 'answers': ['cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya'], 'offsets_in_documents': [{'start': 15, 'end': 132}], 'offsets_in_contexts': [{'start': 15, 'end': 132}], 'document_ids': ['6c4b691406cbf5c0aea520bf23ed478c-0'], 'contexts': ['under_sampling cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya dapat diimplementasikan menggunakan class editednearestneighbours dari library imblearn under_sampling oversampling akan meningkatkan jumlah sampel dari kelas minoritas']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3d1d53ef-4d7c-4714-9e50-fe1a280e767f', 'query': 'apa yang dihasilkan oleh adasyn?', 'document': {'id': '907a0740068dcbba31d132a7631085ba-0', 'content': 'smote adalah sebuah sampel sintetis dari kelas minoritas dibuat dengan cara interpolasi di antara k tetangga terdekatnya dapat diimlementasikan menggunakan class smote dari library imblearn over_sampling adasyn adalah sampel dari kelas minoritas diciptakan berdasarkan distribusi densitasnya adasyn menghasilkan lebih banyak data sintetis dari kelas minortas yang lebih sulit dipelajari dibandingkan dengan sampel minoritas yang lebih mudah dipelajari', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1217, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'adasyn menghasilkan lebih banyak data sintetis dari kelas minortas yang lebih sulit dipelajari dibandingkan dengan sampel minoritas yang lebih mudah dipelajar', 'type': 'extractive', 'score': 0.0, 'context': 'smote adalah sebuah sampel sintetis dari kelas minoritas dibuat dengan cara interpolasi di antara k tetangga terdekatnya dapat diimlementasikan menggunakan class smote dari library imblearn over_sampling adasyn adalah sampel dari kelas minoritas diciptakan berdasarkan distribusi densitasnya adasyn menghasilkan lebih banyak data sintetis dari kelas minortas yang lebih sulit dipelajari dibandingkan dengan sampel minoritas yang lebih mudah dipelajari', 'offsets_in_document': [{'start': 292, 'end': 450}], 'offsets_in_context': [{'start': 292, 'end': 450}], 'document_ids': ['907a0740068dcbba31d132a7631085ba-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dihasilkan oleh adasyn?', 'filters': None, 'id': 'd751fe07f3451932fa9e41547b935e9f', 'no_answer': False, 'answers': ['adasyn menghasilkan lebih banyak data sintetis dari kelas minortas yang lebih sulit dipelajari dibandingkan dengan sampel minoritas yang lebih mudah dipelajar'], 'offsets_in_documents': [{'start': 292, 'end': 450}], 'offsets_in_contexts': [{'start': 292, 'end': 450}], 'document_ids': ['907a0740068dcbba31d132a7631085ba-0'], 'contexts': ['smote adalah sebuah sampel sintetis dari kelas minoritas dibuat dengan cara interpolasi di antara k tetangga terdekatnya dapat diimlementasikan menggunakan class smote dari library imblearn over_sampling adasyn adalah sampel dari kelas minoritas diciptakan berdasarkan distribusi densitasnya adasyn menghasilkan lebih banyak data sintetis dari kelas minortas yang lebih sulit dipelajari dibandingkan dengan sampel minoritas yang lebih mudah dipelajari']}>,\n",
              " <MultiLabel: {'labels': [{'id': '94a02b8b-5d46-47bb-9e48-cbc9614ae420', 'query': 'apa maksud dari nonlinear feature extraction?', 'document': {'id': '6d47ca3f0244ff579c330db883f33154-0', 'content': 'nonlinear feature extraction memetakan data ke high dimensional space menggunakan lifting function sehingga relasi antarfitur dapat dipandang secara linear dan lebih mudah dideteksi  lalu data dipetakan lagi ke ruang dimensi yang lebih rendah sehingga relasi antar fitur kini dapat dipandang secara non linear contoh   wavelet transform  radon transform pca  principal component analysis  adalah transformasi linear yang dapat digunakan untuk mereduksi dimensi data  namun tetap mempertahankan informasi yang penting sebanyak mungkin pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1227, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'nonlinear feature extraction memetakan data ke high dimensional space menggunakan lifting function sehingga relasi antarfitur dapat dipandang secara linear dan lebih mudah dideteksi  lalu data dipetakan lagi ke ruang dimensi yang lebih rendah sehingga relasi antar fitur kini dapat dipandang secara non linear', 'type': 'extractive', 'score': 0.0, 'context': 'nonlinear feature extraction memetakan data ke high dimensional space menggunakan lifting function sehingga relasi antarfitur dapat dipandang secara linear dan lebih mudah dideteksi  lalu data dipetakan lagi ke ruang dimensi yang lebih rendah sehingga relasi antar fitur kini dapat dipandang secara non linear contoh   wavelet transform  radon transform pca  principal component analysis  adalah transformasi linear yang dapat digunakan untuk mereduksi dimensi data  namun tetap mempertahankan informasi yang penting sebanyak mungkin pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset', 'offsets_in_document': [{'start': 0, 'end': 309}], 'offsets_in_context': [{'start': 0, 'end': 309}], 'document_ids': ['6d47ca3f0244ff579c330db883f33154-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa maksud dari nonlinear feature extraction?', 'filters': None, 'id': '44e897e1367e63241f00ad777a7f961c', 'no_answer': False, 'answers': ['nonlinear feature extraction memetakan data ke high dimensional space menggunakan lifting function sehingga relasi antarfitur dapat dipandang secara linear dan lebih mudah dideteksi  lalu data dipetakan lagi ke ruang dimensi yang lebih rendah sehingga relasi antar fitur kini dapat dipandang secara non linear'], 'offsets_in_documents': [{'start': 0, 'end': 309}], 'offsets_in_contexts': [{'start': 0, 'end': 309}], 'document_ids': ['6d47ca3f0244ff579c330db883f33154-0'], 'contexts': ['nonlinear feature extraction memetakan data ke high dimensional space menggunakan lifting function sehingga relasi antarfitur dapat dipandang secara linear dan lebih mudah dideteksi  lalu data dipetakan lagi ke ruang dimensi yang lebih rendah sehingga relasi antar fitur kini dapat dipandang secara non linear contoh   wavelet transform  radon transform pca  principal component analysis  adalah transformasi linear yang dapat digunakan untuk mereduksi dimensi data  namun tetap mempertahankan informasi yang penting sebanyak mungkin pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset']}>,\n",
              " <MultiLabel: {'labels': [{'id': '26697922-c2b0-4d3b-abe0-56781201ea04', 'query': 'apa saja jenis metode undersampling ', 'document': {'id': '64928af8204ef541c6b128127a408854-0', 'content': 'keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn random undersampling adalah metode non heuristic yang mengeliminasi sampel dari kelas mayoritas secara random sehingga jumlahnya sama dengan kelas minoritas  karena eliminasi sampel dilakukan secara random  maka sangat berpotensi menghilangkan sampel yang penting atau menjadikan sampel yang ada tidak merepresentasikan populasi secara umum dapat diimplementasikan menggunakan classs randomundersampler dari library imblearn under_sampling', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1212, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'tomek link  dan edited nearest neighbor  enn', 'type': 'extractive', 'score': 0.0, 'context': 'keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn random undersampling adalah metode non heuristic yang mengeliminasi sampel dari kelas mayoritas secara random sehingga jumlahnya sama dengan kelas minoritas  karena eliminasi sampel dilakukan secara random  maka sangat berpotensi menghilangkan sampel yang penting atau menjadikan sampel yang ada tidak merepresentasikan populasi secara umum dapat diimplementasikan menggunakan classs randomundersampler dari library imblearn under_sampling', 'offsets_in_document': [{'start': 118, 'end': 162}], 'offsets_in_context': [{'start': 118, 'end': 162}], 'document_ids': ['64928af8204ef541c6b128127a408854-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja jenis metode undersampling ', 'filters': None, 'id': '6218416b99d2dab7a78209546008f92a', 'no_answer': False, 'answers': ['tomek link  dan edited nearest neighbor  enn'], 'offsets_in_documents': [{'start': 118, 'end': 162}], 'offsets_in_contexts': [{'start': 118, 'end': 162}], 'document_ids': ['64928af8204ef541c6b128127a408854-0'], 'contexts': ['keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn random undersampling adalah metode non heuristic yang mengeliminasi sampel dari kelas mayoritas secara random sehingga jumlahnya sama dengan kelas minoritas  karena eliminasi sampel dilakukan secara random  maka sangat berpotensi menghilangkan sampel yang penting atau menjadikan sampel yang ada tidak merepresentasikan populasi secara umum dapat diimplementasikan menggunakan classs randomundersampler dari library imblearn under_sampling']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'fb48536c-df95-4618-8723-0379db646b60', 'query': 'class apa yang digunakan untuk mengimplementasikan random undersampling ', 'document': {'id': '64928af8204ef541c6b128127a408854-0', 'content': 'keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn random undersampling adalah metode non heuristic yang mengeliminasi sampel dari kelas mayoritas secara random sehingga jumlahnya sama dengan kelas minoritas  karena eliminasi sampel dilakukan secara random  maka sangat berpotensi menghilangkan sampel yang penting atau menjadikan sampel yang ada tidak merepresentasikan populasi secara umum dapat diimplementasikan menggunakan classs randomundersampler dari library imblearn under_sampling', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1212, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' classs randomundersampler dari library imblearn under_sampling', 'type': 'extractive', 'score': 0.0, 'context': 'keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn random undersampling adalah metode non heuristic yang mengeliminasi sampel dari kelas mayoritas secara random sehingga jumlahnya sama dengan kelas minoritas  karena eliminasi sampel dilakukan secara random  maka sangat berpotensi menghilangkan sampel yang penting atau menjadikan sampel yang ada tidak merepresentasikan populasi secara umum dapat diimplementasikan menggunakan classs randomundersampler dari library imblearn under_sampling', 'offsets_in_document': [{'start': 539, 'end': 602}], 'offsets_in_context': [{'start': 539, 'end': 602}], 'document_ids': ['64928af8204ef541c6b128127a408854-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'class apa yang digunakan untuk mengimplementasikan random undersampling ', 'filters': None, 'id': 'f4c13a4feb79500335bd21a2dde91291', 'no_answer': False, 'answers': [' classs randomundersampler dari library imblearn under_sampling'], 'offsets_in_documents': [{'start': 539, 'end': 602}], 'offsets_in_contexts': [{'start': 539, 'end': 602}], 'document_ids': ['64928af8204ef541c6b128127a408854-0'], 'contexts': ['keuntungannya dapat mempercepat proses pelatihan karena jumlah dataset berkurang jenisnya yaitu random undersampling  tomek link  dan edited nearest neighbor  enn random undersampling adalah metode non heuristic yang mengeliminasi sampel dari kelas mayoritas secara random sehingga jumlahnya sama dengan kelas minoritas  karena eliminasi sampel dilakukan secara random  maka sangat berpotensi menghilangkan sampel yang penting atau menjadikan sampel yang ada tidak merepresentasikan populasi secara umum dapat diimplementasikan menggunakan classs randomundersampler dari library imblearn under_sampling']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'f395ae97-c323-42dd-8f80-7e59f13065be', 'query': 'bagaimana statistical imputation menangani missing value?', 'document': {'id': 'd9bf089cae1bfacdbab8965a1cf3d837-0', 'content': 'statistical imputation menggunakan metode statistik untuk mengestimasi nilai pada sebuah kolom yang terdapat missing value  lalu menggantikan semua missing value dengan nilai tersebut contohnya yaitu column mean value  median value  mode value  serta constant value column mode value dapat diterapkan pada data categorical pada knn imputation  sampel baru diperhitungkan dengan menemukan sejumlah sampel dalam training set yang  paling dekat  dengannya  kemudian nilai sampel yang kosong pada kolom tertentu diisi berdasarkan nilai sample terdekat pada kolom tersebut untuk menerapkan knn imputation dengan tepat  sebaiknya knn imputation hanya dihitung pada latih saja  lalu hasil yang didapat baru diterapkan pada data uji', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1197, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'statistical imputation menggunakan metode statistik untuk mengestimasi nilai pada sebuah kolom yang terdapat missing value  lalu menggantikan semua missing value dengan nilai tersebut', 'type': 'extractive', 'score': 0.0, 'context': 'statistical imputation menggunakan metode statistik untuk mengestimasi nilai pada sebuah kolom yang terdapat missing value  lalu menggantikan semua missing value dengan nilai tersebut contohnya yaitu column mean value  median value  mode value  serta constant value column mode value dapat diterapkan pada data categorical pada knn imputation  sampel baru diperhitungkan dengan menemukan sejumlah sampel dalam training set yang  paling dekat  dengannya  kemudian nilai sampel yang kosong pada kolom tertentu diisi berdasarkan nilai sample terdekat pada kolom tersebut untuk menerapkan knn imputation dengan tepat  sebaiknya knn imputation hanya dihitung pada latih saja  lalu hasil yang didapat baru diterapkan pada data uji', 'offsets_in_document': [{'start': 0, 'end': 183}], 'offsets_in_context': [{'start': 0, 'end': 183}], 'document_ids': ['d9bf089cae1bfacdbab8965a1cf3d837-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana statistical imputation menangani missing value?', 'filters': None, 'id': '3be3a4742773f99d8ade32c9c29d880b', 'no_answer': False, 'answers': ['statistical imputation menggunakan metode statistik untuk mengestimasi nilai pada sebuah kolom yang terdapat missing value  lalu menggantikan semua missing value dengan nilai tersebut'], 'offsets_in_documents': [{'start': 0, 'end': 183}], 'offsets_in_contexts': [{'start': 0, 'end': 183}], 'document_ids': ['d9bf089cae1bfacdbab8965a1cf3d837-0'], 'contexts': ['statistical imputation menggunakan metode statistik untuk mengestimasi nilai pada sebuah kolom yang terdapat missing value  lalu menggantikan semua missing value dengan nilai tersebut contohnya yaitu column mean value  median value  mode value  serta constant value column mode value dapat diterapkan pada data categorical pada knn imputation  sampel baru diperhitungkan dengan menemukan sejumlah sampel dalam training set yang  paling dekat  dengannya  kemudian nilai sampel yang kosong pada kolom tertentu diisi berdasarkan nilai sample terdekat pada kolom tersebut untuk menerapkan knn imputation dengan tepat  sebaiknya knn imputation hanya dihitung pada latih saja  lalu hasil yang didapat baru diterapkan pada data uji']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd63a0f97-8bea-4875-bf6b-3f0dc9c58f06', 'query': 'dimana imbalance dataset dapat ditemukan?', 'document': {'id': '9d7efe9b498954a5cc5207e1040b5206-0', 'content': 'kasus imbalance dataset dapat ditemukan dalam berbagai domain seperti fraud detection  spam filtering  prediksi penyakit di bidang kesehatan  customer churn prediction  dll klasifikasi pada imbalanced dataset biasanya menghasilkan akurasi yang baik secara keseluruhan  tetapi akurasi di setiap kelasnya tidak merata  biasanya akurasi pada kelas minoritas menurun drastis imbalance ratio  ir  adalah rasio antara jumlah sampel di kelas mayoritas terhadap jumlah sampel di kelas minoritas imbalanced ratio dapat dihitung dengan membagi jumlah sampel di kelas mayoritas dengan jumlah sampel di kelas minoritas metode resampling menyeimbangkan ruang sampel dengan cara menyamakan jumlah sampel data di setiap kelas', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1202, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'kasus imbalance dataset dapat ditemukan dalam berbagai domain seperti fraud detection  spam filtering  prediksi penyakit di bidang kesehatan  customer churn prediction  dll', 'type': 'extractive', 'score': 0.0, 'context': 'kasus imbalance dataset dapat ditemukan dalam berbagai domain seperti fraud detection  spam filtering  prediksi penyakit di bidang kesehatan  customer churn prediction  dll klasifikasi pada imbalanced dataset biasanya menghasilkan akurasi yang baik secara keseluruhan  tetapi akurasi di setiap kelasnya tidak merata  biasanya akurasi pada kelas minoritas menurun drastis imbalance ratio  ir  adalah rasio antara jumlah sampel di kelas mayoritas terhadap jumlah sampel di kelas minoritas imbalanced ratio dapat dihitung dengan membagi jumlah sampel di kelas mayoritas dengan jumlah sampel di kelas minoritas metode resampling menyeimbangkan ruang sampel dengan cara menyamakan jumlah sampel data di setiap kelas', 'offsets_in_document': [{'start': 0, 'end': 172}], 'offsets_in_context': [{'start': 0, 'end': 172}], 'document_ids': ['9d7efe9b498954a5cc5207e1040b5206-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'dimana imbalance dataset dapat ditemukan?', 'filters': None, 'id': '863336745f665e97e4ff97adea73ed9c', 'no_answer': False, 'answers': ['kasus imbalance dataset dapat ditemukan dalam berbagai domain seperti fraud detection  spam filtering  prediksi penyakit di bidang kesehatan  customer churn prediction  dll'], 'offsets_in_documents': [{'start': 0, 'end': 172}], 'offsets_in_contexts': [{'start': 0, 'end': 172}], 'document_ids': ['9d7efe9b498954a5cc5207e1040b5206-0'], 'contexts': ['kasus imbalance dataset dapat ditemukan dalam berbagai domain seperti fraud detection  spam filtering  prediksi penyakit di bidang kesehatan  customer churn prediction  dll klasifikasi pada imbalanced dataset biasanya menghasilkan akurasi yang baik secara keseluruhan  tetapi akurasi di setiap kelasnya tidak merata  biasanya akurasi pada kelas minoritas menurun drastis imbalance ratio  ir  adalah rasio antara jumlah sampel di kelas mayoritas terhadap jumlah sampel di kelas minoritas imbalanced ratio dapat dihitung dengan membagi jumlah sampel di kelas mayoritas dengan jumlah sampel di kelas minoritas metode resampling menyeimbangkan ruang sampel dengan cara menyamakan jumlah sampel data di setiap kelas']}>,\n",
              " <MultiLabel: {'labels': [{'id': '64fe4d66-53c8-4f28-9190-882c7899e4fa', 'query': 'algoritma ml apa yang bisa digunakan dengan kasus dataset tidak seimbang?', 'document': {'id': 'cda566dbaa40d7dffae0bbe5ec81a799-0', 'content': 'over_sampling hybrid method adalah kombinasi dari oversampling dan undersampling untuk menyeimbangkan jumlah sampel dari setiap kelas dalam dataset biasanya algoritma smote digunakan sebagai oversampling sedangkan tomeklink atau enn sebagai undersampling memodifikasi algoritma klasifikasi agar dapat secara spesifik menangani dataset tidak seimbang tanpa mengubah distribusi dari original dataset beberapa algoritma ml juga robust saat digunakan pada kasus dataset tidak seimbang seperti decision tree  random forest  gradient boosted tree', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1207, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'beberapa algoritma ml juga robust saat digunakan pada kasus dataset tidak seimbang seperti decision tree  random forest  gradient boosted tree', 'type': 'extractive', 'score': 0.0, 'context': 'over_sampling hybrid method adalah kombinasi dari oversampling dan undersampling untuk menyeimbangkan jumlah sampel dari setiap kelas dalam dataset biasanya algoritma smote digunakan sebagai oversampling sedangkan tomeklink atau enn sebagai undersampling memodifikasi algoritma klasifikasi agar dapat secara spesifik menangani dataset tidak seimbang tanpa mengubah distribusi dari original dataset beberapa algoritma ml juga robust saat digunakan pada kasus dataset tidak seimbang seperti decision tree  random forest  gradient boosted tree', 'offsets_in_document': [{'start': 398, 'end': 540}], 'offsets_in_context': [{'start': 398, 'end': 540}], 'document_ids': ['cda566dbaa40d7dffae0bbe5ec81a799-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'algoritma ml apa yang bisa digunakan dengan kasus dataset tidak seimbang?', 'filters': None, 'id': '84d1bb1b3d50026e17a643a31a77e78b', 'no_answer': False, 'answers': ['beberapa algoritma ml juga robust saat digunakan pada kasus dataset tidak seimbang seperti decision tree  random forest  gradient boosted tree'], 'offsets_in_documents': [{'start': 398, 'end': 540}], 'offsets_in_contexts': [{'start': 398, 'end': 540}], 'document_ids': ['cda566dbaa40d7dffae0bbe5ec81a799-0'], 'contexts': ['over_sampling hybrid method adalah kombinasi dari oversampling dan undersampling untuk menyeimbangkan jumlah sampel dari setiap kelas dalam dataset biasanya algoritma smote digunakan sebagai oversampling sedangkan tomeklink atau enn sebagai undersampling memodifikasi algoritma klasifikasi agar dapat secara spesifik menangani dataset tidak seimbang tanpa mengubah distribusi dari original dataset beberapa algoritma ml juga robust saat digunakan pada kasus dataset tidak seimbang seperti decision tree  random forest  gradient boosted tree']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b0b91240-931f-47a2-9570-2721acfef703', 'query': 'apa yang dilakukan oleh ekstraksi fitur?', 'document': {'id': '76900c0d3db6fa30e5d90380afb0416-0', 'content': 'ekstraksi fitur memetakan original feature space ke new feature space dengan dimensionality yang lebih rendah   namun tetap mempertahankan informasi yang penting sebanyak mungkin fitur diekstrak dari kombinasi fitur yang asli sehingga berada pada feature space atau domain data yang baru yang berbeda fitur aslinya akibatnya   sulit untuk mengkaitkan hasil ekstraksi fitur dengan fitur aslinya untuk analisis fitur lebih lanjut dua kategori utama ekstraksi fitur adalah linear dan non linear linear feature extraction memproyeksikan mentransformasi data secara linear', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1233, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'ekstraksi fitur memetakan original feature space ke new feature space dengan dimensionality yang lebih rendah', 'type': 'extractive', 'score': 0.0, 'context': 'ekstraksi fitur memetakan original feature space ke new feature space dengan dimensionality yang lebih rendah   namun tetap mempertahankan informasi yang penting sebanyak mungkin fitur diekstrak dari kombinasi fitur yang asli sehingga berada pada feature space atau domain data yang baru yang berbeda fitur aslinya akibatnya   sulit untuk mengkaitkan hasil ekstraksi fitur dengan fitur aslinya untuk analisis fitur lebih lanjut dua kategori utama ekstraksi fitur adalah linear dan non linear linear feature extraction memproyeksikan mentransformasi data secara linear', 'offsets_in_document': [{'start': 0, 'end': 109}], 'offsets_in_context': [{'start': 0, 'end': 109}], 'document_ids': ['76900c0d3db6fa30e5d90380afb0416-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dilakukan oleh ekstraksi fitur?', 'filters': None, 'id': '7fb4fb47dded6c61ce68a745ec7785b3', 'no_answer': False, 'answers': ['ekstraksi fitur memetakan original feature space ke new feature space dengan dimensionality yang lebih rendah'], 'offsets_in_documents': [{'start': 0, 'end': 109}], 'offsets_in_contexts': [{'start': 0, 'end': 109}], 'document_ids': ['76900c0d3db6fa30e5d90380afb0416-0'], 'contexts': ['ekstraksi fitur memetakan original feature space ke new feature space dengan dimensionality yang lebih rendah   namun tetap mempertahankan informasi yang penting sebanyak mungkin fitur diekstrak dari kombinasi fitur yang asli sehingga berada pada feature space atau domain data yang baru yang berbeda fitur aslinya akibatnya   sulit untuk mengkaitkan hasil ekstraksi fitur dengan fitur aslinya untuk analisis fitur lebih lanjut dua kategori utama ekstraksi fitur adalah linear dan non linear linear feature extraction memproyeksikan mentransformasi data secara linear']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'd1cadf5b-9872-4d57-a9af-1d07a14305ef', 'query': 'apa contoh subset fitur yang dipilih filter method ', 'document': {'id': 'f7addd578e8e440296c448983a488cb5-0', 'content': 'berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method filter method memilih subset fitur yang signifikan berdasarkan karakteristik intrinsik pada data latih seperti distance  consistency  dependency  information  dan correlation filter method tidak memiliki ketergantungan  indenpenden  terhadap algoritma pembelajaran yang digunakan contoh  fisher score  information gain  relief  relieff  fast correlation based filter  fcbf   chi squared  dll', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1229, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'distance  consistency  dependency  information  dan correlation', 'type': 'extractive', 'score': 0.0, 'context': 'berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method filter method memilih subset fitur yang signifikan berdasarkan karakteristik intrinsik pada data latih seperti distance  consistency  dependency  information  dan correlation filter method tidak memiliki ketergantungan  indenpenden  terhadap algoritma pembelajaran yang digunakan contoh  fisher score  information gain  relief  relieff  fast correlation based filter  fcbf   chi squared  dll', 'offsets_in_document': [{'start': 349, 'end': 412}], 'offsets_in_context': [{'start': 349, 'end': 412}], 'document_ids': ['f7addd578e8e440296c448983a488cb5-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa contoh subset fitur yang dipilih filter method ', 'filters': None, 'id': '2c7842c9ac4bb7d0c9695496795057e3', 'no_answer': False, 'answers': ['distance  consistency  dependency  information  dan correlation'], 'offsets_in_documents': [{'start': 349, 'end': 412}], 'offsets_in_contexts': [{'start': 349, 'end': 412}], 'document_ids': ['f7addd578e8e440296c448983a488cb5-0'], 'contexts': ['berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method filter method memilih subset fitur yang signifikan berdasarkan karakteristik intrinsik pada data latih seperti distance  consistency  dependency  information  dan correlation filter method tidak memiliki ketergantungan  indenpenden  terhadap algoritma pembelajaran yang digunakan contoh  fisher score  information gain  relief  relieff  fast correlation based filter  fcbf   chi squared  dll']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c913c4e7-3e21-43de-a85e-7214c46026b5', 'query': 'apa yang ingin dicegah atau dikurangi dengan diterapkannya reduksi dimensi ', 'document': {'id': 'e0e88832fb37614098b66869bfad69a-0', 'content': 'seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya reduksi dimensi bertujuan untuk menghapus fitur yang tidak relevan atau tidak berkontribusi banyak terhadap variabel target untuk mencapai akurasi model yang lebih baik  mencegah mengurangi resiko overfitting  mempercepat waktu komputasi  mengurangi kompleksitas model sehingga lebih mudah untuk direpresentasikan  noise removal  kompresi data  efisiensi storage   retrieval berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1228, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' resiko overfitting', 'type': 'extractive', 'score': 0.0, 'context': 'seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya reduksi dimensi bertujuan untuk menghapus fitur yang tidak relevan atau tidak berkontribusi banyak terhadap variabel target untuk mencapai akurasi model yang lebih baik  mencegah mengurangi resiko overfitting  mempercepat waktu komputasi  mengurangi kompleksitas model sehingga lebih mudah untuk direpresentasikan  noise removal  kompresi data  efisiensi storage   retrieval berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method', 'offsets_in_document': [{'start': 452, 'end': 471}], 'offsets_in_context': [{'start': 452, 'end': 471}], 'document_ids': ['e0e88832fb37614098b66869bfad69a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang ingin dicegah atau dikurangi dengan diterapkannya reduksi dimensi ', 'filters': None, 'id': 'a6f43fe0707326f7493a7e557061eb41', 'no_answer': False, 'answers': [' resiko overfitting'], 'offsets_in_documents': [{'start': 452, 'end': 471}], 'offsets_in_contexts': [{'start': 452, 'end': 471}], 'document_ids': ['e0e88832fb37614098b66869bfad69a-0'], 'contexts': ['seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya reduksi dimensi bertujuan untuk menghapus fitur yang tidak relevan atau tidak berkontribusi banyak terhadap variabel target untuk mencapai akurasi model yang lebih baik  mencegah mengurangi resiko overfitting  mempercepat waktu komputasi  mengurangi kompleksitas model sehingga lebih mudah untuk direpresentasikan  noise removal  kompresi data  efisiensi storage   retrieval berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method']}>,\n",
              " <MultiLabel: {'labels': [{'id': '3ede88c9-b2c3-4d78-b5e1-284934d9cd4f', 'query': 'apa yang dicapai dengan diterapkannya reduksi dimensi ', 'document': {'id': 'e0e88832fb37614098b66869bfad69a-0', 'content': 'seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya reduksi dimensi bertujuan untuk menghapus fitur yang tidak relevan atau tidak berkontribusi banyak terhadap variabel target untuk mencapai akurasi model yang lebih baik  mencegah mengurangi resiko overfitting  mempercepat waktu komputasi  mengurangi kompleksitas model sehingga lebih mudah untuk direpresentasikan  noise removal  kompresi data  efisiensi storage   retrieval berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1228, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'reduksi dimensi bertujuan untuk menghapus fitur yang tidak relevan atau tidak berkontribusi banyak terhadap variabel target untuk mencapai akurasi model yang lebih baik  ', 'type': 'extractive', 'score': 0.0, 'context': 'seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya reduksi dimensi bertujuan untuk menghapus fitur yang tidak relevan atau tidak berkontribusi banyak terhadap variabel target untuk mencapai akurasi model yang lebih baik  mencegah mengurangi resiko overfitting  mempercepat waktu komputasi  mengurangi kompleksitas model sehingga lebih mudah untuk direpresentasikan  noise removal  kompresi data  efisiensi storage   retrieval berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method', 'offsets_in_document': [{'start': 263, 'end': 433}], 'offsets_in_context': [{'start': 263, 'end': 433}], 'document_ids': ['e0e88832fb37614098b66869bfad69a-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang dicapai dengan diterapkannya reduksi dimensi ', 'filters': None, 'id': 'b8f6e4593bbd282e1c3f8d9d4acc65c1', 'no_answer': False, 'answers': ['reduksi dimensi bertujuan untuk menghapus fitur yang tidak relevan atau tidak berkontribusi banyak terhadap variabel target untuk mencapai akurasi model yang lebih baik  '], 'offsets_in_documents': [{'start': 263, 'end': 433}], 'offsets_in_contexts': [{'start': 263, 'end': 433}], 'document_ids': ['e0e88832fb37614098b66869bfad69a-0'], 'contexts': ['seleksi fitur akan memilih subset fitur yang signifikan dan nilai fitur tetap berada pada domain aslinya sedangkan ekstraksi fitur akan mengekstrak fitur ke dalam format tertentu dan nilai hasil ekstraksi fitur berada pada domain yang berbeda dengan data aslinya reduksi dimensi bertujuan untuk menghapus fitur yang tidak relevan atau tidak berkontribusi banyak terhadap variabel target untuk mencapai akurasi model yang lebih baik  mencegah mengurangi resiko overfitting  mempercepat waktu komputasi  mengurangi kompleksitas model sehingga lebih mudah untuk direpresentasikan  noise removal  kompresi data  efisiensi storage   retrieval berdasarkan ada tidaknya label pada data  metode seleksi fitur dibagi menjadi supervised  unsupervised  dan semi supervised feature selection supervised feature selection dibagi menjadi  filter method  wrapper method  dan embedded method']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e99498b1-5ced-4dd3-a97b-1eed3d197486', 'query': 'apa saja contoh dari wrapper method pada supervised feature selection ', 'document': {'id': 'ecf2b56b4542fbb5697e090148cf3f1e-0', 'content': 'biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi contoh  forward selection  backward elimination  bi directional elimination  stepwise selection   randomized wrapper method  genetic algorithm  particle swarm optimization  artificial bee colony embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1232, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'forward selection  backward elimination  bi directional elimination  stepwise selection   randomized wrapper method  genetic algorithm  particle swarm optimization  artificial bee colony ', 'type': 'extractive', 'score': 0.0, 'context': 'biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi contoh  forward selection  backward elimination  bi directional elimination  stepwise selection   randomized wrapper method  genetic algorithm  particle swarm optimization  artificial bee colony embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method', 'offsets_in_document': [{'start': 476, 'end': 663}], 'offsets_in_context': [{'start': 476, 'end': 663}], 'document_ids': ['ecf2b56b4542fbb5697e090148cf3f1e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa saja contoh dari wrapper method pada supervised feature selection ', 'filters': None, 'id': 'f157188a4db60a11e4ed928264792b8b', 'no_answer': False, 'answers': ['forward selection  backward elimination  bi directional elimination  stepwise selection   randomized wrapper method  genetic algorithm  particle swarm optimization  artificial bee colony '], 'offsets_in_documents': [{'start': 476, 'end': 663}], 'offsets_in_contexts': [{'start': 476, 'end': 663}], 'document_ids': ['ecf2b56b4542fbb5697e090148cf3f1e-0'], 'contexts': ['biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi contoh  forward selection  backward elimination  bi directional elimination  stepwise selection   randomized wrapper method  genetic algorithm  particle swarm optimization  artificial bee colony embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'e58bc827-12d9-4d80-a5fa-b392b99370f5', 'query': 'apa contoh randomized wrapper method ', 'document': {'id': 'ecf2b56b4542fbb5697e090148cf3f1e-0', 'content': 'biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi contoh  forward selection  backward elimination  bi directional elimination  stepwise selection   randomized wrapper method  genetic algorithm  particle swarm optimization  artificial bee colony embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1232, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' genetic algorithm  particle swarm optimization  artificial bee colony', 'type': 'extractive', 'score': 0.0, 'context': 'biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi contoh  forward selection  backward elimination  bi directional elimination  stepwise selection   randomized wrapper method  genetic algorithm  particle swarm optimization  artificial bee colony embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method', 'offsets_in_document': [{'start': 592, 'end': 662}], 'offsets_in_context': [{'start': 592, 'end': 662}], 'document_ids': ['ecf2b56b4542fbb5697e090148cf3f1e-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa contoh randomized wrapper method ', 'filters': None, 'id': '775e76179cb8054bcc97d248adc2a5ba', 'no_answer': False, 'answers': [' genetic algorithm  particle swarm optimization  artificial bee colony'], 'offsets_in_documents': [{'start': 592, 'end': 662}], 'offsets_in_contexts': [{'start': 592, 'end': 662}], 'document_ids': ['ecf2b56b4542fbb5697e090148cf3f1e-0'], 'contexts': ['biasanya mampu mencapai predicitive accuracy pada algoritma pembelajaran dibandingkan dengan filter method langkah langkah dalam wrapper method yaitu  1  mencari subset fitur   2  menggunakan subset fitur yang dipilih pada langkah 1 secara langsung pada algoritma pembelajaran yang digunakan  lalu mengevaluasi performa algoritma pembelajaran pada subset fitur tersebut   3  mengulangi langkah 1 dan 2 hingga kondisi berhenti atau kriteria evaluasi tertentu terpenuhi contoh  forward selection  backward elimination  bi directional elimination  stepwise selection   randomized wrapper method  genetic algorithm  particle swarm optimization  artificial bee colony embedded method menggabungkan kelebihan filter dan wrapper method dengan cara meng embed proses seleksi fitur dengan pembangunan model classifier namun  embedded method tidak mencoba semua kemungkinan subset fitur seperti pada wrapper method  sehingga waktu komputasi yang diperlukan lebih reasonable dan risiko overfitting juga lebih kecil dibanding dengan wrapper method']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'c2a69e5f-2185-4691-a151-58686ae90a19', 'query': 'bagaimana cara menerapkan algoritma pca ', 'document': {'id': 'e19dc4f5f28fb88bd9ae93def956b052-0', 'content': 'pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1236, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data', 'type': 'extractive', 'score': 0.0, 'context': 'pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data', 'offsets_in_document': [{'start': 677, 'end': 1035}], 'offsets_in_context': [{'start': 677, 'end': 1035}], 'document_ids': ['e19dc4f5f28fb88bd9ae93def956b052-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'bagaimana cara menerapkan algoritma pca ', 'filters': None, 'id': '9029e2dc5d76362dbd72147f468b236a', 'no_answer': False, 'answers': ['algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data'], 'offsets_in_documents': [{'start': 677, 'end': 1035}], 'offsets_in_contexts': [{'start': 677, 'end': 1035}], 'document_ids': ['e19dc4f5f28fb88bd9ae93def956b052-0'], 'contexts': ['pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'fdc87808-c976-4d1c-8fd1-3c2912618b4d', 'query': 'apa langkah terakhir pada algoritma pca ', 'document': {'id': 'e19dc4f5f28fb88bd9ae93def956b052-0', 'content': 'pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1236, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': 'menurunkan dataset baru hasil transformasi  dan getting data back', 'type': 'extractive', 'score': 0.0, 'context': 'pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data', 'offsets_in_document': [{'start': 911, 'end': 976}], 'offsets_in_context': [{'start': 911, 'end': 976}], 'document_ids': ['e19dc4f5f28fb88bd9ae93def956b052-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa langkah terakhir pada algoritma pca ', 'filters': None, 'id': 'e69e82f010cb57536f00260a4a2d7667', 'no_answer': False, 'answers': ['menurunkan dataset baru hasil transformasi  dan getting data back'], 'offsets_in_documents': [{'start': 911, 'end': 976}], 'offsets_in_contexts': [{'start': 911, 'end': 976}], 'document_ids': ['e19dc4f5f28fb88bd9ae93def956b052-0'], 'contexts': ['pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data']}>,\n",
              " <MultiLabel: {'labels': [{'id': 'b681856d-ba1a-48b5-ad26-36fde8cee85c', 'query': 'apa yang harus dilakukan untuk mengawali algoritma pca ', 'document': {'id': 'e19dc4f5f28fb88bd9ae93def956b052-0', 'content': 'pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data', 'content_type': 'text', 'meta': {'name': None, 'document_id': 1236, '_split_id': 0, '_split_offset': 0}, 'id_hash_keys': ['content'], 'score': None, 'embedding': None}, 'is_correct_answer': True, 'is_correct_document': True, 'origin': 'gold-label', 'answer': {'answer': ' standardisasi  ', 'type': 'extractive', 'score': 0.0, 'context': 'pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data', 'offsets_in_document': [{'start': 711, 'end': 727}], 'offsets_in_context': [{'start': 711, 'end': 727}], 'document_ids': ['e19dc4f5f28fb88bd9ae93def956b052-0'], 'meta': {}}, 'pipeline_id': None, 'created_at': '2024-05-10 08:45:39', 'updated_at': '2024-05-10 08:45:39', 'meta': {}, 'filters': None}], 'query': 'apa yang harus dilakukan untuk mengawali algoritma pca ', 'filters': None, 'id': '76a09159b2a61ec3c3cdd57564331faf', 'no_answer': False, 'answers': [' standardisasi  '], 'offsets_in_documents': [{'start': 711, 'end': 727}], 'offsets_in_contexts': [{'start': 711, 'end': 727}], 'document_ids': ['e19dc4f5f28fb88bd9ae93def956b052-0'], 'contexts': ['pca melakukan transformasi linear dengan memilih sistem koordinat baru untuk dataset sedemikian hingga varians terbesar oleh proyeksi tertentu dari dataset terletak pada sumbu pertama  kemudian disebut sebagai principal component pertama   varians terbesar kedua pada sumbu kedua  dan seterusnya principal component didapat dengan menemukan pasangan eigenvalues and eigenvectors dari covariance matrix dataset eigenvector merupakan arah sumbu yang memiliki varians paling banyak  sebagian besar informasi   disebut sebagai principal component eigenvalue adalah koefisien yang melekat pada eigenvector yang memberikan jumlah varians yang dibawa dalam setiap principal component algoritma pca terdiri dari step 1  standardisasi  step 2  menghitung covariance matrix  step 3  menghitung eigenvectors dan eigenvalues dari covariance matrix  step 4  memilih principal component untuk membentuk vektor fitur  step 5  menurunkan dataset baru hasil transformasi  dan getting data back  mengembalikan dataset hasil transformasi ke original data']}>]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "eval_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkAQp_b9aRBk"
      },
      "outputs": [],
      "source": [
        "eval_result = pipeline.eval(labels=eval_labels, params= {\"SparseRetriever\": {\"top_k\": 1}, \"DenseRetriever\": {\"top_k\": 1}, \"JoinDocuments\": {\"top_k_join\": 2, \"debug\":True}, \"ReRanker\":{\"top_k\":1, \"debug\":True}})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.save(\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wdjx6sqi5pyi",
        "outputId": "8a9ae8d3-e3b3-42f5-9097-7a845b37d544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.schema:Saving evaluation results to /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhiVoJnqPVIh",
        "outputId": "0a6c5cc0-cdf6-42d6-ff82-ccbacdf831f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.9919028340080972,\n",
              "  'recall_single_hit': 0.9919028340080972,\n",
              "  'precision': 0.07912645490044871,\n",
              "  'map': 0.8488586813866329,\n",
              "  'mrr': 0.8603120486776996,\n",
              "  'ndcg': 0.8893290573112195},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.9757085020242915,\n",
              "  'recall_single_hit': 0.9757085020242915,\n",
              "  'precision': 0.07469635627530366,\n",
              "  'map': 0.7052182329438191,\n",
              "  'mrr': 0.7426662518767781,\n",
              "  'ndcg': 0.7830985207424542},\n",
              " 'JoinDocuments': {'recall_multi_hit': 0.9919028340080972,\n",
              "  'recall_single_hit': 0.9919028340080972,\n",
              "  'precision': 0.04834569015375151,\n",
              "  'map': 0.8483235238641795,\n",
              "  'mrr': 0.8603120486776996,\n",
              "  'ndcg': 0.8890518506610406},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9919028340080972,\n",
              "  'recall_single_hit': 0.9919028340080972,\n",
              "  'precision': 0.07672064777327936,\n",
              "  'map': 0.8355740638129301,\n",
              "  'mrr': 0.8890768588137009,\n",
              "  'ndcg': 0.8891097103046356}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HARM2-5wHbcc",
        "outputId": "3f10dffb-751c-46d3-9dcb-cd83971bf930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.9990291262135922,\n",
              "  'recall_single_hit': 0.9990291262135922,\n",
              "  'precision': 0.3211650485436893,\n",
              "  'map': 0.9684358144552319,\n",
              "  'mrr': 0.9867313915857605,\n",
              "  'ndcg': 0.9813553761972242},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.945631067961165,\n",
              "  'recall_single_hit': 0.945631067961165,\n",
              "  'precision': 0.2928155339805825,\n",
              "  'map': 0.8011394282632146,\n",
              "  'mrr': 0.8248058252427184,\n",
              "  'ndcg': 0.845350006940612},\n",
              " 'JoinDocuments': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.23331676683618432,\n",
              "  'map': 0.9588376868546771,\n",
              "  'mrr': 0.9868700878409616,\n",
              "  'ndcg': 0.9770869570259663},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9922330097087378,\n",
              "  'recall_single_hit': 0.9922330097087378,\n",
              "  'precision': 0.3170873786407768,\n",
              "  'map': 0.8532524271844659,\n",
              "  'mrr': 0.8855339805825243,\n",
              "  'ndcg': 0.8982049069757914}}"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5,5,10,5 #12\n",
        "eval_result.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGiEfDr_A3mT",
        "outputId": "295a2830-55d6-4703-8ef5-298da67cd519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.9990291262135922,\n",
              "  'recall_single_hit': 0.9990291262135922,\n",
              "  'precision': 0.3213592233009709,\n",
              "  'map': 0.9679611650485437,\n",
              "  'mrr': 0.9872168284789644,\n",
              "  'ndcg': 0.9812514421510329},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.9271844660194175,\n",
              "  'recall_single_hit': 0.9271844660194175,\n",
              "  'precision': 0.28388349514563105,\n",
              "  'map': 0.7733346817691478,\n",
              "  'mrr': 0.7933495145631068,\n",
              "  'ndcg': 0.8190896348452924},\n",
              " 'JoinDocuments': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.23051317614424413,\n",
              "  'map': 0.9591023655416859,\n",
              "  'mrr': 0.9873555247341655,\n",
              "  'ndcg': 0.9773709777478317},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9941747572815534,\n",
              "  'recall_single_hit': 0.9941747572815534,\n",
              "  'precision': 0.31708737864077674,\n",
              "  'map': 0.8521601941747572,\n",
              "  'mrr': 0.8842233009708738,\n",
              "  'ndcg': 0.8977232487350842}}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ_A1aHGidSA"
      },
      "outputs": [],
      "source": [
        "eval_result2 = pipeline.eval(labels=eval_labels, params= {\"SparseRetriever\": {\"top_k\": 10}, \"DenseRetriever\": {\"top_k\": 10}, \"JoinDocuments\": {\"top_k_join\": 20, \"debug\":True}, \"ReRanker\":{\"top_k\":10, \"debug\":True}})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result2.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "x4QsdVHiPqbn",
        "outputId": "8fe07e6c-0dc9-40b6-a4f4-0c5f9261d168"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'eval_result2' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a3219573c7b2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_result2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'eval_result2' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiYBqq9GXCaC",
        "outputId": "6e1c37b8-6ed7-4005-8ecb-7d89c9311bf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.16759439050701191,\n",
              "  'map': 0.9557553442088811,\n",
              "  'mrr': 0.9868932038834951,\n",
              "  'ndcg': 0.9755700538324369},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.9786407766990292,\n",
              "  'recall_single_hit': 0.9786407766990292,\n",
              "  'precision': 0.16368932038834955,\n",
              "  'map': 0.7848736515641854,\n",
              "  'mrr': 0.8293492834026815,\n",
              "  'ndcg': 0.8466910096866652},\n",
              " 'JoinDocuments': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.1150465578094527,\n",
              "  'map': 0.9449883651600481,\n",
              "  'mrr': 0.9868932038834951,\n",
              "  'ndcg': 0.9706836820770883},\n",
              " 'ReRanker': {'recall_multi_hit': 0.996116504854369,\n",
              "  'recall_single_hit': 0.996116504854369,\n",
              "  'precision': 0.16708737864077672,\n",
              "  'map': 0.8156115466772829,\n",
              "  'mrr': 0.8677311604253353,\n",
              "  'ndcg': 0.8760220638191221}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#10\n",
        "eval_result2.calculate_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUyA_C2EkmiT"
      },
      "outputs": [],
      "source": [
        "eval_result3 = pipeline.eval(labels=eval_labels, params= {\"SparseRetriever\": {\"top_k\": 15}, \"DenseRetriever\": {\"top_k\": 15}, \"JoinDocuments\": {\"top_k_join\": 30, \"debug\":True}, \"ReRanker\":{\"top_k\":15, \"debug\":True}})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result3.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gojkd-duIbqW",
        "outputId": "1d16ed50-f8ab-42fe-c669-9f52eb3376d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.11420095546309139,\n",
              "  'map': 0.9489653191943415,\n",
              "  'mrr': 0.9868932038834951,\n",
              "  'ndcg': 0.9723577403020937},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.983495145631068,\n",
              "  'recall_single_hit': 0.983495145631068,\n",
              "  'precision': 0.1129449838187702,\n",
              "  'map': 0.7788662817636465,\n",
              "  'mrr': 0.8297523431261294,\n",
              "  'ndcg': 0.8451577182285669},\n",
              " 'JoinDocuments': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.07630602736557054,\n",
              "  'map': 0.9384872281859054,\n",
              "  'mrr': 0.9868932038834951,\n",
              "  'ndcg': 0.9675258337833305},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9980582524271845,\n",
              "  'recall_single_hit': 0.9980582524271845,\n",
              "  'precision': 0.11346278317152103,\n",
              "  'map': 0.7999539496756903,\n",
              "  'mrr': 0.860011183530601,\n",
              "  'ndcg': 0.866357557598478}}"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r8GFq50k378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "615cd3fc-fbbe-4fc7-e67e-9cf719335dff"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.11420095546309139,\n",
              "  'map': 0.9489653191943415,\n",
              "  'mrr': 0.9868932038834951,\n",
              "  'ndcg': 0.9723577403020937},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.983495145631068,\n",
              "  'recall_single_hit': 0.983495145631068,\n",
              "  'precision': 0.1129449838187702,\n",
              "  'map': 0.7788662817636465,\n",
              "  'mrr': 0.8297523431261294,\n",
              "  'ndcg': 0.8451577182285669},\n",
              " 'JoinDocuments': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.11411003236245955,\n",
              "  'map': 0.9489653191943415,\n",
              "  'mrr': 0.9868932038834951,\n",
              "  'ndcg': 0.9723577403020937},\n",
              " 'ReRanker': {'recall_multi_hit': 1.0,\n",
              "  'recall_single_hit': 1.0,\n",
              "  'precision': 0.11411003236245955,\n",
              "  'map': 0.8011921354274567,\n",
              "  'mrr': 0.8619980424106639,\n",
              "  'ndcg': 0.8680664590395107}}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "#15\n",
        "eval_result3.calculate_metrics()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result4 = pipeline.eval(labels=eval_labels, params= {\"top_k\":1})"
      ],
      "metadata": {
        "id": "2_W0lC_fSkCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result4 = pipeline.eval(labels=eval_labels, params= {\"SparseRetriever\": {\"top_k\": 1}, \"DenseRetriever\": {\"top_k\": 1}, \"JoinDocuments\": {\"top_k_join\": 2, \"debug\":True}, \"ReRanker\":{\"top_k\":1, \"debug\":True}})"
      ],
      "metadata": {
        "id": "NfofB24YjJy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result4.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7tDRX9WBCcf",
        "outputId": "cd58837e-15b4-41ca-f30c-e3e4b5fff8c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.974757281553398,\n",
              "  'recall_single_hit': 0.974757281553398,\n",
              "  'precision': 0.974757281553398,\n",
              "  'map': 0.974757281553398,\n",
              "  'mrr': 0.974757281553398,\n",
              "  'ndcg': 0.974757281553398},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.7378640776699029,\n",
              "  'recall_single_hit': 0.7378640776699029,\n",
              "  'precision': 0.7378640776699029,\n",
              "  'map': 0.7378640776699029,\n",
              "  'mrr': 0.7378640776699029,\n",
              "  'ndcg': 0.7378640776699029},\n",
              " 'JoinDocuments': {'recall_multi_hit': 0.9854368932038835,\n",
              "  'recall_single_hit': 0.9854368932038835,\n",
              "  'precision': 0.8563106796116505,\n",
              "  'map': 0.9800970873786408,\n",
              "  'mrr': 0.9800970873786408,\n",
              "  'ndcg': 0.9814953663002777},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9436893203883495,\n",
              "  'recall_single_hit': 0.9436893203883495,\n",
              "  'precision': 0.9436893203883495,\n",
              "  'map': 0.9436893203883495,\n",
              "  'mrr': 0.9436893203883495,\n",
              "  'ndcg': 0.9436893203883495}}"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result4.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1nbcDHS5BGs",
        "outputId": "db5c75e1-9838-4c7b-dac2-adf44ef9a7b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.9757281553398058,\n",
              "  'recall_single_hit': 0.9757281553398058,\n",
              "  'precision': 0.9757281553398058,\n",
              "  'map': 0.9757281553398058,\n",
              "  'mrr': 0.9757281553398058,\n",
              "  'ndcg': 0.9757281553398058},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.7378640776699029,\n",
              "  'recall_single_hit': 0.7378640776699029,\n",
              "  'precision': 0.7378640776699029,\n",
              "  'map': 0.7378640776699029,\n",
              "  'mrr': 0.7378640776699029,\n",
              "  'ndcg': 0.7378640776699029},\n",
              " 'JoinDocuments': {'recall_multi_hit': 0.987378640776699,\n",
              "  'recall_single_hit': 0.987378640776699,\n",
              "  'precision': 0.8567961165048543,\n",
              "  'map': 0.9815533980582525,\n",
              "  'mrr': 0.9815533980582525,\n",
              "  'ndcg': 0.9830787932454927},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9446601941747573,\n",
              "  'recall_single_hit': 0.9446601941747573,\n",
              "  'precision': 0.9446601941747573,\n",
              "  'map': 0.9446601941747573,\n",
              "  'mrr': 0.9446601941747573,\n",
              "  'ndcg': 0.9446601941747573}}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result4.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Zz3i8yp4MBy",
        "outputId": "4a6502fc-855d-4c0a-bc38-3b19457cc0e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.9757281553398058,\n",
              "  'recall_single_hit': 0.9757281553398058,\n",
              "  'precision': 0.9757281553398058,\n",
              "  'map': 0.9757281553398058,\n",
              "  'mrr': 0.9757281553398058,\n",
              "  'ndcg': 0.9757281553398058},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.7378640776699029,\n",
              "  'recall_single_hit': 0.7378640776699029,\n",
              "  'precision': 0.7378640776699029,\n",
              "  'map': 0.7378640776699029,\n",
              "  'mrr': 0.7378640776699029,\n",
              "  'ndcg': 0.7378640776699029},\n",
              " 'JoinDocuments': {'recall_multi_hit': 0.9757281553398058,\n",
              "  'recall_single_hit': 0.9757281553398058,\n",
              "  'precision': 0.9757281553398058,\n",
              "  'map': 0.9757281553398058,\n",
              "  'mrr': 0.9757281553398058,\n",
              "  'ndcg': 0.9757281553398058},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9757281553398058,\n",
              "  'recall_single_hit': 0.9757281553398058,\n",
              "  'precision': 0.9757281553398058,\n",
              "  'map': 0.9757281553398058,\n",
              "  'mrr': 0.9757281553398058,\n",
              "  'ndcg': 0.9757281553398058}}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#datalatih 6\n",
        "eval_result4.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKmGET1zjsWU",
        "outputId": "1d1e2c93-4a82-4aaf-a0a1-a5e9d6f8ffc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.974757281553398,\n",
              "  'recall_single_hit': 0.974757281553398,\n",
              "  'precision': 0.974757281553398,\n",
              "  'map': 0.974757281553398,\n",
              "  'mrr': 0.974757281553398,\n",
              "  'ndcg': 0.974757281553398},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.7378640776699029,\n",
              "  'recall_single_hit': 0.7378640776699029,\n",
              "  'precision': 0.7378640776699029,\n",
              "  'map': 0.7378640776699029,\n",
              "  'mrr': 0.7378640776699029,\n",
              "  'ndcg': 0.7378640776699029},\n",
              " 'JoinDocuments': {'recall_multi_hit': 0.9854368932038835,\n",
              "  'recall_single_hit': 0.9854368932038835,\n",
              "  'precision': 0.8563106796116505,\n",
              "  'map': 0.9800970873786408,\n",
              "  'mrr': 0.9800970873786408,\n",
              "  'ndcg': 0.9814953663002777},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9378640776699029,\n",
              "  'recall_single_hit': 0.9378640776699029,\n",
              "  'precision': 0.9378640776699029,\n",
              "  'map': 0.9378640776699029,\n",
              "  'mrr': 0.9378640776699029,\n",
              "  'ndcg': 0.9378640776699029}}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#datauji 6\n",
        "eval_result4.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFgZFTM7WOfv",
        "outputId": "78efa7cc-4852-409b-9c61-34cf3a7a6b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.8385214007782101,\n",
              "  'recall_single_hit': 0.8404669260700389,\n",
              "  'precision': 0.8404669260700389,\n",
              "  'map': 0.8385214007782101,\n",
              "  'mrr': 0.8404669260700389,\n",
              "  'ndcg': 0.8389616622286594},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.6478599221789884,\n",
              "  'recall_single_hit': 0.6498054474708171,\n",
              "  'precision': 0.6498054474708171,\n",
              "  'map': 0.6478599221789884,\n",
              "  'mrr': 0.6498054474708171,\n",
              "  'ndcg': 0.6483001836294376},\n",
              " 'JoinDocuments': {'recall_multi_hit': 0.9280155642023347,\n",
              "  'recall_single_hit': 0.9299610894941635,\n",
              "  'precision': 0.745136186770428,\n",
              "  'map': 0.8832684824902723,\n",
              "  'mrr': 0.8852140077821011,\n",
              "  'ndcg': 0.8954261927039261},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9124513618677043,\n",
              "  'recall_single_hit': 0.914396887159533,\n",
              "  'precision': 0.914396887159533,\n",
              "  'map': 0.9124513618677043,\n",
              "  'mrr': 0.914396887159533,\n",
              "  'ndcg': 0.9128916233181535}}"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#datauji 12\n",
        "eval_result4.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfG7EmgMsvhR",
        "outputId": "9704ba50-8b64-446f-ebd7-ddd8885481f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.8385214007782101,\n",
              "  'recall_single_hit': 0.8404669260700389,\n",
              "  'precision': 0.8404669260700389,\n",
              "  'map': 0.8385214007782101,\n",
              "  'mrr': 0.8404669260700389,\n",
              "  'ndcg': 0.8389616622286594},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.6478599221789884,\n",
              "  'recall_single_hit': 0.6498054474708171,\n",
              "  'precision': 0.6498054474708171,\n",
              "  'map': 0.6478599221789884,\n",
              "  'mrr': 0.6498054474708171,\n",
              "  'ndcg': 0.6483001836294376},\n",
              " 'JoinDocuments': {'recall_multi_hit': 0.9280155642023347,\n",
              "  'recall_single_hit': 0.9299610894941635,\n",
              "  'precision': 0.745136186770428,\n",
              "  'map': 0.8832684824902723,\n",
              "  'mrr': 0.8852140077821011,\n",
              "  'ndcg': 0.8954261927039261},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9163424124513618,\n",
              "  'recall_single_hit': 0.9182879377431906,\n",
              "  'precision': 0.9182879377431906,\n",
              "  'map': 0.9163424124513618,\n",
              "  'mrr': 0.9182879377431906,\n",
              "  'ndcg': 0.9167826739018111}}"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result4.save(\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gNOZXRLtW7W",
        "outputId": "e6260f36-5dbf-482b-d55b-59cc865576e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.schema:Saving evaluation results to /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result4.calculate_metrics()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atmu16RhFLgv",
        "outputId": "af70dd40-49b8-499d-e5c8-2270d3334ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'SparseRetriever': {'recall_multi_hit': 0.9757281553398058,\n",
              "  'recall_single_hit': 0.9757281553398058,\n",
              "  'precision': 0.9757281553398058,\n",
              "  'map': 0.9757281553398058,\n",
              "  'mrr': 0.9757281553398058,\n",
              "  'ndcg': 0.9757281553398058},\n",
              " 'DenseRetriever': {'recall_multi_hit': 0.7378640776699029,\n",
              "  'recall_single_hit': 0.7378640776699029,\n",
              "  'precision': 0.7378640776699029,\n",
              "  'map': 0.7378640776699029,\n",
              "  'mrr': 0.7378640776699029,\n",
              "  'ndcg': 0.7378640776699029},\n",
              " 'JoinDocuments': {'recall_multi_hit': 0.987378640776699,\n",
              "  'recall_single_hit': 0.987378640776699,\n",
              "  'precision': 0.8567961165048543,\n",
              "  'map': 0.9815533980582525,\n",
              "  'mrr': 0.9815533980582525,\n",
              "  'ndcg': 0.9830787932454927},\n",
              " 'ReRanker': {'recall_multi_hit': 0.9446601941747573,\n",
              "  'recall_single_hit': 0.9446601941747573,\n",
              "  'precision': 0.9446601941747573,\n",
              "  'map': 0.9446601941747573,\n",
              "  'mrr': 0.9446601941747573,\n",
              "  'ndcg': 0.9446601941747573}}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_result4.save(\"/content\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2IvF-86M1Qw",
        "outputId": "a6afd39e-b14b-4217-c3f3-529597b36bd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:haystack.schema:Saving evaluation results to /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7OcUORgtTF5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "ul9mFz4BtHyK",
        "outputId": "e7ed1008-bbe6-4499-f030-141f13048435"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     answer_id  document_id  question_id  \\\n",
              "0         1275         1079         2923   \n",
              "1         1276         1081         2918   \n",
              "2         1407         1234         3065   \n",
              "3         1217          995         2843   \n",
              "4         1218          996         2960   \n",
              "..         ...          ...          ...   \n",
              "254       1517         1225         3172   \n",
              "255       1518         1227         3173   \n",
              "256       1520         1027         3174   \n",
              "257       1521         1230         3175   \n",
              "258       1522         1233         3176   \n",
              "\n",
              "                                                  text  answer_start  \\\n",
              "0    dua titik dianggap sebagai tetangga jika jarak...           414   \n",
              "1     separasi dinyatakan dalam ukuran between clus...           321   \n",
              "2    algoritma relief menghitung bobot  quality est...           258   \n",
              "3    tujuan utama data science adalah menganalisis ...           124   \n",
              "4    business understanding adalah tahap pertama da...             1   \n",
              "..                                                 ...           ...   \n",
              "254  algoritma relief mampu menangani fitur kategor...           117   \n",
              "255  nonlinear feature extraction memetakan data ke...             0   \n",
              "256  contoh dari performance matrics  accuracy  err...           564   \n",
              "257  linear feature extraction memproyeksikan mentr...            64   \n",
              "258  ekstraksi fitur memetakan original feature spa...             0   \n",
              "\n",
              "     answer_end  answer_category  \\\n",
              "0           510              NaN   \n",
              "1           389              NaN   \n",
              "2           332              NaN   \n",
              "3           226              NaN   \n",
              "4            71              NaN   \n",
              "..          ...              ...   \n",
              "254         250              NaN   \n",
              "255         309              NaN   \n",
              "256         743              NaN   \n",
              "257         139              NaN   \n",
              "258         109              NaN   \n",
              "\n",
              "                                              question  file_name  \\\n",
              "0    bagaimana sebuah titik ditentukan sebagai titi...        NaN   \n",
              "1    bagaimana cara mengukur separasi pada clustering         NaN   \n",
              "2    apa salah satu yang dihitung pada algoritma re...        NaN   \n",
              "3                       apa tujuan utama data science         NaN   \n",
              "4        apa tahap pertama dalam proyek analitik data         NaN   \n",
              "..                                                 ...        ...   \n",
              "254  apa yang tidak dapat ditangani oleh algoritma ...        NaN   \n",
              "255      apa maksud dari nonlinear feature extraction?        NaN   \n",
              "256        apa saja yang termasuk performance matrics?        NaN   \n",
              "257       bagaimana linear feature extraction bekerja?        NaN   \n",
              "258           apa yang dilakukan oleh ekstraksi fitur?        NaN   \n",
              "\n",
              "                                               context  \n",
              "0    se dengan baik ide utama dbscan adalah bahwa s...  \n",
              "1    engan within cluster sum of square  wcss  atau...  \n",
              "2    algoritma fisher score mengevaluasi fitur seca...  \n",
              "3    data science atau sains data adalah kombinasi ...  \n",
              "4     business understanding adalah tahap pertama d...  \n",
              "..                                                 ...  \n",
              "254  sebaliknya  jika nilai fitur a pada sampel r i...  \n",
              "255  nonlinear feature extraction memetakan data ke...  \n",
              "256  taset terdiri dari baris dan kolom yang berkai...  \n",
              "257  dua kategori utama ekstraksi fitur adalah line...  \n",
              "258  ekstraksi fitur memetakan original feature spa...  \n",
              "\n",
              "[259 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f4ef6a7-160a-4e6b-87b8-7f2048084539\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answer_id</th>\n",
              "      <th>document_id</th>\n",
              "      <th>question_id</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_category</th>\n",
              "      <th>question</th>\n",
              "      <th>file_name</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1275</td>\n",
              "      <td>1079</td>\n",
              "      <td>2923</td>\n",
              "      <td>dua titik dianggap sebagai tetangga jika jarak...</td>\n",
              "      <td>414</td>\n",
              "      <td>510</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana sebuah titik ditentukan sebagai titi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>se dengan baik ide utama dbscan adalah bahwa s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1276</td>\n",
              "      <td>1081</td>\n",
              "      <td>2918</td>\n",
              "      <td>separasi dinyatakan dalam ukuran between clus...</td>\n",
              "      <td>321</td>\n",
              "      <td>389</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana cara mengukur separasi pada clustering</td>\n",
              "      <td>NaN</td>\n",
              "      <td>engan within cluster sum of square  wcss  atau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1407</td>\n",
              "      <td>1234</td>\n",
              "      <td>3065</td>\n",
              "      <td>algoritma relief menghitung bobot  quality est...</td>\n",
              "      <td>258</td>\n",
              "      <td>332</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa salah satu yang dihitung pada algoritma re...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>algoritma fisher score mengevaluasi fitur seca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1217</td>\n",
              "      <td>995</td>\n",
              "      <td>2843</td>\n",
              "      <td>tujuan utama data science adalah menganalisis ...</td>\n",
              "      <td>124</td>\n",
              "      <td>226</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa tujuan utama data science</td>\n",
              "      <td>NaN</td>\n",
              "      <td>data science atau sains data adalah kombinasi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1218</td>\n",
              "      <td>996</td>\n",
              "      <td>2960</td>\n",
              "      <td>business understanding adalah tahap pertama da...</td>\n",
              "      <td>1</td>\n",
              "      <td>71</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa tahap pertama dalam proyek analitik data</td>\n",
              "      <td>NaN</td>\n",
              "      <td>business understanding adalah tahap pertama d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>1517</td>\n",
              "      <td>1225</td>\n",
              "      <td>3172</td>\n",
              "      <td>algoritma relief mampu menangani fitur kategor...</td>\n",
              "      <td>117</td>\n",
              "      <td>250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa yang tidak dapat ditangani oleh algoritma ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sebaliknya  jika nilai fitur a pada sampel r i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>1518</td>\n",
              "      <td>1227</td>\n",
              "      <td>3173</td>\n",
              "      <td>nonlinear feature extraction memetakan data ke...</td>\n",
              "      <td>0</td>\n",
              "      <td>309</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa maksud dari nonlinear feature extraction?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nonlinear feature extraction memetakan data ke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>1520</td>\n",
              "      <td>1027</td>\n",
              "      <td>3174</td>\n",
              "      <td>contoh dari performance matrics  accuracy  err...</td>\n",
              "      <td>564</td>\n",
              "      <td>743</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa saja yang termasuk performance matrics?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>taset terdiri dari baris dan kolom yang berkai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>1521</td>\n",
              "      <td>1230</td>\n",
              "      <td>3175</td>\n",
              "      <td>linear feature extraction memproyeksikan mentr...</td>\n",
              "      <td>64</td>\n",
              "      <td>139</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana linear feature extraction bekerja?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dua kategori utama ekstraksi fitur adalah line...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>1522</td>\n",
              "      <td>1233</td>\n",
              "      <td>3176</td>\n",
              "      <td>ekstraksi fitur memetakan original feature spa...</td>\n",
              "      <td>0</td>\n",
              "      <td>109</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa yang dilakukan oleh ekstraksi fitur?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ekstraksi fitur memetakan original feature spa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>259 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f4ef6a7-160a-4e6b-87b8-7f2048084539')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1f4ef6a7-160a-4e6b-87b8-7f2048084539 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1f4ef6a7-160a-4e6b-87b8-7f2048084539');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6a1a87a6-16e7-4544-a6ec-77b7243b2a24\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6a1a87a6-16e7-4544-a6ec-77b7243b2a24')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6a1a87a6-16e7-4544-a6ec-77b7243b2a24 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 259,\n  \"fields\": [\n    {\n      \"column\": \"answer_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82,\n        \"min\": 1217,\n        \"max\": 1522,\n        \"num_unique_values\": 259,\n        \"samples\": [\n          1289,\n          1264,\n          1223\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 72,\n        \"min\": 994,\n        \"max\": 1236,\n        \"num_unique_values\": 196,\n        \"samples\": [\n          1020,\n          1222,\n          1035\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 90,\n        \"min\": 2843,\n        \"max\": 3176,\n        \"num_unique_values\": 259,\n        \"samples\": [\n          2926,\n          2884,\n          2845\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 257,\n        \"samples\": [\n          \"one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik \",\n          \"praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang\",\n          \"shared computing cluster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 237,\n        \"min\": 0,\n        \"max\": 1030,\n        \"num_unique_values\": 192,\n        \"samples\": [\n          113,\n          376,\n          329\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 250,\n        \"min\": 17,\n        \"max\": 1318,\n        \"num_unique_values\": 230,\n        \"samples\": [\n          501,\n          343,\n          424\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_category\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 259,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 253,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "df = pd.read_csv('gt_data_uji.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYNfOpMQtYHS",
        "outputId": "a05a5bac-96a5-452d-bb8e-e53f393f5ff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      bagaimana sebuah titik ditentukan sebagai titi...\n",
              "1      bagaimana cara mengukur separasi pada clustering \n",
              "2      apa salah satu yang dihitung pada algoritma re...\n",
              "3                         apa tujuan utama data science \n",
              "4          apa tahap pertama dalam proyek analitik data \n",
              "                             ...                        \n",
              "254    apa yang tidak dapat ditangani oleh algoritma ...\n",
              "255        apa maksud dari nonlinear feature extraction?\n",
              "256          apa saja yang termasuk performance matrics?\n",
              "257         bagaimana linear feature extraction bekerja?\n",
              "258             apa yang dilakukan oleh ekstraksi fitur?\n",
              "Name: question, Length: 259, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ],
      "source": [
        "df2 = df['question']\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL3q5oor-a2H",
        "outputId": "8caaec67-532e-48ff-cbb7-680f4f71f2fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bagaimana sebuah titik ditentukan sebagai titik yang bertetangga dalam dbscan clustering \n",
            "bagaimana cara mengukur separasi pada clustering \n",
            "apa salah satu yang dihitung pada algoritma relief \n",
            "apa tujuan utama data science \n",
            "apa tahap pertama dalam proyek analitik data \n",
            "mengapa proyek data sains juga disebut sebagai proyek bisnis \n",
            "selain harus selalu berorientasi pada pencapaian hasil yang fokus pada bisnis  apakah hal penting yang menunjukkan bahwa proyek data sains adalah proyek bisnis \n",
            "siapa yang akan menggunakan informasi yang dihasilkan dari penggunaan data \n",
            "dimanakan produk diuji ketika test phase \n",
            "mengapa penting menguji model dalam sains data \n",
            "apa yang dilakukan dalam langkah evaluasi dalam praktik ilmuwan data \n",
            "terdiri dari apa saja tree pada decision tree \n",
            "apa yang dimaksud dengan decision tree \n",
            "apa yang disebut probabilitas awal dari h pada teorema bayes \n",
            "apa yang melambangkan data training atau data pembelajaran pada konteks machine learning  supervised learning  \n",
            "apa yang melambangkan kelas atau label data pada konteks machine learning  supervised learning  \n",
            "apa saja jenis pendekatan yang digunakan pada bayes learning \n",
            "apa yang disebut bukti pengamatan d pada teorema bayes \n",
            "pendekatan apa yang dilakukan teorema bayes ketika kita memiliki beberapa alternatif hipotesis \n",
            "apa yang dimaksud dengan random forest \n",
            "apa yang digunakan algoritma berbasis  tree  dari beberapa decision tree untuk membuat keputusan \n",
            "mengapa stratified sampling digunakan dalam stratified k folds cross validation \n",
            "algortima apa yang menggunakan tree sebagai model classifiernya \n",
            "k folds cross validation  stratified k folds cross validation  dan eave one out cross validation merupakan \n",
            "mengapa jst lapis tunggal cocok untuk pengenalan pola \n",
            "apa itu fungsi aktivasi \n",
            "apa saja yang diperlukan pada proses training \n",
            "apa yang dapat dilakukan untuk melatih jaringan syaraf tiruan \n",
            "bagaimana cara kerja fungsi langkah biner \n",
            "apa saja 7 fungsi aktivasi non linear \n",
            "apa salah satu kekurangan fungsi tanh \n",
            "fungsi apakah yang memiliki kekurangan yang sama dengan fungsi sigmoid \n",
            "mengapa fungsi aktivasi linier mengubah jaringan saraf menjadi hanya satu lapisan \n",
            "apa saja kelemahan support vector regression \n",
            "berapa jumlah layer tersembunyi pada jaringan syaraf tiruan lapis banyak \n",
            "apa sebutan lain dari backpropagation \n",
            "bagaimana contoh operasi pooling layer \n",
            "apa yang dilakukan pertama kali dalam proses pembelajaran pada cnn \n",
            "kapan proses pembelajaran cnn akan berakhir \n",
            "apa yang dimaksud dengan atribut binary \n",
            "apa yang dimaksud dengan atribut \n",
            "apa yang dimaksud dengan atribut ordinal \n",
            "atribut terbagi menjadi tipe apa saja \n",
            "apa saja yang termasuk metrik evaluasi regresi \n",
            "apa yang dimaksud dengan regresi linear \n",
            "apa saja dua jenis regresi linear \n",
            "apa itu prediksi akhir \n",
            "apa saja kelebihan support vector regression \n",
            "algoritma optimasi mengestimasi parameter koefisien  b  dan konstanta  bo  pada regresi linier variabel jamak dengan menggunakan \n",
            "bagaimana evaluasi clustering dengan menggunakan clustering \n",
            "apa yang dimaksud sebagai clustering \n",
            "apa yang dimaksud dengan metode bottom up dalam hierarchical clustering \n",
            "apa yang dimaksud dengan metode top down dalam hierarchical clustering \n",
            "apa yang dimaksud dengan dbscan \n",
            "bagaimana cara mengevaluasi cluster dengan kohesi dan separasi \n",
            "apa ciri dari hasil clustering yang baik \n",
            "apa yang menentukan sebuah titik menjadi titik boundary \n",
            "mengapa suatu titik dianggap sebagai core points \n",
            "apa yang dimaksud sebagai parameter minpts pada dbscan clustering \n",
            "mengapa suatu titik dianggap sebagai outlier \n",
            "apa yang dimaksud dengan evaluasi clustering dengan label \n",
            "mengapa cluster yang baik bersifat lebih kohesif \n",
            "bagaimana cara mengukur kohesi pada hasil clustering \n",
            "bagaimana peran metodologi crisp dm dalam proyek data mining \n",
            "apa saja yang dijelaskan dalam beberapa fase dari sdlc \n",
            "mengapa planning phase merupakan fase yang penting dalam sdlc \n",
            "apa input dari analysis phase \n",
            "apa output dari deployment phase \n",
            "berapa model yang terdapat pada sdlc\n",
            "apa yang harus diikuti oleh pengembang dalam build phase \n",
            "apa input dari test phase \n",
            "apa tujuan menggunakan alat pemograman \n",
            "dimanakah togaf banyak digunakan \n",
            "apa sebutan lain dengan togaf \n",
            "bagaimana cara kerja model v \n",
            "apakah prediksi termasuk dalam deskripsi tipe masalah \n",
            "apa tahap yang dilakukan setelah analisis peryaratan pada analysis phase dalam sdlc \n",
            "apa hasil yang harus disesuaikan dengan ddd \n",
            "bagaimana metode agile dalam sdlc membedakan diri dari model pengembangan perangkat lunak lainnya \n",
            "bagaimana proses tradisional dolakukan dalam dasar sains data \n",
            "apa itu fungsi range dalam python \n",
            "apa yang dihasilkan dan tidak dihasilkan dari fungsi range \n",
            "apa kepanjangan dari scc \n",
            "apa fungsi files output outflobj.writelines l  \n",
            "apa fungsi size dalam membuat data frame pada panda \n",
            "apa fungsi shape dalam membuat data frame pada panda \n",
            "apa maksud dari method std   pada pandas \n",
            "apa pustaka yang menyediakan alat untuk manipulasi data seperti menyortir dan agregasi \n",
            "apa pustaka yang dibangun di atas numpy  scipy  dan matplotlib \n",
            "apa operator boolean yang digunakan untuk menggambarkan sesuatu yang lebih kecil sama dengan \n",
            "apa fungsi axes dalam pustaka panda \n",
            "apa nama method yang digunakan untuk menghapus kolom yang hanya berisi single value \n",
            "apa fungsi dari penggunaan method df.nunique   \n",
            "apakah mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya merupakan hal yang penting dalam business understanding \n",
            "apakah masalah dan solusi termasuk dalam tahapan desain alur sistem \n",
            "apa saja yang termasuk dalam komponen kasus kendaraan otonom \n",
            "berapa jumlah komponen yang termasuk dalam kasus kendaraan otonom \n",
            "apakah masalah yang paling umum yang disebabkan oleh kendaraan otonom \n",
            "apakah hasil dari sensor merupakan output dalam tahapan desain alur sistem \n",
            "bagaimana cara mengidentifikasi tujuan bisnis dalam business understanding \n",
            "setelah mengidentifikasi tujuan bisnis maka selanjutnya tugas apa yang dilakukan dalam business understanding \n",
            "apakah yang dituliskan pada langkah langkah tersebut \n",
            "setelah menuliskan semua langkah  kemudian tugas apa yang harus dilakukan \n",
            "mengapa dalam proses data preparataion dapat berbeda berbeda atau bervariasi \n",
            "apakah pengertian data collection \n",
            "apa yang perlu diperhatikan dalam sumber data \n",
            "mengapa metode sampling probabilitas digunakan ketika menginginkan sampel yang akurat \n",
            "mengapa metode sampling probabilitas digunakan ketika populasinya beragam \n",
            "mengapa data penduduk di suatu negara menggunakan penyampelan kluster \n",
            "apa metode yang digunakan jika setiap elemen populasi tidak mempunyai peluang yang sama untuk dipilih \n",
            "data sekuensial termasuk jenis data apa \n",
            "data gambar dan suara termasuk jenis data apa \n",
            "mengapa data understanding memerlukan statistik dasar \n",
            "data dikelompokkan menjadi apa saja \n",
            "median dikenal juga dengan sebutan apa \n",
            "sebutan lain q2 adalah apa \n",
            "apa saja metode sampling yang dapat digunakan \n",
            "bagaimana cara melakukan sampling dalam data understanding \n",
            "bagaimana data understanding diterapkan \n",
            "apa tahap selanjutnya setelah data understanding \n",
            "mengidentifikasi subset dari data dilakukan pada dokumentasi apa \n",
            "apa yang dimaksud dengan data objek \n",
            "apa saja istilah penting dalam data understanding \n",
            "bagaimana peran penyampelan dalam data understanding \n",
            "populasi pada data understanding mengacu pada apa \n",
            "bagaimana cara mendapatkan rata rata \n",
            "apa yang dimaksud dengan populasi \n",
            "apa nama aspek yang berkaitan dengan kesiapan data dalam data preparation \n",
            "apa hal yang harus dipertimbangkan dalam data usability \n",
            "apakah tujuan memvisualisasi data bagi user \n",
            "apa saja tujuan dari visualisasi data \n",
            "barchart digunakan untuk apa \n",
            "bagaimana tujuan visualisasi dalam konteks distribusi \n",
            "apa informasi yang dapat diberikan oleh scatter plot \n",
            "apa bentuk visualisasi lainnya selain pie chart dan bar chart \n",
            "apa yang dimaksud heatmap \n",
            "apa yang dapat digunakan untuk dapat membuat histogram dengan mudah \n",
            "apa teknik yang digunakan untuk visualisasi\n",
            "bagaimana scatter plot digunakan untuk menggambarkan hubungan antara dua variabel \n",
            "bagaimana data direpresentasikan dalam scatter plot \n",
            "untuk menampilkan simpangan  deviasi  data visualisasi data apa yang dipilih \n",
            "untuk menunjukan perubahan nilai pada satu variabel. visualisasi data apa yang dipilih \n",
            "pada scatter plot dengan contoh perbandingan apel dengan pear dengan data berat g  dan diameter cm  apa informasi yang dapat diperoleh \n",
            "untuk menonjolkan satu variabel  visualisasi data apa yang dipilih \n",
            "hal apa yang melatarbelakangi reduksi dimensi \n",
            "apa tujuan dari data preparation \n",
            "bagaimana cara mengidentifikasi dan melakukan perbaikan error dalam dataset \n",
            "apa fungsi method df.drop   pada library pandas \n",
            "apa yang dimaksud dengan data preparation \n",
            "apa tujuan dari proses reduksi dimensi \n",
            "apa penyebab dari outlier \n",
            "apa nama algoritma yang biasa digunakan pada iterative imputation \n",
            "mengapa algoritma regresi yang berbeda bisa digunakan dalam iterative imputation \n",
            "apa yang dimaksud dengan iterative imputation \n",
            "apa contoh regularization method pada embedded method \n",
            "apa saja yang termasuk dalam perbaikan structural errors \n",
            "apa itu teknik data scaling normalization \n",
            "apa sebutan lain untuk discretization \n",
            "apa kelemahan dari metode undersampling \n",
            "bagaimana cara kerja random oversampling \n",
            "bagaimana cara random oversampling menyamakan sampel kelas minoritas dengan sampel mayoritas \n",
            "data biasanya terdiri atas apa \n",
            "apa saja jenis metode undersampling \n",
            "class apa yang digunakan untuk mengimplementasikan random undersampling \n",
            "apa kelebihan dari metode oversampling \n",
            "apa yang dimaksud dengan high dimensionality data \n",
            "kapan waktu yang tepat untuk menggunakan standardisasi \n",
            "mengapa one hote encoding digunakan \n",
            "apa class yang digunakan untuk mengimplementasikan one hot encoding \n",
            "bagaimana cara kerja quantile discretization transform \n",
            "bagaimana cara mengimplementasikan adasyn \n",
            "bagaimana cara menerapkan filter method \n",
            "apa yang ingin dicegah atau dikurangi dengan diterapkannya reduksi dimensi \n",
            "apa yang dicapai dengan diterapkannya reduksi dimensi \n",
            "apa contoh subset fitur yang dipilih filter method \n",
            "bagaimana suatu fitur dapat dikatakan sebagai fitur yang berkualitas \n",
            "berasal dari kelas mana instance fitur yang memiliki nilai berbeda supaya fitur tersebut bisa dikatakan fitur yang berkualitas \n",
            "apa saja contoh dari wrapper method pada supervised feature selection \n",
            "apa contoh randomized wrapper method \n",
            "mengapa algoritma fisher score tidak mampu menangani redundant features \n",
            "bagaimana cara kerja algoritma relief \n",
            "bagaimana cara menerapkan algoritma pca \n",
            "apa yang harus dilakukan untuk mengawali algoritma pca \n",
            "apa langkah terakhir pada algoritma pca \n",
            "Aspek apa saja yang termasuk ke dalam bidang data science?\n",
            "Kenapa data science penting dalam era digital saat ini?\n",
            "Apa saja langkah-langkah penggunaan data?\n",
            "Kenapa kemampuan pemrograman harus dikuasai oleh seorang saintis data?\n",
            "Apa saja praktik yang dilakukan oleh ilmuwan data?\n",
            "Bagaimana informasi dapat diimplementasikan?\n",
            "Apa saja latar belakang yang harus dimiliki oleh seorang saintis data?\n",
            "Untuk apa intuisi diperlukan dalam sains data?\n",
            "Apa saja komponen utama dalam sains data?\n",
            "Apa alat yang digunakan untuk analisis data?\n",
            "Bagaimana aplikasi sains data dalam bidang pemasaran?\n",
            "Bagaimana aplikasi sains data dalam bidang perbankan?\n",
            "bagaimana aplikasi sains data dalam berbagai indiustri?\n",
            "bagaimana cara untuk analisis tren data?\n",
            "apa contoh kekurangan dari penggunaan sains data?\n",
            "apa saja industi yang menggunakan sains data?\n",
            "apa kegunaan fungsi f pada supervised learning?\n",
            "apa yang dimaksud artificial intelligence?\n",
            "bagaimana proses komputasi pada algoritma knn?\n",
            "apa saja jenis supervised learning?\n",
            "kapan kondisi berhenti dicapai pada algoritma decision tree?\n",
            "bagaimana algoritma decision tree bekerja?\n",
            "kapan karnel linear digunakan pada svm?\n",
            "apa itu confusion matrix?\n",
            "bagaimana cara menerapkan knn imputation dengan tepat?\n",
            "fungsi apa yang mencegah masalah dari fungsi relu?\n",
            "siapa yang menemukan fungsi swish?\n",
            "apa itu layer konvolusi?\n",
            "apa yang diharapkan setelah proses pelatihan jst?\n",
            "apa keuntungunan fungsi sigmoid?\n",
            "kenapa model regresi liner lebih cocok digunakan untuk mendapatkan akurasi prediksi yang tinggi?\n",
            "apa itu regresi non linear?\n",
            "kenapa model non linear lebih rumit?\n",
            "bagaimana algoritma dtr bekerja?\n",
            "apa yang dimaksud dengan root mean squared error?\n",
            "bagaimana cara memuat data ke pandas?\n",
            "tipe metode sampling non probabilitas penyampelan kemudahan digunakan untuk apa?\n",
            "bagaimana cara mencari modus?\n",
            "apa saja karakteristik kualitas data yang harus diperhatikan?\n",
            "data matrix dan document data termasuk data bebentuk apa?\n",
            "apa yang harus dilakukan agar audiens dapat memahami informasi yang ingin disampaikan?\n",
            "apa yang dilakukan pada model spiral?\n",
            "siapa yang menulis paper bagus tentang data science atau sains data ?\n",
            "kapan intuisi diperlukan?\n",
            "apa saja kegunaan dari metode group by?\n",
            "kenapa wrapper method lebih beresiko dibandingkan dengan filte method?\n",
            "apa saja jenis sampling probabilitas?\n",
            "kapan stacked bar chart digunakan?\n",
            "apa perbedaan bar chart dengan diagram lingkaran?\n",
            "bagaimana jika nilai unik sebuah kolom hanya 1?\n",
            "apa yang digambarkan pada box plot?\n",
            "apa manfaat dari pemilihan visualisasi yang sesuai?\n",
            "apa akibat dari adanya outlier?\n",
            "apakah missing value dapat meningkat?\n",
            "bagaimana mengidentifikasi outlier pada data yang tidak memiliki distribusi normal?\n",
            "apa saja metode untuk mengidentifikasi outlier?\n",
            "apa itu missing value imputation?\n",
            "kapan missing value terjadi?\n",
            "bagaimana statistical imputation menangani missing value?\n",
            "algoritma apa yang hanya menggunakan nilai numerik?\n",
            "bagaimana cara mengubah data categorical ke bentuk numerik?\n",
            "dimana imbalance dataset dapat ditemukan?\n",
            "bagaimana resampling bekerja?\n",
            "apa itu smote?\n",
            "algoritma ml apa yang bisa digunakan dengan kasus dataset tidak seimbang?\n",
            "apa kegunaan image augmentation?\n",
            "kenapa diperlukan transformasi data numeric ke categorical?\n",
            "apa itu tomek link?\n",
            "bagaimana cara kerja estimated nearest neighbor?\n",
            "apa yang dihasilkan oleh adasyn?\n",
            "apa yang tidak dapat ditangani oleh algoritma relief?\n",
            "apa maksud dari nonlinear feature extraction?\n",
            "apa saja yang termasuk performance matrics?\n",
            "bagaimana linear feature extraction bekerja?\n",
            "apa yang dilakukan oleh ekstraksi fitur?\n"
          ]
        }
      ],
      "source": [
        "for text in df2:\n",
        "    query_text = text\n",
        "    print(query_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RT3PEYI5Fj5"
      },
      "outputs": [],
      "source": [
        "def runquestions(df):\n",
        "  results = []\n",
        "  for text in df2:\n",
        "        # Get the question from the DataFrame\n",
        "        query_text = text\n",
        "\n",
        "        question_results =[]\n",
        "\n",
        "        # Define the parameters for the pipeline\n",
        "        params = {\n",
        "            \"SparseRetriever\": {\"top_k\": 5},\n",
        "            \"DenseRetriever\": {\"top_k\": 5},\n",
        "            \"JoinDocuments\": {\"top_k_join\": 10, \"debug\":True},\n",
        "            \"ReRanker\": {\"top_k\": 1},\n",
        "        }\n",
        "\n",
        "        # Run the prediction with the specified query and parameters\n",
        "        prediction = pipeline.run(query=query_text, params=params)\n",
        "\n",
        "        for doc in prediction[\"documents\"]:\n",
        "          id = doc.id\n",
        "          score = doc.score\n",
        "          content = doc.content\n",
        "\n",
        "        # Append the result to the list\n",
        "        question_results.append({'question': query_text, 'id': id, 'score' : score, 'content' : content})\n",
        "\n",
        "        results.extend(question_results)\n",
        "\n",
        "  # Convert the results list to a DataFrame\n",
        "  results_df = pd.DataFrame(results)\n",
        "\n",
        "  return results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxFH3SSW8D0J"
      },
      "outputs": [],
      "source": [
        "resultdf = runquestions(df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dFFiafU8Kw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "ac10441e-b19a-4bf3-bd1a-26fa7112cb83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              question  \\\n",
              "0    bagaimana sebuah titik ditentukan sebagai titi...   \n",
              "1    bagaimana cara mengukur separasi pada clustering    \n",
              "2    apa salah satu yang dihitung pada algoritma re...   \n",
              "3                       apa tujuan utama data science    \n",
              "4        apa tahap pertama dalam proyek analitik data    \n",
              "..                                                 ...   \n",
              "254  apa yang tidak dapat ditangani oleh algoritma ...   \n",
              "255      apa maksud dari nonlinear feature extraction?   \n",
              "256        apa saja yang termasuk performance matrics?   \n",
              "257       bagaimana linear feature extraction bekerja?   \n",
              "258           apa yang dilakukan oleh ekstraksi fitur?   \n",
              "\n",
              "                                     id     score  \\\n",
              "0    8d37e96e15a66bc72585441cdb2f6f2e-0  0.997214   \n",
              "1    5d05a1f9c28b85f344efcbf954b53dd4-0  0.997691   \n",
              "2    cd0b8414ac45a94a5921b8f1a72dbf3e-0  0.991723   \n",
              "3    9e10a7173d9194f36bcd731256aefdf3-0  0.998549   \n",
              "4    7b13cd5e078eaf6a4ae6e39eaafe5672-0  0.999717   \n",
              "..                                  ...       ...   \n",
              "254  6e0e9298f0af1cb84aa4213636823ba9-0  0.996199   \n",
              "255  fe1054ff949aa343ae97c94501805627-0  0.996683   \n",
              "256  69068109e51f1d3e2c678865632736d2-0  0.481371   \n",
              "257  6d47ca3f0244ff579c330db883f33154-0  0.473563   \n",
              "258   e0e88832fb37614098b66869bfad69a-0  0.993484   \n",
              "\n",
              "                                               content  \n",
              "0    sebuah titik adalah outlier jika titik tersebu...  \n",
              "1    kohesi dapat diukur dengan within cluster sum ...  \n",
              "2    sebaliknya  jika nilai fitur a pada sampel r i...  \n",
              "3    data science atau sains data adalah kombinasi ...  \n",
              "4     business understanding adalah tahap pertama d...  \n",
              "..                                                 ...  \n",
              "254  algoritma relieff lebih robost sehingga mampu ...  \n",
              "255  dua kategori utama ekstraksi fitur adalah line...  \n",
              "256  teknik ini tidak menjamin distribusi sample da...  \n",
              "257  nonlinear feature extraction memetakan data ke...  \n",
              "258  seleksi fitur akan memilih subset fitur yang s...  \n",
              "\n",
              "[259 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9ba7c3b7-49bf-415f-b12b-54e048f65290\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>id</th>\n",
              "      <th>score</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bagaimana sebuah titik ditentukan sebagai titi...</td>\n",
              "      <td>8d37e96e15a66bc72585441cdb2f6f2e-0</td>\n",
              "      <td>0.997214</td>\n",
              "      <td>sebuah titik adalah outlier jika titik tersebu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bagaimana cara mengukur separasi pada clustering</td>\n",
              "      <td>5d05a1f9c28b85f344efcbf954b53dd4-0</td>\n",
              "      <td>0.997691</td>\n",
              "      <td>kohesi dapat diukur dengan within cluster sum ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>apa salah satu yang dihitung pada algoritma re...</td>\n",
              "      <td>cd0b8414ac45a94a5921b8f1a72dbf3e-0</td>\n",
              "      <td>0.991723</td>\n",
              "      <td>sebaliknya  jika nilai fitur a pada sampel r i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>apa tujuan utama data science</td>\n",
              "      <td>9e10a7173d9194f36bcd731256aefdf3-0</td>\n",
              "      <td>0.998549</td>\n",
              "      <td>data science atau sains data adalah kombinasi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>apa tahap pertama dalam proyek analitik data</td>\n",
              "      <td>7b13cd5e078eaf6a4ae6e39eaafe5672-0</td>\n",
              "      <td>0.999717</td>\n",
              "      <td>business understanding adalah tahap pertama d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>apa yang tidak dapat ditangani oleh algoritma ...</td>\n",
              "      <td>6e0e9298f0af1cb84aa4213636823ba9-0</td>\n",
              "      <td>0.996199</td>\n",
              "      <td>algoritma relieff lebih robost sehingga mampu ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>apa maksud dari nonlinear feature extraction?</td>\n",
              "      <td>fe1054ff949aa343ae97c94501805627-0</td>\n",
              "      <td>0.996683</td>\n",
              "      <td>dua kategori utama ekstraksi fitur adalah line...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>apa saja yang termasuk performance matrics?</td>\n",
              "      <td>69068109e51f1d3e2c678865632736d2-0</td>\n",
              "      <td>0.481371</td>\n",
              "      <td>teknik ini tidak menjamin distribusi sample da...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>bagaimana linear feature extraction bekerja?</td>\n",
              "      <td>6d47ca3f0244ff579c330db883f33154-0</td>\n",
              "      <td>0.473563</td>\n",
              "      <td>nonlinear feature extraction memetakan data ke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>apa yang dilakukan oleh ekstraksi fitur?</td>\n",
              "      <td>e0e88832fb37614098b66869bfad69a-0</td>\n",
              "      <td>0.993484</td>\n",
              "      <td>seleksi fitur akan memilih subset fitur yang s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>259 rows × 4 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9ba7c3b7-49bf-415f-b12b-54e048f65290')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9ba7c3b7-49bf-415f-b12b-54e048f65290 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9ba7c3b7-49bf-415f-b12b-54e048f65290');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6dd13d33-166e-4aac-a039-323a58b8ff38\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6dd13d33-166e-4aac-a039-323a58b8ff38')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6dd13d33-166e-4aac-a039-323a58b8ff38 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "resultdf",
              "summary": "{\n  \"name\": \"resultdf\",\n  \"rows\": 259,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 259,\n        \"samples\": [\n          \"apa input dari analysis phase \",\n          \"apa saja dua jenis regresi linear \",\n          \"mengapa penting menguji model dalam sains data \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 145,\n        \"samples\": [\n          \"a38dce38dc4af2664ff15853adb97af9-0\",\n          \"6c4b691406cbf5c0aea520bf23ed478c-0\",\n          \"b1083baf0b46be5d2a0fd488420a90d9-0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.08925670395379173,\n        \"min\": 0.11948560178279877,\n        \"max\": 0.9998002648353577,\n        \"num_unique_values\": 259,\n        \"samples\": [\n          0.9870880842208862,\n          0.9818019270896912,\n          0.9984748959541321\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"content\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 145,\n        \"samples\": [\n          \"data dikelompokkan menjadi beberapa jenis  yaitu bentuk record  bentuk graph dan network  ordered  serta spatial  image  dan multimedia data yang berbentuk record terdiri atas data matrix  document data  dan transaction data data yang berbentuk graph dan network mencakup world wide web  molecular structures  dan social networks data yang ordered terdiri atas video data  spatio temporal data  sequential data  dan genetic sequence data sedangkan data spatial  image  dan multimedia mencakup spatial data  maps   image data  voice data  dan video data\",\n          \"under_sampling cara kerja estimated nearest neighbors yaitu instance dari kelas mayoritas dihapus berdasarkan k tetangga terdekatnya dapat diimplementasikan menggunakan class editednearestneighbours dari library imblearn under_sampling oversampling akan meningkatkan jumlah sampel dari kelas minoritas\",\n          \" jaringan syaraf manusia terdiri dari dendrit  akson  sinapsis  dan soma  sedangkan untuk jaringan syaraf tiruan atau disebut juga artificial neural network merupakan sistem pemroses informasi yang memiliki karakteristik mirip dengan jaringan syarat biologi faktor penentu jst ada tiga yaitu pola hubungan antar neuron  fungsi aktivasi  dan metode untuk menentukan bobot penghubung sebuah jst memiliki sejumlah neuron yang tersusun pada sejumlah layer atau lapisan sebagai berikut  satu layer input berfungsi menerima masukan  satu layer output untuk menghasilkan output dan layer tersembunyi yaitu layer di antara layer input dan output ada dua jenis jst berdasarkan layer yakni single layer dan multi layer sedangkan jenis jst yang berdasarkan aliran sinyal juga memiliki dua jenis yakni feedfoward dan recurrent karakterisik jst lapis tunggal cocok untuk pengenalan pola karena semua unit dihubungkan dengan unit output  unit input output dalam satu lapisan  layer  tidak saling berhubungan  bobot wji    bobot hubungan unit input i dengan unit input j  bobot saling independen\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "resultdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g81pUOnG_2Y"
      },
      "outputs": [],
      "source": [
        "resultdf.to_csv('prediction.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dfgt = pd.read_csv(\"gt_data_uji.csv\")\n",
        "dfgt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "id": "T1lvNgG117AF",
        "outputId": "51d5941b-70a5-4ac7-f2d6-4a3c597e96f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     answer_id  document_id  question_id  \\\n",
              "0         1275         1079         2923   \n",
              "1         1276         1081         2918   \n",
              "2         1407         1234         3065   \n",
              "3         1217          995         2843   \n",
              "4         1218          996         2960   \n",
              "..         ...          ...          ...   \n",
              "254       1517         1225         3172   \n",
              "255       1518         1227         3173   \n",
              "256       1520         1027         3174   \n",
              "257       1521         1230         3175   \n",
              "258       1522         1233         3176   \n",
              "\n",
              "                                                  text  answer_start  \\\n",
              "0    dua titik dianggap sebagai tetangga jika jarak...           414   \n",
              "1     separasi dinyatakan dalam ukuran between clus...           321   \n",
              "2    algoritma relief menghitung bobot  quality est...           258   \n",
              "3    tujuan utama data science adalah menganalisis ...           124   \n",
              "4    business understanding adalah tahap pertama da...             1   \n",
              "..                                                 ...           ...   \n",
              "254  algoritma relief mampu menangani fitur kategor...           117   \n",
              "255  nonlinear feature extraction memetakan data ke...             0   \n",
              "256  contoh dari performance matrics  accuracy  err...           564   \n",
              "257  linear feature extraction memproyeksikan mentr...            64   \n",
              "258  ekstraksi fitur memetakan original feature spa...             0   \n",
              "\n",
              "     answer_end  answer_category  \\\n",
              "0           510              NaN   \n",
              "1           389              NaN   \n",
              "2           332              NaN   \n",
              "3           226              NaN   \n",
              "4            71              NaN   \n",
              "..          ...              ...   \n",
              "254         250              NaN   \n",
              "255         309              NaN   \n",
              "256         743              NaN   \n",
              "257         139              NaN   \n",
              "258         109              NaN   \n",
              "\n",
              "                                              question  file_name  \\\n",
              "0    bagaimana sebuah titik ditentukan sebagai titi...        NaN   \n",
              "1    bagaimana cara mengukur separasi pada clustering         NaN   \n",
              "2    apa salah satu yang dihitung pada algoritma re...        NaN   \n",
              "3                       apa tujuan utama data science         NaN   \n",
              "4        apa tahap pertama dalam proyek analitik data         NaN   \n",
              "..                                                 ...        ...   \n",
              "254  apa yang tidak dapat ditangani oleh algoritma ...        NaN   \n",
              "255      apa maksud dari nonlinear feature extraction?        NaN   \n",
              "256        apa saja yang termasuk performance matrics?        NaN   \n",
              "257       bagaimana linear feature extraction bekerja?        NaN   \n",
              "258           apa yang dilakukan oleh ekstraksi fitur?        NaN   \n",
              "\n",
              "                                               context  \n",
              "0    se dengan baik ide utama dbscan adalah bahwa s...  \n",
              "1    engan within cluster sum of square  wcss  atau...  \n",
              "2    algoritma fisher score mengevaluasi fitur seca...  \n",
              "3    data science atau sains data adalah kombinasi ...  \n",
              "4     business understanding adalah tahap pertama d...  \n",
              "..                                                 ...  \n",
              "254  sebaliknya  jika nilai fitur a pada sampel r i...  \n",
              "255  nonlinear feature extraction memetakan data ke...  \n",
              "256  taset terdiri dari baris dan kolom yang berkai...  \n",
              "257  dua kategori utama ekstraksi fitur adalah line...  \n",
              "258  ekstraksi fitur memetakan original feature spa...  \n",
              "\n",
              "[259 rows x 10 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-aef10e26-2ed6-4cd9-b0bb-dc78169adfb8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>answer_id</th>\n",
              "      <th>document_id</th>\n",
              "      <th>question_id</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_category</th>\n",
              "      <th>question</th>\n",
              "      <th>file_name</th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1275</td>\n",
              "      <td>1079</td>\n",
              "      <td>2923</td>\n",
              "      <td>dua titik dianggap sebagai tetangga jika jarak...</td>\n",
              "      <td>414</td>\n",
              "      <td>510</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana sebuah titik ditentukan sebagai titi...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>se dengan baik ide utama dbscan adalah bahwa s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1276</td>\n",
              "      <td>1081</td>\n",
              "      <td>2918</td>\n",
              "      <td>separasi dinyatakan dalam ukuran between clus...</td>\n",
              "      <td>321</td>\n",
              "      <td>389</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana cara mengukur separasi pada clustering</td>\n",
              "      <td>NaN</td>\n",
              "      <td>engan within cluster sum of square  wcss  atau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1407</td>\n",
              "      <td>1234</td>\n",
              "      <td>3065</td>\n",
              "      <td>algoritma relief menghitung bobot  quality est...</td>\n",
              "      <td>258</td>\n",
              "      <td>332</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa salah satu yang dihitung pada algoritma re...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>algoritma fisher score mengevaluasi fitur seca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1217</td>\n",
              "      <td>995</td>\n",
              "      <td>2843</td>\n",
              "      <td>tujuan utama data science adalah menganalisis ...</td>\n",
              "      <td>124</td>\n",
              "      <td>226</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa tujuan utama data science</td>\n",
              "      <td>NaN</td>\n",
              "      <td>data science atau sains data adalah kombinasi ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1218</td>\n",
              "      <td>996</td>\n",
              "      <td>2960</td>\n",
              "      <td>business understanding adalah tahap pertama da...</td>\n",
              "      <td>1</td>\n",
              "      <td>71</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa tahap pertama dalam proyek analitik data</td>\n",
              "      <td>NaN</td>\n",
              "      <td>business understanding adalah tahap pertama d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>254</th>\n",
              "      <td>1517</td>\n",
              "      <td>1225</td>\n",
              "      <td>3172</td>\n",
              "      <td>algoritma relief mampu menangani fitur kategor...</td>\n",
              "      <td>117</td>\n",
              "      <td>250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa yang tidak dapat ditangani oleh algoritma ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sebaliknya  jika nilai fitur a pada sampel r i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>255</th>\n",
              "      <td>1518</td>\n",
              "      <td>1227</td>\n",
              "      <td>3173</td>\n",
              "      <td>nonlinear feature extraction memetakan data ke...</td>\n",
              "      <td>0</td>\n",
              "      <td>309</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa maksud dari nonlinear feature extraction?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>nonlinear feature extraction memetakan data ke...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>256</th>\n",
              "      <td>1520</td>\n",
              "      <td>1027</td>\n",
              "      <td>3174</td>\n",
              "      <td>contoh dari performance matrics  accuracy  err...</td>\n",
              "      <td>564</td>\n",
              "      <td>743</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa saja yang termasuk performance matrics?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>taset terdiri dari baris dan kolom yang berkai...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>257</th>\n",
              "      <td>1521</td>\n",
              "      <td>1230</td>\n",
              "      <td>3175</td>\n",
              "      <td>linear feature extraction memproyeksikan mentr...</td>\n",
              "      <td>64</td>\n",
              "      <td>139</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana linear feature extraction bekerja?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>dua kategori utama ekstraksi fitur adalah line...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>1522</td>\n",
              "      <td>1233</td>\n",
              "      <td>3176</td>\n",
              "      <td>ekstraksi fitur memetakan original feature spa...</td>\n",
              "      <td>0</td>\n",
              "      <td>109</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa yang dilakukan oleh ekstraksi fitur?</td>\n",
              "      <td>NaN</td>\n",
              "      <td>ekstraksi fitur memetakan original feature spa...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>259 rows × 10 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-aef10e26-2ed6-4cd9-b0bb-dc78169adfb8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-aef10e26-2ed6-4cd9-b0bb-dc78169adfb8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-aef10e26-2ed6-4cd9-b0bb-dc78169adfb8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bd43339f-6385-401e-9f86-55c2d702ad64\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bd43339f-6385-401e-9f86-55c2d702ad64')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bd43339f-6385-401e-9f86-55c2d702ad64 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "dfgt",
              "summary": "{\n  \"name\": \"dfgt\",\n  \"rows\": 259,\n  \"fields\": [\n    {\n      \"column\": \"answer_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 82,\n        \"min\": 1217,\n        \"max\": 1522,\n        \"num_unique_values\": 259,\n        \"samples\": [\n          1289,\n          1264,\n          1223\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 72,\n        \"min\": 994,\n        \"max\": 1236,\n        \"num_unique_values\": 196,\n        \"samples\": [\n          1020,\n          1222,\n          1035\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 90,\n        \"min\": 2843,\n        \"max\": 3176,\n        \"num_unique_values\": 259,\n        \"samples\": [\n          2926,\n          2884,\n          2845\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 257,\n        \"samples\": [\n          \"one hot encoding digunakan untuk encoding variabel nominal ke nilai numerik \",\n          \"praktik ilmuwan data melibatkan eksplorasi dalam data  pembersihan  persiapan data  pembuatan hipotesis dan pemodelan  evaluasi  dan interpretasi yang berulang\",\n          \"shared computing cluster\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 237,\n        \"min\": 0,\n        \"max\": 1030,\n        \"num_unique_values\": 192,\n        \"samples\": [\n          113,\n          376,\n          329\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 250,\n        \"min\": 17,\n        \"max\": 1318,\n        \"num_unique_values\": 230,\n        \"samples\": [\n          501,\n          343,\n          424\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_category\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 259,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 253,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "matching_rows_count = df[resultdf['content'] == dfgt['context']].shape[0]"
      ],
      "metadata": {
        "id": "IPoWiYUa2aqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matching_rows_count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn_NhW7-5qOx",
        "outputId": "e0e1ed02-8ac6-44aa-d87a-b20369f99c81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "21"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resultdf.rename(columns={'content': 'context'}, inplace=True)"
      ],
      "metadata": {
        "id": "Hrmpy1BK3ez-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.merge(resultdf, df2, on='context')"
      ],
      "metadata": {
        "id": "0hFWeyu52jz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df = merged_df.sort_values(by='question_x')"
      ],
      "metadata": {
        "id": "7Dp1oVO-4E9B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "asY1Ree44cnc",
        "outputId": "bac2187e-39e8-4efa-bf1a-bf8e12fadf7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                           question_x  \\\n",
              "12          apa ciri dari hasil clustering yang baik    \n",
              "83                  apa definisi dari random sampling   \n",
              "82                  apa definisi dari random sampling   \n",
              "35   apa fungsi method df.drop   pada library pandas    \n",
              "36   apa fungsi method df.drop   pada library pandas    \n",
              "..                                                ...   \n",
              "70                        sebutan lain q2 adalah apa    \n",
              "56  setelah mengidentifikasi tujuan bisnis maka se...   \n",
              "2   setelah menuliskan semua langkah  kemudian tug...   \n",
              "72  untuk menampilkan simpangan  deviasi  data vis...   \n",
              "73  untuk perbandingan  komparasi  antarkategori  ...   \n",
              "\n",
              "                                    id     score  \\\n",
              "12  4f0f576566298295e7e73d5faf516249-0  0.999410   \n",
              "83  4ae36c63f7f7cfffd5516c6e7e521a79-0  0.973814   \n",
              "82  4ae36c63f7f7cfffd5516c6e7e521a79-0  0.973814   \n",
              "35  2465fb2a1cf7434f31b2b8455d62af79-0  0.997434   \n",
              "36  2465fb2a1cf7434f31b2b8455d62af79-0  0.997434   \n",
              "..                                 ...       ...   \n",
              "70  2782c706c505bd67ec2f5d05f7c9f444-0  0.984372   \n",
              "56  48f5412cf30205b9e0b6cdd8cc8c8840-0  0.994547   \n",
              "2   d6686b06e1cb7f288626baed940accb2-0  0.990586   \n",
              "72  46b7e44f8d927ed59def62f77d7c83da-0  0.998657   \n",
              "73  46b7e44f8d927ed59def62f77d7c83da-0  0.998965   \n",
              "\n",
              "                                              context  answer_id  document_id  \\\n",
              "12  cluster yang dihasilkan dinilai valid atau tid...       1270         1074   \n",
              "83  kelemahannya jumlah data semakin bertambah  da...       1385         1205   \n",
              "82  kelemahannya jumlah data semakin bertambah  da...       1384         1205   \n",
              "35  drop   dari library pandas untuk menghapus kol...       1314         1130   \n",
              "36  drop   dari library pandas untuk menghapus kol...       1315         1130   \n",
              "..                                                ...        ...          ...   \n",
              "70  simpangan baku adalah salah satu ukuran sebara...       1337         1147   \n",
              "56  tiga hal utama dalam business understanding ad...       1317         1131   \n",
              "2   kemudian berdasarkan umpan balik  produk dapat...       1290         1096   \n",
              "72   visualisasi merupakan suatu teknik dalam pemb...       1353         1162   \n",
              "73   visualisasi merupakan suatu teknik dalam pemb...       1353         1162   \n",
              "\n",
              "    question_id                                               text  \\\n",
              "12         2916   evaluasi pada clustering dapat dilakukan deng...   \n",
              "83         3049  mereplikasi sampel dari kelas minoritas secara...   \n",
              "82         3041  mereplikasi sampel dari kelas minoritas secara...   \n",
              "35         3026                                         df nunique   \n",
              "36         3030  mengidentifikasi dan hapus kolom yang hanya be...   \n",
              "..          ...                                                ...   \n",
              "70         2996                                                 q2   \n",
              "56         2970  tahapan business understanding dalam ai melipu...   \n",
              "2          2942  program   perangkat lunak yang diuji bermigras...   \n",
              "72         3013  visualisasi sendiri memiliki empat tujuan anta...   \n",
              "73         3013  visualisasi sendiri memiliki empat tujuan anta...   \n",
              "\n",
              "    answer_start  answer_end  answer_category  \\\n",
              "12           292         434              NaN   \n",
              "83           266         377              NaN   \n",
              "82           266         377              NaN   \n",
              "35           230         240              NaN   \n",
              "36           131         211              NaN   \n",
              "..           ...         ...              ...   \n",
              "70           294         297              NaN   \n",
              "56           284         397              NaN   \n",
              "2            237         313              NaN   \n",
              "72           296         908              NaN   \n",
              "73           296         908              NaN   \n",
              "\n",
              "                                           question_y  file_name  \n",
              "12  bagaimana evaluasi clustering dengan menggunak...        NaN  \n",
              "83  bagaimana cara random oversampling menyamakan ...        NaN  \n",
              "82          bagaimana cara kerja random oversampling         NaN  \n",
              "35  apa nama method yang digunakan untuk menghapus...        NaN  \n",
              "36    apa fungsi dari penggunaan method df.nunique           NaN  \n",
              "..                                                ...        ...  \n",
              "70            median dikenal juga dengan sebutan apa         NaN  \n",
              "56  apakah masalah dan solusi termasuk dalam tahap...        NaN  \n",
              "2                   apa output dari deployment phase         NaN  \n",
              "72             apa saja tujuan dari visualisasi data         NaN  \n",
              "73             apa saja tujuan dari visualisasi data         NaN  \n",
              "\n",
              "[92 rows x 13 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-333bf6c0-dcbc-4143-bb6b-a3bd8b6e92d5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question_x</th>\n",
              "      <th>id</th>\n",
              "      <th>score</th>\n",
              "      <th>context</th>\n",
              "      <th>answer_id</th>\n",
              "      <th>document_id</th>\n",
              "      <th>question_id</th>\n",
              "      <th>text</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer_end</th>\n",
              "      <th>answer_category</th>\n",
              "      <th>question_y</th>\n",
              "      <th>file_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>apa ciri dari hasil clustering yang baik</td>\n",
              "      <td>4f0f576566298295e7e73d5faf516249-0</td>\n",
              "      <td>0.999410</td>\n",
              "      <td>cluster yang dihasilkan dinilai valid atau tid...</td>\n",
              "      <td>1270</td>\n",
              "      <td>1074</td>\n",
              "      <td>2916</td>\n",
              "      <td>evaluasi pada clustering dapat dilakukan deng...</td>\n",
              "      <td>292</td>\n",
              "      <td>434</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana evaluasi clustering dengan menggunak...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>apa definisi dari random sampling</td>\n",
              "      <td>4ae36c63f7f7cfffd5516c6e7e521a79-0</td>\n",
              "      <td>0.973814</td>\n",
              "      <td>kelemahannya jumlah data semakin bertambah  da...</td>\n",
              "      <td>1385</td>\n",
              "      <td>1205</td>\n",
              "      <td>3049</td>\n",
              "      <td>mereplikasi sampel dari kelas minoritas secara...</td>\n",
              "      <td>266</td>\n",
              "      <td>377</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana cara random oversampling menyamakan ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>apa definisi dari random sampling</td>\n",
              "      <td>4ae36c63f7f7cfffd5516c6e7e521a79-0</td>\n",
              "      <td>0.973814</td>\n",
              "      <td>kelemahannya jumlah data semakin bertambah  da...</td>\n",
              "      <td>1384</td>\n",
              "      <td>1205</td>\n",
              "      <td>3041</td>\n",
              "      <td>mereplikasi sampel dari kelas minoritas secara...</td>\n",
              "      <td>266</td>\n",
              "      <td>377</td>\n",
              "      <td>NaN</td>\n",
              "      <td>bagaimana cara kerja random oversampling</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>apa fungsi method df.drop   pada library pandas</td>\n",
              "      <td>2465fb2a1cf7434f31b2b8455d62af79-0</td>\n",
              "      <td>0.997434</td>\n",
              "      <td>drop   dari library pandas untuk menghapus kol...</td>\n",
              "      <td>1314</td>\n",
              "      <td>1130</td>\n",
              "      <td>3026</td>\n",
              "      <td>df nunique</td>\n",
              "      <td>230</td>\n",
              "      <td>240</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa nama method yang digunakan untuk menghapus...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>apa fungsi method df.drop   pada library pandas</td>\n",
              "      <td>2465fb2a1cf7434f31b2b8455d62af79-0</td>\n",
              "      <td>0.997434</td>\n",
              "      <td>drop   dari library pandas untuk menghapus kol...</td>\n",
              "      <td>1315</td>\n",
              "      <td>1130</td>\n",
              "      <td>3030</td>\n",
              "      <td>mengidentifikasi dan hapus kolom yang hanya be...</td>\n",
              "      <td>131</td>\n",
              "      <td>211</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa fungsi dari penggunaan method df.nunique</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>sebutan lain q2 adalah apa</td>\n",
              "      <td>2782c706c505bd67ec2f5d05f7c9f444-0</td>\n",
              "      <td>0.984372</td>\n",
              "      <td>simpangan baku adalah salah satu ukuran sebara...</td>\n",
              "      <td>1337</td>\n",
              "      <td>1147</td>\n",
              "      <td>2996</td>\n",
              "      <td>q2</td>\n",
              "      <td>294</td>\n",
              "      <td>297</td>\n",
              "      <td>NaN</td>\n",
              "      <td>median dikenal juga dengan sebutan apa</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>setelah mengidentifikasi tujuan bisnis maka se...</td>\n",
              "      <td>48f5412cf30205b9e0b6cdd8cc8c8840-0</td>\n",
              "      <td>0.994547</td>\n",
              "      <td>tiga hal utama dalam business understanding ad...</td>\n",
              "      <td>1317</td>\n",
              "      <td>1131</td>\n",
              "      <td>2970</td>\n",
              "      <td>tahapan business understanding dalam ai melipu...</td>\n",
              "      <td>284</td>\n",
              "      <td>397</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apakah masalah dan solusi termasuk dalam tahap...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>setelah menuliskan semua langkah  kemudian tug...</td>\n",
              "      <td>d6686b06e1cb7f288626baed940accb2-0</td>\n",
              "      <td>0.990586</td>\n",
              "      <td>kemudian berdasarkan umpan balik  produk dapat...</td>\n",
              "      <td>1290</td>\n",
              "      <td>1096</td>\n",
              "      <td>2942</td>\n",
              "      <td>program   perangkat lunak yang diuji bermigras...</td>\n",
              "      <td>237</td>\n",
              "      <td>313</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa output dari deployment phase</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>untuk menampilkan simpangan  deviasi  data vis...</td>\n",
              "      <td>46b7e44f8d927ed59def62f77d7c83da-0</td>\n",
              "      <td>0.998657</td>\n",
              "      <td>visualisasi merupakan suatu teknik dalam pemb...</td>\n",
              "      <td>1353</td>\n",
              "      <td>1162</td>\n",
              "      <td>3013</td>\n",
              "      <td>visualisasi sendiri memiliki empat tujuan anta...</td>\n",
              "      <td>296</td>\n",
              "      <td>908</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa saja tujuan dari visualisasi data</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>untuk perbandingan  komparasi  antarkategori  ...</td>\n",
              "      <td>46b7e44f8d927ed59def62f77d7c83da-0</td>\n",
              "      <td>0.998965</td>\n",
              "      <td>visualisasi merupakan suatu teknik dalam pemb...</td>\n",
              "      <td>1353</td>\n",
              "      <td>1162</td>\n",
              "      <td>3013</td>\n",
              "      <td>visualisasi sendiri memiliki empat tujuan anta...</td>\n",
              "      <td>296</td>\n",
              "      <td>908</td>\n",
              "      <td>NaN</td>\n",
              "      <td>apa saja tujuan dari visualisasi data</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>92 rows × 13 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-333bf6c0-dcbc-4143-bb6b-a3bd8b6e92d5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-333bf6c0-dcbc-4143-bb6b-a3bd8b6e92d5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-333bf6c0-dcbc-4143-bb6b-a3bd8b6e92d5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a6443a4f-4bb3-4d43-9f6f-f777b52aa8ca\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a6443a4f-4bb3-4d43-9f6f-f777b52aa8ca')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a6443a4f-4bb3-4d43-9f6f-f777b52aa8ca button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sorted_df",
              "summary": "{\n  \"name\": \"sorted_df\",\n  \"rows\": 92,\n  \"fields\": [\n    {\n      \"column\": \"question_x\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 77,\n        \"samples\": [\n          \"apa input dari analysis phase \",\n          \"apa yang harus dimaksimalkan dalam hasil clustering \",\n          \"apa maksud dari method std pada pandas\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"4869ad62b741eca84728ab4f1f885641-0\",\n          \"260459b2d4e07d5a4678ad87d3c5b494-0\",\n          \"48f5412cf30205b9e0b6cdd8cc8c8840-0\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.026454045072682335,\n        \"min\": 0.8081156611442566,\n        \"max\": 0.9997932314872742,\n        \"num_unique_values\": 75,\n        \"samples\": [\n          0.8081156611442566,\n          0.9967934489250183,\n          0.9994766116142273\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 28,\n        \"samples\": [\n          \"konsep dari slack variable yaitu untuk setiap nilai yang berada di luar \\u03f5  dapat dinyatakan deviasinya dari margin sebagai \\u03be deviasi berusaha untuk diminimalkan  sehingga dapat ditambahkan pada objective function  minimize   yaitu min \\u00bd   w   2   c _ i 1  n   \\u03bei    dengan batasan  yi   wixi   \\u025b  \\u03bei kelebihan support vector regression adalah robust pada outliers  generalisasi sangat baik dan akurasi prediksi tinggi sementara kelemahannya adalah tidak cocok untuk dataset besar dan peforma buruk ketika dataset memiliki banyak noise decision tree merupakan algoritma supervised learning yang dapat digunakan baik untuk klasifikasi maupun regresi\",\n          \"setiap nilai kategori dalam variabel akan diubah menjadi nilai biner dapat diimplementasikan menggunakan class onehotencoder dari library scikit learn transformasi nilai numerik ke categorical diperlukan karena beberapa algoritma ml  seperti decision tree atau rule based algorithm memerlukan input berupa kategori atau variabel ordinal serta banyak algoritma ml yang bekerja lebih baik saat variabel input numerik yang digunakan mengikuti standard probability distribution solusinya transformasi variabel numerik menjadi nilai diskret dimana setiap nilai diberikan label yang memiliki keterurutan discretization adalah transformasi nilai variabel input output numerik menjadi nilai diskret atau label ordinal\",\n          \"tiga hal utama dalam business understanding adalah menentukan masalah yang akan diselesaikan  menentukan tujuan proyek  dan melihat solusi yang memungkinkan dari perspektif bisnis penting juga untuk mendefinisikan bagaimana kita mengukur  keberhasilan  dan bagaimana cara mengukurnya tahapan business understanding dalam ai meliputi business case  desain alur sistem  komponen  masalah  dan solusi business case   kasus kendaraan otonom dengan tujuan untuk menyediakan kendaraan tanpa pengemudi yang aman  ekonomis  dan praktis desain alur sistem  meliputi sistem sensor  machine learning  dan output  aktuator\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 55,\n        \"min\": 1261,\n        \"max\": 1513,\n        \"num_unique_values\": 34,\n        \"samples\": [\n          1295,\n          1261,\n          1406\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"document_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 44,\n        \"min\": 1049,\n        \"max\": 1234,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          1067,\n          1210,\n          1131\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 64,\n        \"min\": 2887,\n        \"max\": 3168,\n        \"num_unique_values\": 34,\n        \"samples\": [\n          2943,\n          2997,\n          3057\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 32,\n        \"samples\": [\n          \"kombinasi  deviation bar  bar chart  dan area\",\n          \"data harus diekstraksi dari sumber yang relevan kemudian  data tersebut harus diintegrasikan dan dipersiapkan agar siap digunakan setelah itu  model model dibangun berdasarkan data yang telah dipersiapkan\",\n          \"masing   masing data diplot menjadi sebuah titik yang diposisikan sesuai nilai pada sumbu xy \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 70,\n        \"min\": 0,\n        \"max\": 300,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          177,\n          202,\n          147\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_end\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 134,\n        \"min\": 96,\n        \"max\": 908,\n        \"num_unique_values\": 29,\n        \"samples\": [\n          200,\n          439,\n          96\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_category\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question_y\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 34,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"file_name\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaVBrD60t2gn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "id": "B2ej1-iZtbb8",
        "outputId": "ba9bffec-82b2-450e-e996-36774d366f5f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1033,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1032,\n        \"samples\": [\n          \"apa yang menjadi kunci dalam memanfaatkan potensi big data bagi seorang saintis data \",\n          \"apa yang dipertegas oleh david donoho tentang tukey  1962  \",\n          \"metrik evaluasi regresi yang mengukur akurasi sebagai persentase  dan dapat dihitung sebagai kesalahan persentase absolut rata rata untuk setiap periode waktu dikurangi nilai aktual dibagi dengan nilai aktual.disebut sebagai \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"positive_ctxs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"negative_ctxs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"hard_negative_ctxs\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-59aa339b-3044-4a9b-8f7d-9ad50bf4c12c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "      <th>positive_ctxs</th>\n",
              "      <th>negative_ctxs</th>\n",
              "      <th>hard_negative_ctxs</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>apa yang dimaksud dengan silhouette coefficient</td>\n",
              "      <td>[nilai kohesi dan separasi ini dapat dievaluas...</td>\n",
              "      <td>[{'title': '', 'text': 'evaluasi ini disebut j...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'kemudian berdasarkan u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apa yang dimaksud dengan kohesi</td>\n",
              "      <td>[ ukuran kedekatan data dalam suatu cluster]</td>\n",
              "      <td>[{'title': '', 'text': 'evaluasi ini disebut j...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'kohesi dapat diukur de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bagaimana kohesi didapatkan dari sebuah cluster</td>\n",
              "      <td>[didapatkan dengan menghitung rata rata jarak ...</td>\n",
              "      <td>[{'title': '', 'text': 'evaluasi ini disebut j...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'kohesi dapat diukur de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>apa yang perlu dihitung untuk mencari nilai ko...</td>\n",
              "      <td>[rata rata jarak data dengan data lain dalam c...</td>\n",
              "      <td>[{'title': '', 'text': 'evaluasi ini disebut j...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'yang keenam  mean abso...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bagaimana cara menghitung nilai separasi dan k...</td>\n",
              "      <td>[silhouette coefficient ]</td>\n",
              "      <td>[{'title': '', 'text': 'evaluasi ini disebut j...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'kohesi dapat diukur de...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1028</th>\n",
              "      <td>apa tujuan visualisasi dengan komposisi  compo...</td>\n",
              "      <td>[untuk melihat komposisi dari suatu variabel  ...</td>\n",
              "      <td>[{'title': '', 'text': 'jika ingin melihat dis...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'tujuan visualisasi ke ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029</th>\n",
              "      <td>visualisasi apa yang biasa digunakan pada komp...</td>\n",
              "      <td>[stacked bar chart]</td>\n",
              "      <td>[{'title': '', 'text': 'jika ingin melihat dis...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'tujuan visualisasi ke ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1030</th>\n",
              "      <td>untuk melihat keterhubungan antara suatu varia...</td>\n",
              "      <td>[relasi atau relationship ]</td>\n",
              "      <td>[{'title': '', 'text': 'jika ingin melihat dis...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'tujuan visualisasi ke ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1031</th>\n",
              "      <td>sebutkan 7 macam visualisasi data</td>\n",
              "      <td>[pie chart  bar chart  line graphs  scatter pl...</td>\n",
              "      <td>[{'title': '', 'text': 'visualisasi data dapat...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'visualisasi yang tepat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1032</th>\n",
              "      <td>apa saja macam   macam visualisasi</td>\n",
              "      <td>[pie chart  bar chart  line graphs  scatter pl...</td>\n",
              "      <td>[{'title': '', 'text': 'visualisasi data dapat...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[{'title': '', 'text': 'visualisasi yang tepat...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1033 rows × 5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-59aa339b-3044-4a9b-8f7d-9ad50bf4c12c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-59aa339b-3044-4a9b-8f7d-9ad50bf4c12c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-59aa339b-3044-4a9b-8f7d-9ad50bf4c12c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b45457ad-de48-4650-b086-a54c03f8e77b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b45457ad-de48-4650-b086-a54c03f8e77b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b45457ad-de48-4650-b086-a54c03f8e77b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               question  \\\n",
              "0      apa yang dimaksud dengan silhouette coefficient    \n",
              "1                      apa yang dimaksud dengan kohesi    \n",
              "2      bagaimana kohesi didapatkan dari sebuah cluster    \n",
              "3     apa yang perlu dihitung untuk mencari nilai ko...   \n",
              "4     bagaimana cara menghitung nilai separasi dan k...   \n",
              "...                                                 ...   \n",
              "1028  apa tujuan visualisasi dengan komposisi  compo...   \n",
              "1029  visualisasi apa yang biasa digunakan pada komp...   \n",
              "1030  untuk melihat keterhubungan antara suatu varia...   \n",
              "1031                  sebutkan 7 macam visualisasi data   \n",
              "1032                apa saja macam   macam visualisasi    \n",
              "\n",
              "                                                answers  \\\n",
              "0     [nilai kohesi dan separasi ini dapat dievaluas...   \n",
              "1          [ ukuran kedekatan data dalam suatu cluster]   \n",
              "2     [didapatkan dengan menghitung rata rata jarak ...   \n",
              "3     [rata rata jarak data dengan data lain dalam c...   \n",
              "4                             [silhouette coefficient ]   \n",
              "...                                                 ...   \n",
              "1028  [untuk melihat komposisi dari suatu variabel  ...   \n",
              "1029                                [stacked bar chart]   \n",
              "1030                        [relasi atau relationship ]   \n",
              "1031  [pie chart  bar chart  line graphs  scatter pl...   \n",
              "1032  [pie chart  bar chart  line graphs  scatter pl...   \n",
              "\n",
              "                                          positive_ctxs negative_ctxs  \\\n",
              "0     [{'title': '', 'text': 'evaluasi ini disebut j...            []   \n",
              "1     [{'title': '', 'text': 'evaluasi ini disebut j...            []   \n",
              "2     [{'title': '', 'text': 'evaluasi ini disebut j...            []   \n",
              "3     [{'title': '', 'text': 'evaluasi ini disebut j...            []   \n",
              "4     [{'title': '', 'text': 'evaluasi ini disebut j...            []   \n",
              "...                                                 ...           ...   \n",
              "1028  [{'title': '', 'text': 'jika ingin melihat dis...            []   \n",
              "1029  [{'title': '', 'text': 'jika ingin melihat dis...            []   \n",
              "1030  [{'title': '', 'text': 'jika ingin melihat dis...            []   \n",
              "1031  [{'title': '', 'text': 'visualisasi data dapat...            []   \n",
              "1032  [{'title': '', 'text': 'visualisasi data dapat...            []   \n",
              "\n",
              "                                     hard_negative_ctxs  \n",
              "0     [{'title': '', 'text': 'kemudian berdasarkan u...  \n",
              "1     [{'title': '', 'text': 'kohesi dapat diukur de...  \n",
              "2     [{'title': '', 'text': 'kohesi dapat diukur de...  \n",
              "3     [{'title': '', 'text': 'yang keenam  mean abso...  \n",
              "4     [{'title': '', 'text': 'kohesi dapat diukur de...  \n",
              "...                                                 ...  \n",
              "1028  [{'title': '', 'text': 'tujuan visualisasi ke ...  \n",
              "1029  [{'title': '', 'text': 'tujuan visualisasi ke ...  \n",
              "1030  [{'title': '', 'text': 'tujuan visualisasi ke ...  \n",
              "1031  [{'title': '', 'text': 'visualisasi yang tepat...  \n",
              "1032  [{'title': '', 'text': 'visualisasi yang tepat...  \n",
              "\n",
              "[1033 rows x 5 columns]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_json(\"answersDPR.json\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS-Ozr0_u2Vh",
        "outputId": "96ad76bf-54ca-44b0-ab3f-8eb792689bb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set - Features: (826, 5)\n",
            "Testing set - Features: (207, 5)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming df is your DataFrame containing the data\n",
        "# X should contain all the columns as features\n",
        "\n",
        "# Splitting the data into training and testing datasets in an 80:20 ratio\n",
        "X_train, X_test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Printing the shapes of the datasets to verify the split\n",
        "print(\"Training set - Features:\", X_train.shape)\n",
        "print(\"Testing set - Features:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aICt9kLavZXY"
      },
      "outputs": [],
      "source": [
        "X_train.to_json('train_data.json', orient='records')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8LP9ALCwNzQ"
      },
      "outputs": [],
      "source": [
        "X_test.to_json('test_data.json', orient='records')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "bda33b16be7e844498c7c2d368d72665b4f1d165582b9547ed22a0249a29ca2e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "df5a0d335c4242b7a67c544cec3154b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_68c72eada4664a6e8b352ffd65821dbe",
              "IPY_MODEL_c5e7c8fd16bb45ad9aa924716d9177ad",
              "IPY_MODEL_0bfd6d15186c4c299286b58a7c57cff2"
            ],
            "layout": "IPY_MODEL_e0162ed9c2964b72bbba4ed681dc7543"
          }
        },
        "68c72eada4664a6e8b352ffd65821dbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d37549c8a605428d915e051c67274d0a",
            "placeholder": "​",
            "style": "IPY_MODEL_7b8e4110fa424b13ac704a71f8b65718",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c5e7c8fd16bb45ad9aa924716d9177ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9feda274ee54631b42103bfe607d4ab",
            "max": 507,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd61b37f72ea44449a50f1985cf298f6",
            "value": 507
          }
        },
        "0bfd6d15186c4c299286b58a7c57cff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49dece008f7e48e7a58531f9bed46476",
            "placeholder": "​",
            "style": "IPY_MODEL_f35de7f64d3a4df0b3396930d4a08898",
            "value": " 507/507 [00:00&lt;00:00, 35.7kB/s]"
          }
        },
        "e0162ed9c2964b72bbba4ed681dc7543": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d37549c8a605428d915e051c67274d0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b8e4110fa424b13ac704a71f8b65718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9feda274ee54631b42103bfe607d4ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd61b37f72ea44449a50f1985cf298f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49dece008f7e48e7a58531f9bed46476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f35de7f64d3a4df0b3396930d4a08898": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4875f7073f9b4f42902a5df0340e0284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_713ea84c144a41c8bf6ef53819e7cf6f",
              "IPY_MODEL_f43fcbea373d4c638977ef28f2f76399",
              "IPY_MODEL_91b24a89db7043b58a1dc28f76073f47"
            ],
            "layout": "IPY_MODEL_9669807cc1304a81bf383e43cd5d4e39"
          }
        },
        "713ea84c144a41c8bf6ef53819e7cf6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83b28ad04d9047019522e3ec82b79d5e",
            "placeholder": "​",
            "style": "IPY_MODEL_1474f785299b4c67aec4cc11e9c30d0e",
            "value": "vocab.txt: 100%"
          }
        },
        "f43fcbea373d4c638977ef28f2f76399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_246bdd7e0eca48bc8e397ef25274a760",
            "max": 229513,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ef221eea1584c81a96efa481df65a5b",
            "value": 229513
          }
        },
        "91b24a89db7043b58a1dc28f76073f47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7583a520733145da9d23308699e1c778",
            "placeholder": "​",
            "style": "IPY_MODEL_51bd02a964f140049817c3dec28d6b16",
            "value": " 230k/230k [00:00&lt;00:00, 1.71MB/s]"
          }
        },
        "9669807cc1304a81bf383e43cd5d4e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b28ad04d9047019522e3ec82b79d5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1474f785299b4c67aec4cc11e9c30d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "246bdd7e0eca48bc8e397ef25274a760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef221eea1584c81a96efa481df65a5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7583a520733145da9d23308699e1c778": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51bd02a964f140049817c3dec28d6b16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02e4717a0bc44b07ac019e9005b6ad42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12af0414e4d24a85b5d6d0a4266dbf53",
              "IPY_MODEL_7ef9ad3610504796b94f122d640961b6",
              "IPY_MODEL_58d100822c0544acb7f963677587c5b7"
            ],
            "layout": "IPY_MODEL_7fc7da596d084cc2bb74b43f78b25e9c"
          }
        },
        "12af0414e4d24a85b5d6d0a4266dbf53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c08f94877dc046cfa8d81f20f576c2a8",
            "placeholder": "​",
            "style": "IPY_MODEL_f1cce9673ad941b4ada834d34b798708",
            "value": "tokenizer.json: 100%"
          }
        },
        "7ef9ad3610504796b94f122d640961b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_189887150e3f4af299571a33f1f4a656",
            "max": 733298,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba3caab06b0d49f092f1caac67ac047e",
            "value": 733298
          }
        },
        "58d100822c0544acb7f963677587c5b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d8d7cdd7ca44270bf49fa5a051a6b62",
            "placeholder": "​",
            "style": "IPY_MODEL_5a8be239cec34f8bb01f0e7651c35ee6",
            "value": " 733k/733k [00:00&lt;00:00, 4.43MB/s]"
          }
        },
        "7fc7da596d084cc2bb74b43f78b25e9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c08f94877dc046cfa8d81f20f576c2a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1cce9673ad941b4ada834d34b798708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "189887150e3f4af299571a33f1f4a656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba3caab06b0d49f092f1caac67ac047e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d8d7cdd7ca44270bf49fa5a051a6b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a8be239cec34f8bb01f0e7651c35ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05b871f2c73c4a21b60eb2024ff2e400": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_adcbe07438534304a9af1b26829c2915",
              "IPY_MODEL_58adbf112cf743e087b07f4548739b8d",
              "IPY_MODEL_08aeb98c869c457482dbc03c9b960487"
            ],
            "layout": "IPY_MODEL_e6d94854ebcf439cb96c9b8cb1022883"
          }
        },
        "adcbe07438534304a9af1b26829c2915": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0ae7e57a1814c7180209628817375f8",
            "placeholder": "​",
            "style": "IPY_MODEL_3cdfbd211caf434e9404c81b8d2b3528",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "58adbf112cf743e087b07f4548739b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4ea44331f934619a41c1dac7181761c",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1710c41452fa40249fa03981be0d3bf9",
            "value": 125
          }
        },
        "08aeb98c869c457482dbc03c9b960487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e93b7002abec4d2ab69dcbdb044ff369",
            "placeholder": "​",
            "style": "IPY_MODEL_73d9e2617a8f446490ec8f77ec1cf730",
            "value": " 125/125 [00:00&lt;00:00, 8.34kB/s]"
          }
        },
        "e6d94854ebcf439cb96c9b8cb1022883": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0ae7e57a1814c7180209628817375f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cdfbd211caf434e9404c81b8d2b3528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4ea44331f934619a41c1dac7181761c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1710c41452fa40249fa03981be0d3bf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e93b7002abec4d2ab69dcbdb044ff369": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d9e2617a8f446490ec8f77ec1cf730": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92b4bf80ba944a36ba4aa758094f39d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47d629e70cfa4bcb80b580bc7377f80d",
              "IPY_MODEL_f851a7c3c6d746ff9ab2001aadf107b9",
              "IPY_MODEL_ca3242b9aef941c0aa3b5e1b3ae9a56a"
            ],
            "layout": "IPY_MODEL_e793f5663a6a45a7a3aec3ce459aa553"
          }
        },
        "47d629e70cfa4bcb80b580bc7377f80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_344184123bb7470aa924364ad23873ea",
            "placeholder": "​",
            "style": "IPY_MODEL_6d0f6a7e92ad4924ac0a4516a2a095b5",
            "value": "config.json: 100%"
          }
        },
        "f851a7c3c6d746ff9ab2001aadf107b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_feac4775fdf84ac99b4bdb4ac0141ec5",
            "max": 679,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_66398be3a19c4c539114709f0990de19",
            "value": 679
          }
        },
        "ca3242b9aef941c0aa3b5e1b3ae9a56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd7014e9217740ce9095e321fce7529a",
            "placeholder": "​",
            "style": "IPY_MODEL_248f046c45ad456b990d2abfb84e7e01",
            "value": " 679/679 [00:00&lt;00:00, 48.7kB/s]"
          }
        },
        "e793f5663a6a45a7a3aec3ce459aa553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "344184123bb7470aa924364ad23873ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d0f6a7e92ad4924ac0a4516a2a095b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "feac4775fdf84ac99b4bdb4ac0141ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66398be3a19c4c539114709f0990de19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bd7014e9217740ce9095e321fce7529a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "248f046c45ad456b990d2abfb84e7e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94722a3f7b9343359281b8f400a0da2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20c6d7b144604daba81a2eac41595175",
              "IPY_MODEL_c4b8359c6cd64b24af57dd2efe02199c",
              "IPY_MODEL_2fad5bf94115494481b1db0351ebee27"
            ],
            "layout": "IPY_MODEL_7d416094e86d4dd0ab4484fe68516b31"
          }
        },
        "20c6d7b144604daba81a2eac41595175": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8af161714f72415fadbfe4e8790374f6",
            "placeholder": "​",
            "style": "IPY_MODEL_80b2db5a023346ec841718127d866d9b",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "c4b8359c6cd64b24af57dd2efe02199c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f816eb6a4b74436bb0a7e8bdec3d760",
            "max": 442560055,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d427a843d9649aeaeff79994cb0b6e5",
            "value": 442560055
          }
        },
        "2fad5bf94115494481b1db0351ebee27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8f1487808c4105b5eac3458e602e65",
            "placeholder": "​",
            "style": "IPY_MODEL_1514664d72b746458ecdc354f4a6a02b",
            "value": " 443M/443M [00:01&lt;00:00, 241MB/s]"
          }
        },
        "7d416094e86d4dd0ab4484fe68516b31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8af161714f72415fadbfe4e8790374f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b2db5a023346ec841718127d866d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f816eb6a4b74436bb0a7e8bdec3d760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d427a843d9649aeaeff79994cb0b6e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef8f1487808c4105b5eac3458e602e65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1514664d72b746458ecdc354f4a6a02b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0eeaf2ba26df404f8cfbed3a2065fa32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6b7c3b0ee3c543e9a9e9d22099efb6ca",
              "IPY_MODEL_536e8d7c69704388986f87d95d92f854",
              "IPY_MODEL_98675d42be2f40f5881a0dce71c370dd"
            ],
            "layout": "IPY_MODEL_85bd700c721440a6afd22086f9e41792"
          }
        },
        "6b7c3b0ee3c543e9a9e9d22099efb6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae3538b64d1e4ee1b2aebec5b0c919f8",
            "placeholder": "​",
            "style": "IPY_MODEL_c153180382414971866e529f27905643",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "536e8d7c69704388986f87d95d92f854": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_148d3bb20f704125a692b129db9a3791",
            "max": 506,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a16ed26372d449f7875e2b22ba838edc",
            "value": 506
          }
        },
        "98675d42be2f40f5881a0dce71c370dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cac3d11335a749ee9a1541bb01ae3ff8",
            "placeholder": "​",
            "style": "IPY_MODEL_a3c6c5bf202442b7b6c1c755f5f6ffcd",
            "value": " 506/506 [00:00&lt;00:00, 33.6kB/s]"
          }
        },
        "85bd700c721440a6afd22086f9e41792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae3538b64d1e4ee1b2aebec5b0c919f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c153180382414971866e529f27905643": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "148d3bb20f704125a692b129db9a3791": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a16ed26372d449f7875e2b22ba838edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cac3d11335a749ee9a1541bb01ae3ff8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3c6c5bf202442b7b6c1c755f5f6ffcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3489928094e64c13b8879397c94d3f8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e65b19a21fcc453ea4f96a9778884b8d",
              "IPY_MODEL_9d0bd28174c94b058a4c5faf5f8c31e6",
              "IPY_MODEL_9f9201dee31748e5929e6950ad1520d2"
            ],
            "layout": "IPY_MODEL_d6bfbb3740e4495f897ffb1e644a0a02"
          }
        },
        "e65b19a21fcc453ea4f96a9778884b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f6825899394831a93b60144512fcd7",
            "placeholder": "​",
            "style": "IPY_MODEL_4be276baa5bf414ba29aa82f9ca519a4",
            "value": "vocab.txt: 100%"
          }
        },
        "9d0bd28174c94b058a4c5faf5f8c31e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92838225bb5247a0b280d903967c9ce1",
            "max": 229513,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f1a8ae9f57e45e999099dddb11976a8",
            "value": 229513
          }
        },
        "9f9201dee31748e5929e6950ad1520d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_843fd818568241339614cb21514b7871",
            "placeholder": "​",
            "style": "IPY_MODEL_c7b9fdbf0d3b4c94bb890a6a60802f75",
            "value": " 230k/230k [00:00&lt;00:00, 3.46MB/s]"
          }
        },
        "d6bfbb3740e4495f897ffb1e644a0a02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f6825899394831a93b60144512fcd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be276baa5bf414ba29aa82f9ca519a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92838225bb5247a0b280d903967c9ce1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f1a8ae9f57e45e999099dddb11976a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "843fd818568241339614cb21514b7871": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7b9fdbf0d3b4c94bb890a6a60802f75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cbe7a1e572d4bcb9608809f34848a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a798ac81743e41f4ae577da38946bc30",
              "IPY_MODEL_bb7edf79b2944c9e94b4ae70e9e45722",
              "IPY_MODEL_3e24a0e4385f4382802446a8d08c427d"
            ],
            "layout": "IPY_MODEL_a6a2da912ce34e5393d6eae36e3058ab"
          }
        },
        "a798ac81743e41f4ae577da38946bc30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_935891f54bca4e01aff37efc779599e2",
            "placeholder": "​",
            "style": "IPY_MODEL_ecad8bab007d42e58fefdb8d4e2ccd24",
            "value": "tokenizer.json: 100%"
          }
        },
        "bb7edf79b2944c9e94b4ae70e9e45722": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd31981dc0414be79eb6e83e398cc2b4",
            "max": 733300,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b90cbf3c48e411cbdd13f6d49275451",
            "value": 733300
          }
        },
        "3e24a0e4385f4382802446a8d08c427d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bced16c8393467a9f47f91aba9b0a6a",
            "placeholder": "​",
            "style": "IPY_MODEL_8e48799a2e124e9987d0ed85f7b614a5",
            "value": " 733k/733k [00:00&lt;00:00, 3.73MB/s]"
          }
        },
        "a6a2da912ce34e5393d6eae36e3058ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "935891f54bca4e01aff37efc779599e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ecad8bab007d42e58fefdb8d4e2ccd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd31981dc0414be79eb6e83e398cc2b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b90cbf3c48e411cbdd13f6d49275451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2bced16c8393467a9f47f91aba9b0a6a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e48799a2e124e9987d0ed85f7b614a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69cae78cfa7f4423aaeedd100c2d2b36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14762d145a9a45069bad0587b37e3525",
              "IPY_MODEL_470fb229ca8d45848041ea7a28b6276a",
              "IPY_MODEL_addfced7b5684cd69c8b350b03ed72e5"
            ],
            "layout": "IPY_MODEL_8919638e0ab748da8c7b278542746f6c"
          }
        },
        "14762d145a9a45069bad0587b37e3525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_387900f34339448e93a8b4c5d0784553",
            "placeholder": "​",
            "style": "IPY_MODEL_366d0eedcd684107a3d38a93774f6ce4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "470fb229ca8d45848041ea7a28b6276a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_430c2b55ebce410783449190e33ca96a",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59b1221fbffd4f6cbb81b99656d05ac3",
            "value": 125
          }
        },
        "addfced7b5684cd69c8b350b03ed72e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4081912d0714582b15cd5827e1f2f9a",
            "placeholder": "​",
            "style": "IPY_MODEL_ed9c91b94a3c407bba1e2748abf24c5d",
            "value": " 125/125 [00:00&lt;00:00, 5.78kB/s]"
          }
        },
        "8919638e0ab748da8c7b278542746f6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "387900f34339448e93a8b4c5d0784553": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "366d0eedcd684107a3d38a93774f6ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "430c2b55ebce410783449190e33ca96a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b1221fbffd4f6cbb81b99656d05ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4081912d0714582b15cd5827e1f2f9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed9c91b94a3c407bba1e2748abf24c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a80f4dcf5af94e28a8736847ba71ba3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb9d01d25798498d974797c6be73c4fd",
              "IPY_MODEL_f340a18cc3fe46069d022580dc5c0190",
              "IPY_MODEL_fef39631a2ae46d9acb5fab314d1eeaf"
            ],
            "layout": "IPY_MODEL_dd2e840939bd450bb8cf970773ab2f33"
          }
        },
        "bb9d01d25798498d974797c6be73c4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_482833cfae25411b8b552c20b667f3b9",
            "placeholder": "​",
            "style": "IPY_MODEL_56257250a24541e2a9ff8fa583e41dcb",
            "value": "config.json: 100%"
          }
        },
        "f340a18cc3fe46069d022580dc5c0190": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5a6c915204e499e8e06c4f41870b8cd",
            "max": 677,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5f1e6bc5474403ab51367e6d6323605",
            "value": 677
          }
        },
        "fef39631a2ae46d9acb5fab314d1eeaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c78ec1732a7146b8a24f96e41f1d4594",
            "placeholder": "​",
            "style": "IPY_MODEL_cf9fbf80345a4e71b131c635e1d5485d",
            "value": " 677/677 [00:00&lt;00:00, 34.0kB/s]"
          }
        },
        "dd2e840939bd450bb8cf970773ab2f33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "482833cfae25411b8b552c20b667f3b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56257250a24541e2a9ff8fa583e41dcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5a6c915204e499e8e06c4f41870b8cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5f1e6bc5474403ab51367e6d6323605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c78ec1732a7146b8a24f96e41f1d4594": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf9fbf80345a4e71b131c635e1d5485d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d53cfbef2e3e4e6597f326813cd80c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc482560f89d411089d688f583d514d2",
              "IPY_MODEL_684b4a92dda34978a770b08eda3834c9",
              "IPY_MODEL_9b7a55b5a1074688a1670a5b02c44988"
            ],
            "layout": "IPY_MODEL_f1a459bf77a6491c920fa6fc8e3a9a59"
          }
        },
        "cc482560f89d411089d688f583d514d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99778f57b0504f69a19a53455804503e",
            "placeholder": "​",
            "style": "IPY_MODEL_e00d9e45d2704eee9395902ff731bbfd",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "684b4a92dda34978a770b08eda3834c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42c0ee07480841a4a05a0fbec4a0ee5b",
            "max": 442558903,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_36e7d27c035a42388c98ba5e86692512",
            "value": 442558903
          }
        },
        "9b7a55b5a1074688a1670a5b02c44988": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_091d7803d934429491c0b9b7ccdd7e3d",
            "placeholder": "​",
            "style": "IPY_MODEL_b1e3ee3d13ae4f7aa16a019a26d0a867",
            "value": " 443M/443M [00:01&lt;00:00, 245MB/s]"
          }
        },
        "f1a459bf77a6491c920fa6fc8e3a9a59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99778f57b0504f69a19a53455804503e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e00d9e45d2704eee9395902ff731bbfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42c0ee07480841a4a05a0fbec4a0ee5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36e7d27c035a42388c98ba5e86692512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "091d7803d934429491c0b9b7ccdd7e3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1e3ee3d13ae4f7aa16a019a26d0a867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b24b39c1c004a37849aff336f32785d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7febcfb713b45968d424b74f88eb7f5",
              "IPY_MODEL_ffa9adb069924de28f32137e7d2d6008",
              "IPY_MODEL_d48320b04a804e1fa2d0502d9a08acba"
            ],
            "layout": "IPY_MODEL_8535f078f951410aa2e3b7e88dfae7c7"
          }
        },
        "c7febcfb713b45968d424b74f88eb7f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b06c77aae4b344599a06cc2d4f19f327",
            "placeholder": "​",
            "style": "IPY_MODEL_27892d59dc9a40ad8f8c7394a74e310c",
            "value": "config.json: 100%"
          }
        },
        "ffa9adb069924de28f32137e7d2d6008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dfb390d92114b69bab15373638e8538",
            "max": 791,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84c831259add40f89bf23734353e71d6",
            "value": 791
          }
        },
        "d48320b04a804e1fa2d0502d9a08acba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28c596cda3d348799043036efaf99c97",
            "placeholder": "​",
            "style": "IPY_MODEL_7261250c64ad48bfa0832510857eb4fd",
            "value": " 791/791 [00:00&lt;00:00, 20.3kB/s]"
          }
        },
        "8535f078f951410aa2e3b7e88dfae7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b06c77aae4b344599a06cc2d4f19f327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27892d59dc9a40ad8f8c7394a74e310c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5dfb390d92114b69bab15373638e8538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84c831259add40f89bf23734353e71d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28c596cda3d348799043036efaf99c97": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7261250c64ad48bfa0832510857eb4fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa0dade3f3eb43308f19050e1ff58fa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fe6b0ec2474d4abab6ca3ffe21050bba",
              "IPY_MODEL_2987d0283f694ea89b596da27f38f2b8",
              "IPY_MODEL_6f585adb998a4f3ab3d39b4f052c6ab6"
            ],
            "layout": "IPY_MODEL_2d67d77022be4db1a5da9433d78fde9d"
          }
        },
        "fe6b0ec2474d4abab6ca3ffe21050bba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0efc7daae48d47849c2591fb92875190",
            "placeholder": "​",
            "style": "IPY_MODEL_c2651a7b3f704b089434559deb69cdd0",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "2987d0283f694ea89b596da27f38f2b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4288ceeb308243748edd4628c1b01c8a",
            "max": 133530889,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_55f322be10f34f44a70479e248082b9f",
            "value": 133530889
          }
        },
        "6f585adb998a4f3ab3d39b4f052c6ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a26b5c0b481a436884353e1f59e7dfe5",
            "placeholder": "​",
            "style": "IPY_MODEL_20527cc890b04b84a55e75e16cdad242",
            "value": " 134M/134M [00:02&lt;00:00, 32.2MB/s]"
          }
        },
        "2d67d77022be4db1a5da9433d78fde9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0efc7daae48d47849c2591fb92875190": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2651a7b3f704b089434559deb69cdd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4288ceeb308243748edd4628c1b01c8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55f322be10f34f44a70479e248082b9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a26b5c0b481a436884353e1f59e7dfe5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20527cc890b04b84a55e75e16cdad242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c332fd616d74ab19ad1fcef6d5a9b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_10a9e2ec02414fd6bbe1ccfcbfb2c97d",
              "IPY_MODEL_c3765d6f9f884f9fb8bb44076ab86803",
              "IPY_MODEL_9f0cb6084cce40ed950a422db7b07920"
            ],
            "layout": "IPY_MODEL_5488fd54d19c4061bfed5b28259a95a3"
          }
        },
        "10a9e2ec02414fd6bbe1ccfcbfb2c97d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6fe06a27564f15b724ed87c6401efe",
            "placeholder": "​",
            "style": "IPY_MODEL_94f21b27d107449686e6ab0f2b913b24",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c3765d6f9f884f9fb8bb44076ab86803": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af1415b095724a398b0e4de1729fd359",
            "max": 316,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9b617aa248340f69963574d21c98708",
            "value": 316
          }
        },
        "9f0cb6084cce40ed950a422db7b07920": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_010ac7aa91444d3d8d140d602de75a7f",
            "placeholder": "​",
            "style": "IPY_MODEL_94983402f65342418d740e5b37eff555",
            "value": " 316/316 [00:00&lt;00:00, 20.9kB/s]"
          }
        },
        "5488fd54d19c4061bfed5b28259a95a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6fe06a27564f15b724ed87c6401efe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94f21b27d107449686e6ab0f2b913b24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af1415b095724a398b0e4de1729fd359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9b617aa248340f69963574d21c98708": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "010ac7aa91444d3d8d140d602de75a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94983402f65342418d740e5b37eff555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c39f99f17195468c8212c62c50264cea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e41646718c840f68e233737e8c4e6a0",
              "IPY_MODEL_5194ff2d24a0409b8585a9eda2ee1e42",
              "IPY_MODEL_69b431f29a5849eaaf4cf3c99e908719"
            ],
            "layout": "IPY_MODEL_e19da90867904a4eb35153186ad42675"
          }
        },
        "5e41646718c840f68e233737e8c4e6a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfb6030c95fd47d79f083cc60f8e87e7",
            "placeholder": "​",
            "style": "IPY_MODEL_bd8e1700d8304c2b87f41d858a055996",
            "value": "vocab.txt: 100%"
          }
        },
        "5194ff2d24a0409b8585a9eda2ee1e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fe53c2f5eeb4fd99fe6ff799ea926d8",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_216653d87322484ebefb805bbb16170f",
            "value": 231508
          }
        },
        "69b431f29a5849eaaf4cf3c99e908719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a06754355b54a1ba4a6c4f50e7e6d0f",
            "placeholder": "​",
            "style": "IPY_MODEL_f9ae544bfffe4d07a004df9cf89d26f6",
            "value": " 232k/232k [00:00&lt;00:00, 5.88MB/s]"
          }
        },
        "e19da90867904a4eb35153186ad42675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb6030c95fd47d79f083cc60f8e87e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd8e1700d8304c2b87f41d858a055996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fe53c2f5eeb4fd99fe6ff799ea926d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "216653d87322484ebefb805bbb16170f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a06754355b54a1ba4a6c4f50e7e6d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9ae544bfffe4d07a004df9cf89d26f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0eb06c6df82041a88e3d68927daca083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd73c23d9f7341a69571d787233fa153",
              "IPY_MODEL_9e0e05d4d318439882d28d4353771d0f",
              "IPY_MODEL_2f87eaaeb31a4536ab4b3f98501c2085"
            ],
            "layout": "IPY_MODEL_d440be7ab703438a91d158dbc6bd0c1d"
          }
        },
        "fd73c23d9f7341a69571d787233fa153": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb232d4200d4f6192c0c32e5ac5f67b",
            "placeholder": "​",
            "style": "IPY_MODEL_a4e545fe49e64f88a42079f80a5f0e0b",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "9e0e05d4d318439882d28d4353771d0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d941a7cbb124e98b137132009db236d",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dcfba2f46606404ba871185e5bafc877",
            "value": 112
          }
        },
        "2f87eaaeb31a4536ab4b3f98501c2085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_465b583afb8f44b297af672b9894c209",
            "placeholder": "​",
            "style": "IPY_MODEL_a5e4a808d2b645a385b8c3ec4ea4a4c0",
            "value": " 112/112 [00:00&lt;00:00, 7.18kB/s]"
          }
        },
        "d440be7ab703438a91d158dbc6bd0c1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb232d4200d4f6192c0c32e5ac5f67b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4e545fe49e64f88a42079f80a5f0e0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d941a7cbb124e98b137132009db236d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcfba2f46606404ba871185e5bafc877": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "465b583afb8f44b297af672b9894c209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5e4a808d2b645a385b8c3ec4ea4a4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}